{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser2: A parser for the NENA Markup standard\n",
    "\n",
    "The goal of this parser is to parse texts written in the plaintext\n",
    "[NENA markup format][nenamarkup] and deliver a structured\n",
    "list of words and their features, as well as paragraphing and line \n",
    "marks, which can be stored in a data format such as [Text-Fabric][textfabric], \n",
    "[Text-as-Graph][textasgraph] or (less optimally), XML or other hierarchical \n",
    "structures. \n",
    "\n",
    "For the Nena Markup parser, we make use of [Sly][sly], a Python implementation \n",
    "of the lex/yacc type of parser generators.\n",
    "\n",
    "This parser development is a follow up from the older, more verbose [NenaParser (1)](NenaParser.ipynb)\n",
    "which contains a lot of shift/reduce conflicts as well as a lot of standards that will\n",
    "no longer be in use (see for instance \"footnoting\").\n",
    "\n",
    "[nenamarkup]: ../docs/nena_format.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[textfabric]: https://github.com/annotation/text-fabric\n",
    "[textasgraph]: https://www.balisage.net/Proceedings/vol19/print/Dekker01/BalisageVol19-Dekker01.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/cody')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.home()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sly import Lexer, Parser\n",
    "import unicodedata\n",
    "from pprint import pprint\n",
    "\n",
    "VERSION = '2.0'\n",
    "PROJECT = Path.home().joinpath('github/CambridgeSemiticsLab/nena_corpus')\n",
    "STANDARDS = PROJECT.joinpath('standards')\n",
    "DIALECT_STANDARDS = STANDARDS.joinpath('dialect_data')\n",
    "TEXT_INPUT = PROJECT.joinpath(f'texts/{VERSION}')\n",
    "JSON_OUTPUT = PROJECT.joinpath(f'parsed_texts/{VERSION}')\n",
    "\n",
    "# prepare alphabet and punctuation standards for processing\n",
    "alphabet_std = STANDARDS.joinpath('alphabet.json')\n",
    "punctuation_std = STANDARDS.joinpath('punctuation.json')\n",
    "lang_std = STANDARDS.joinpath('foreign_languages.json')\n",
    "\n",
    "with open(alphabet_std, 'r') as infile:\n",
    "    alphabet_data = json.load(infile)    \n",
    "with open(punctuation_std, 'r') as infile:    \n",
    "    punct_data = json.load(infile)\n",
    "with open(lang_std, 'r') as infile:\n",
    "    lang_data = json.load(infile)\n",
    "\n",
    "alphabet_re = '|'.join(let['decomposed_regex'] for let in alphabet_data)\n",
    "punct_begin_re = '|'.join(punct['regex'] for punct in punct_data\n",
    "                            if punct['position'] == 'begin')\n",
    "punct_end_re = '|'.join(punct['regex'] for punct in punct_data \n",
    "                            if punct['position'] == 'end')\n",
    "foreign_codes = '|'.join(lang['code'] for lang in lang_data)\n",
    "                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/cody/github/CambridgeSemiticsLab/nena_corpus/standards/dialect_data/barwar')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIALECT_STANDARDS.joinpath('barwar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in DIALECT_STANDARDS.joinpath('barwar').glob('*'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Text\n",
    "\n",
    "Below is a dummy text we can use to test the parsers on, written in the\n",
    "[NENA Markup](../docs/nena_format.md) standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unicodedata.normalize('NFD', '''\n",
    "dialect:: Urmi_C\n",
    "title:: When Shall I Die?\n",
    "encoding:: UTF8\n",
    "speakers:: YD=Yulia Davudi, GK=Geoffrey Khan, CK=Cody Kingham\n",
    "place:: +Hassar +Baba-čanɟa, N\n",
    "transcriber:: Geoffrey Khan\n",
    "text_id:: A32 \n",
    "\n",
    "1 YD    xá-yuma \"⁺malla ⁺Nasrádən\" váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə\n",
    "⁺vàrəva,ˈ mə́rrə ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P: bŏ́ro> \n",
    "bàbaˈ ʾàtən=daˈ ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux\n",
    ".ˈ ʾána ⁺šūl-ɟànilə.ˈ náplən nàplən.ˈ\n",
    "2 0:08    ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla bitáyələ drúm \n",
    "⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən ⁺dílux \n",
    "ʾána bət-náplənva m-⁺al-ʾilàna.ˈ bas-tánili xázən ʾána ʾíman bət-mètən.ˈ \n",
    "ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ \n",
    "mə̀drə,ˈ\n",
    "\n",
    "3 0:14 GK    maxlèta?\n",
    "4 0:15 YD    ⁺rába ⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ \n",
    "ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "5 0:18    ⁺málla\n",
    "6 0:19 CK    <E:Why hello there!> \n",
    "7 0:21 YD    múttəva ... ⁺ṱànaˈ ⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi \n",
    "⁺rába múttəva ⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ ⁺báyyava ʾask̭áva ⁺ʾùllul.ˈ\n",
    "ʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ ʾána dū́n\n",
    "k̭arbúnə k̭a-myàta.ˈ\n",
    "8 0:20    <E:ok>\n",
    "9 0:21 CK    <E:yes?>\n",
    "10 0:22 YD    xáč̭č̭a=da sə̀k̭laˈ xa-xìta.ˈ ɟánu mudməxxálə ⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ \n",
    "xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ xuyravàtuˈ xə́šlun tílun \n",
    "mə̀rrunˈ: ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə xob-ʾána mìtən.ˈ\n",
    "lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun ʾat-xàya!ˈ hamzùməvət.ˈ bəšvák̭una \n",
    "⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺parmùyə.ˈ\n",
    " ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a text with NENA Markup\n",
    "\n",
    "Below is a representation of the tree-like structure of a NENA standard text file. This is the structure that the parser must recognize and reproduce.\n",
    "\n",
    "`+` is used to represent one or more elements.\n",
    "\n",
    "```\n",
    "text\n",
    "  |\n",
    "  metadata block\n",
    "  |  |\n",
    "  |  +attribute\n",
    "  | \n",
    "  text block\n",
    "    |\n",
    "    +paragraph\n",
    "      |   \n",
    "      +line\n",
    "        |\n",
    "        +word\n",
    "          |\n",
    "          +letter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These items will be returned in the following Pythonic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = [ # text\n",
    "    [ # metadata block\n",
    "        {\n",
    "            'dialect': 'Urmi_C',\n",
    "            'title': 'When Shall I Die?',\n",
    "            'encoding': 'UTF8',\n",
    "        }\n",
    "    ],\n",
    "    [ # text block\n",
    "        [ # paragraph\n",
    "            { # line\n",
    "                'number': '1', \n",
    "                'timestamp': '0:00',\n",
    "                'words': [\n",
    "                    { # word\n",
    "                        'text':'xá',\n",
    "                        'begin':'',\n",
    "                        'end':'-',\n",
    "                        'lang':'NENA', \n",
    "                        'letters':('x','á'),\n",
    "                    },\n",
    "                    # ...\n",
    "                    { # foreign word\n",
    "                        'text':'bŏ́ro',\n",
    "                        'begin':'<P:',\n",
    "                        'end': '> ',\n",
    "                        'lang': 'P', \n",
    "                        'letters':('b','ŏ́','r','o'),\n",
    "                    }, \n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = re.compile(r'\\d+:\\d+\\d*')\n",
    "linenum = re.compile(f'\\d+')\n",
    "initials = re.compile(r'\\D\\D')\n",
    "\n",
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    def error(self, t):\n",
    "        \"\"\"Give warning for bad characters\"\"\"\n",
    "        print(f\"Illegal character {repr(t.value[0])} @ index {self.index}\")\n",
    "        self.index += 1\n",
    "    \n",
    "    # set of token names as required by\n",
    "    # the Lexer class\n",
    "    tokens = {\n",
    "        LETTER, PUNCT_BEGIN, PUNCT_END, NEWLINES,\n",
    "        NEWLINE, NEWLINES, LINE_INDICATOR, ATTRIBUTE, \n",
    "        FOREIGN_LETTER, LANG_START, LANG_END        \n",
    "    }\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'[a-z0-9_]+\\s*::\\s*.*')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        field, value = tuple(t.value.split('::'))\n",
    "        t.value = {field.strip(): value.strip()}\n",
    "        for attr, val in t.value.items():\n",
    "            # arrange loaded speakers into dict\n",
    "            if attr == 'speakers':\n",
    "                speakers = {}\n",
    "                for speakset in val.split(','):\n",
    "                    initials, speaker = speakset.split('=')\n",
    "                    speakers[initials.strip()] = speaker.strip()\n",
    "                t.value[attr] = speakers\n",
    "        return t\n",
    "    \n",
    "    @_(r'(?<=\\n)\\d+.*?\\s{2,}',\n",
    "       r'(?<=\\n)\\d+.*?\\t')\n",
    "    def LINE_INDICATOR(self, t):\n",
    "        attribs = {}\n",
    "        elements = t.value.split()\n",
    "        attribs['number'] = elements[0]\n",
    "        for element in elements:\n",
    "            if timestamp.match(element):\n",
    "                attribs['timestamp'] = element\n",
    "            elif linenum.match(element):\n",
    "                attribs['number'] = element\n",
    "            elif initials.match(element):\n",
    "                attribs['speaker'] = element\n",
    "            else:\n",
    "                raise Exception(f'invalid element {element} in line indicator {t.value}')\n",
    "        t.value = attribs\n",
    "        return t\n",
    "    \n",
    "    NEWLINES = r'\\n\\s*\\n\\s*' # i.e. marks text-blocks\n",
    "    LETTER = alphabet_re    \n",
    "    PUNCT_BEGIN = punct_begin_re\n",
    "    PUNCT_END = punct_end_re\n",
    "    NEWLINE = '\\n\\s*'\n",
    "    \n",
    "    @_(r'<[A-Za-z?]+:\\s*')\n",
    "    def LANG_START(self, t):\n",
    "        lang = re.match(r'<([A-Za-z?]+):', t.value).group(1)\n",
    "        tag = t.value.strip() + ' ' # ensure spacing\n",
    "        t.value = (tag, lang)\n",
    "        return t\n",
    "        \n",
    "    LANG_END = r'>'\n",
    "    \n",
    "    # NB: tokens evaluated in order of appearance here\n",
    "    # thus foreign string matched lastly\n",
    "    FOREIGN_LETTER = r'[a-zA-ZðÐɟəƏɛƐʾʿθΘ][\\u0300-\\u033d]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "tokens = [(tok.type, tok.value) for tok in lexer.tokenize(example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'dialect': 'Urmi_C'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'title': 'When Shall I Die?'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'encoding': 'UTF8'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE',\n",
      "  {'speakers': {'CK': 'Cody Kingham',\n",
      "                'GK': 'Geoffrey Khan',\n",
      "                'YD': 'Yulia Davudi'}}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'place': '+Hassar +Baba-čanɟa, N'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'transcriber': 'Geoffrey Khan'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'text_id': 'A32'}),\n",
      " ('NEWLINES', '\\n\\n'),\n",
      " ('LINE_INDICATOR', {'number': '1', 'speaker': 'YD'}),\n",
      " ('LETTER', 'x'),\n",
      " ('LETTER', 'á'),\n",
      " ('PUNCT_END', '-'),\n",
      " ('LETTER', 'y')]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.match(r'^[a-z0-9_]+: .*(?=\\n)', 're: wáy b-šɛ̀ fsdf\\nfesfes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word(letters, beginnings=[], endings=[]):\n",
    "    \"\"\"Return word dictionary\"\"\"\n",
    "    return {\n",
    "        'word': ''.join(letters),\n",
    "        'letters': letters,\n",
    "        'beginnings': beginnings,\n",
    "        'endings': endings,\n",
    "    }\n",
    "\n",
    "def modify_attribute(words, key, value):\n",
    "    \"\"\"Modify dict attribute for a list of words\"\"\"\n",
    "    for word in words:\n",
    "        word[key] = value\n",
    "    return words\n",
    "\n",
    "def format_tag_endings(tag, endings=[]):\n",
    "    \"\"\"Format punctuation around a tag.\n",
    "    \n",
    "    Normalizes in case of irregularity. For instance, in the\n",
    "    cases of both\n",
    "        words.</> \n",
    "        words</>.\n",
    "    the tags will be normalized to either an in/exclusive order.\n",
    "    \"\"\"\n",
    "    return [tag] + endings\n",
    "    \n",
    "def tag_speakers(text, speakers_dict):\n",
    "    \"\"\"Tag speakers in a text.\n",
    "    \n",
    "    Speakers can be activated or deactivated as needed.\n",
    "    \"\"\"\n",
    "    for paragraph in text:\n",
    "        for line in paragraph:\n",
    "            if 'speaker' not in line:\n",
    "                line['speaker'] = cur_speaker\n",
    "            else:\n",
    "                new_speaker = line['speaker']\n",
    "                try: \n",
    "                    cur_speaker = speakers_dict[new_speaker]\n",
    "                    line['speaker'] = cur_speaker\n",
    "                except KeyError:\n",
    "                    raise Exception(f'speaker {cur_speaker} not specified in speakers metadata')\n",
    "    \n",
    "class NenaParser(Parser):\n",
    "    \n",
    "    #debugfile = 'nena_parser.out'\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    def error(self, t):\n",
    "        raise Exception(f'unexpected {t.type} ({repr(t.value)}) at index {t.index}')\n",
    "    \n",
    "    @_('attributes NEWLINES text_block')\n",
    "    def nena(self, p):\n",
    "        tag_speakers(p.text_block, p.attributes['speakers'])\n",
    "        return [p.attributes, p.text_block]\n",
    "    \n",
    "    @_('attributes NEWLINE ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        p.attributes.update(p.ATTRIBUTE)\n",
    "        return p.attributes\n",
    "    \n",
    "    @_('NEWLINE ATTRIBUTE', 'ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        return p.ATTRIBUTE\n",
    "    \n",
    "    @_('text_block NEWLINES paragraph')\n",
    "    def text_block(self, p):\n",
    "        return p.text_block + [p.paragraph]\n",
    "    \n",
    "    @_('paragraph')\n",
    "    def text_block(self, p):\n",
    "        return [p.paragraph]\n",
    "    \n",
    "    @_('paragraph line')\n",
    "    def paragraph(self, p):\n",
    "        return p.paragraph + [p.line]\n",
    "    \n",
    "    @_('line')\n",
    "    def paragraph(self, p):\n",
    "        return [p.line]\n",
    "    \n",
    "    @_('LINE_INDICATOR span words',\n",
    "      'LINE_INDICATOR span span words',\n",
    "      'LINE_INDICATOR span word span words',\n",
    "      'LINE_INDICATOR span',\n",
    "      )\n",
    "    def line(self, p):\n",
    "        words = []\n",
    "        for wordtype in list(p)[1:]:\n",
    "            if type(wordtype) == list: \n",
    "                words += wordtype\n",
    "            else:\n",
    "                words.append(wordtype)\n",
    "        p.LINE_INDICATOR['words'] = words\n",
    "        return p.LINE_INDICATOR\n",
    "    \n",
    "    @_('LINE_INDICATOR word span words')\n",
    "    def line(self, p):\n",
    "        p.LINE_INDICATOR['words'] = [p.word] + p.span + p.words\n",
    "        return p.LINE_INDICATOR\n",
    "    \n",
    "    @_('LINE_INDICATOR words')\n",
    "    def line(self, p):\n",
    "        p.LINE_INDICATOR['words'] = p.words\n",
    "        return p.LINE_INDICATOR\n",
    "    \n",
    "    @_('LINE_INDICATOR word')\n",
    "    def line(self, p):\n",
    "        p.LINE_INDICATOR['words'] = [p.word]\n",
    "        return p.LINE_INDICATOR\n",
    "    \n",
    "    @_('words span')\n",
    "    def words(self, p):\n",
    "        return p.words + p.span\n",
    "    \n",
    "    @_('LANG_START letters LANG_END',\n",
    "       'LANG_START letters LANG_END endings',\n",
    "       'LANG_START letters LANG_END NEWLINE',\n",
    "       'LANG_START beginnings letters LANG_END endings',\n",
    "       'LANG_START beginnings letters LANG_END NEWLINE',\n",
    "      )\n",
    "    def span(self, p):\n",
    "        begin_tag, value = p.LANG_START\n",
    "        beginnings = [begin_tag] + getattr(p, 'beginnings', [])\n",
    "        \n",
    "        # build ends\n",
    "        trailing_ends = getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')\n",
    "        endings = format_tag_endings(p.LANG_END, trailing_ends)\n",
    "        \n",
    "        word = make_word(p.letters, beginnings=beginnings, endings=endings)\n",
    "        word['lang'] = value\n",
    "        return [word]\n",
    "    \n",
    "    @_('LANG_START word letters LANG_END',\n",
    "       'LANG_START word letters LANG_END endings',\n",
    "       'LANG_START word letters LANG_END NEWLINE',\n",
    "       'LANG_START word beginnings letters LANG_END endings',\n",
    "       'LANG_START word beginnings letters LANG_END NEWLINE',\n",
    "       'LANG_START words letters LANG_END endings',\n",
    "       'LANG_START words letters LANG_END NEWLINE',\n",
    "       'LANG_START words beginnings letters LANG_END endings',\n",
    "      )\n",
    "    def span(self, p):\n",
    "        begin_tag, value = p[0]\n",
    "        \n",
    "        # compile words\n",
    "        words = []\n",
    "        if getattr(p, 'word', None):\n",
    "            p.word['beginnings'].insert(0, begin_tag)\n",
    "            words.append(p.word)\n",
    "        elif getattr(p, 'words', None):\n",
    "            words.extend(p.words)\n",
    "            \n",
    "        # build new word from dangling letters and ends\n",
    "        trailing_ends = getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')\n",
    "        endings = format_tag_endings(p.LANG_END, trailing_ends)\n",
    "        beginnings = getattr(p, 'beginnings', [])\n",
    "        words.append(make_word(p.letters, beginnings=beginnings, endings=endings))\n",
    "        \n",
    "        return modify_attribute(words, 'lang', value)\n",
    "    \n",
    "    @_('LANG_START words LANG_END',\n",
    "       'LANG_START words LANG_END endings',\n",
    "       'LANG_START words LANG_END NEWLINE',\n",
    "       'LANG_START word LANG_END',\n",
    "       'LANG_START word LANG_END endings',\n",
    "       'LANG_START word LANG_END NEWLINE',)\n",
    "    def span(self, p):\n",
    "        words = getattr(p, 'words', [p[1]])\n",
    "        begin_tag, value = p[0]\n",
    "        first_word, last_word = words[0], words[-1]\n",
    "        first_word['beginnings'].insert(0, begin_tag)\n",
    "        \n",
    "        # build ends\n",
    "        trailing_ends = last_word['endings'] + getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')        \n",
    "        last_word['endings'] = format_tag_endings(p[2], trailing_ends)\n",
    "        \n",
    "        return modify_attribute(words, 'lang', value)\n",
    "    \n",
    "    @_('words word')\n",
    "    def words(self, p):\n",
    "        return p.words + [p.word]\n",
    "    \n",
    "    @_('word word')\n",
    "    def words(self, p):\n",
    "        return [p[0]] + [p[1]]\n",
    "    \n",
    "    @_('beginnings letters endings', \n",
    "       'letters endings',\n",
    "       'letters NEWLINE',\n",
    "       'letters NEWLINE endings',\n",
    "       'beginnings letters NEWLINE',\n",
    "       'beginnings letters NEWLINE endings',\n",
    "      )\n",
    "    def word(self, p):\n",
    "        beginnings = getattr(p, 'beginnings', [])\n",
    "        endings =  getattr(p, 'endings', [' '])\n",
    "        return make_word(p.letters, beginnings, endings)\n",
    "\n",
    "    @_('PUNCT_BEGIN beginnings')\n",
    "    def beginnings(self, p):\n",
    "        return [p.PUNCT_BEGIN] + p.beginnings\n",
    "    \n",
    "    @_('PUNCT_BEGIN')\n",
    "    def beginnings(self, p):\n",
    "        return [p.PUNCT_BEGIN]\n",
    "    \n",
    "    @_('endings NEWLINE')\n",
    "    def endings(self, p):\n",
    "        if p.endings[-1] != ' ':\n",
    "            p.endings.append(' ')\n",
    "        return p.endings\n",
    "    \n",
    "    @_('endings PUNCT_END')\n",
    "    def endings(self, p):\n",
    "        return p.endings + [p.PUNCT_END]\n",
    "    \n",
    "    @_('PUNCT_END')\n",
    "    def endings(self, p):\n",
    "        return [p.PUNCT_END]\n",
    "        \n",
    "    @_('LETTER letters', \n",
    "       'FOREIGN_LETTER letters')\n",
    "    def letters(self, p):\n",
    "        return [p[0]] + p[1]\n",
    "    \n",
    "    @_('LETTER', \n",
    "       'FOREIGN_LETTER')\n",
    "    def letters(self, p):\n",
    "        return [p[0]]\n",
    "\n",
    "parser = NenaParser()\n",
    "test = parser.parse(lexer.tokenize(example))\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, text = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialect': 'Urmi_C',\n",
       " 'title': 'When Shall I Die?',\n",
       " 'encoding': 'UTF8',\n",
       " 'speakers': {'YD': 'Yulia Davudi',\n",
       "  'GK': 'Geoffrey Khan',\n",
       "  'CK': 'Cody Kingham'},\n",
       " 'place': '+Hassar +Baba-čanɟa, N',\n",
       " 'transcriber': 'Geoffrey Khan',\n",
       " 'text_id': 'A32'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-paragraphs\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, n-lines\n",
    "len(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'number': '1',\n",
       "  'speaker': 'Yulia Davudi',\n",
       "  'words': [{'word': 'xá',\n",
       "    'letters': ['x', 'á'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'yuma',\n",
       "    'letters': ['y', 'u', 'm', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'malla',\n",
       "    'letters': ['m', 'a', 'l', 'l', 'a'],\n",
       "    'beginnings': ['\"', '⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'Nasrádən',\n",
       "    'letters': ['N', 'a', 's', 'r', 'á', 'd', 'ə', 'n'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['\"', ' ']},\n",
       "   {'word': 'váyələ',\n",
       "    'letters': ['v', 'á', 'y', 'ə', 'l', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'tíva',\n",
       "    'letters': ['t', 'í', 'v', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾal',\n",
       "    'letters': ['ʾ', 'a', 'l'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'k̭èsa',\n",
       "    'letters': ['k̭', 'è', 's', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'xá', 'letters': ['x', 'á'], 'beginnings': [], 'endings': [' ']},\n",
       "   {'word': 'mən',\n",
       "    'letters': ['m', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'nášə',\n",
       "    'letters': ['n', 'á', 'š', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'vàrəva',\n",
       "    'letters': ['v', 'à', 'r', 'ə', 'v', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [',', 'ˈ', ' ']},\n",
       "   {'word': 'mə́rrə',\n",
       "    'letters': ['m', 'ə́', 'r', 'r', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'màlla',\n",
       "    'letters': ['m', 'à', 'l', 'l', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'ʾátən',\n",
       "    'letters': ['ʾ', 'á', 't', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾo', 'letters': ['ʾ', 'o'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'k̭ésa',\n",
       "    'letters': ['k̭', 'é', 's', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'pràmut',\n",
       "    'letters': ['p', 'r', 'à', 'm', 'u', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': [',', 'ˈ', ' ']},\n",
       "   {'word': 'bət',\n",
       "    'letters': ['b', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'nàplət',\n",
       "    'letters': ['n', 'à', 'p', 'l', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'mə́rrə',\n",
       "    'letters': ['m', 'ə́', 'r', 'r', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bŏ́ro',\n",
       "    'letters': ['b', 'ŏ́', 'r', 'o'],\n",
       "    'beginnings': ['<P: '],\n",
       "    'endings': ['>', ' '],\n",
       "    'lang': 'P'},\n",
       "   {'word': 'bàba',\n",
       "    'letters': ['b', 'à', 'b', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'ʾàtən',\n",
       "    'letters': ['ʾ', 'à', 't', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['=']},\n",
       "   {'word': 'da',\n",
       "    'letters': ['d', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'šúla',\n",
       "    'letters': ['š', 'ú', 'l', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'lə̀tlux',\n",
       "    'letters': ['l', 'ə̀', 't', 'l', 'u', 'x'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'tíyyət',\n",
       "    'letters': ['t', 'í', 'y', 'y', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'b', 'letters': ['b'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'dìyyi',\n",
       "    'letters': ['d', 'ì', 'y', 'y', 'i'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'k̭ítət',\n",
       "    'letters': ['k̭', 'í', 't', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'šúk̭',\n",
       "    'letters': ['š', 'ú', 'k̭'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'si', 'letters': ['s', 'i'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'bar',\n",
       "    'letters': ['b', 'a', 'r'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'šùlux',\n",
       "    'letters': ['š', 'ù', 'l', 'u', 'x'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'ʾána',\n",
       "    'letters': ['ʾ', 'á', 'n', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'šūl',\n",
       "    'letters': ['š', 'ū', 'l'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'ɟànilə',\n",
       "    'letters': ['ɟ', 'à', 'n', 'i', 'l', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'náplən',\n",
       "    'letters': ['n', 'á', 'p', 'l', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'nàplən',\n",
       "    'letters': ['n', 'à', 'p', 'l', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']}]},\n",
       " {'number': '2',\n",
       "  'timestamp': '0:08',\n",
       "  'words': [{'word': 'hàla',\n",
       "    'letters': ['h', 'à', 'l', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'ʾo', 'letters': ['ʾ', 'o'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'náša',\n",
       "    'letters': ['n', 'á', 'š', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'léva',\n",
       "    'letters': ['l', 'é', 'v', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'xíša',\n",
       "    'letters': ['x', 'í', 'š', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'xá', 'letters': ['x', 'á'], 'beginnings': [], 'endings': [' ']},\n",
       "   {'word': 'ʾəsrá',\n",
       "    'letters': ['ʾ', 'ə', 's', 'r', 'á'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'pasulyày',\n",
       "    'letters': ['p', 'a', 's', 'u', 'l', 'y', 'à', 'y'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [',', 'ˈ', ' ']},\n",
       "   {'word': 'málla',\n",
       "    'letters': ['m', 'á', 'l', 'l', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bitáyələ',\n",
       "    'letters': ['b', 'i', 't', 'á', 'y', 'ə', 'l', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'drúm',\n",
       "    'letters': ['d', 'r', 'ú', 'm'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾal',\n",
       "    'letters': ['ʾ', 'a', 'l'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'ʾàrra',\n",
       "    'letters': ['ʾ', 'à', 'r', 'r', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'bək̭yámələ',\n",
       "    'letters': ['b', 'ə', 'k̭', 'y', 'á', 'm', 'ə', 'l', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bərxáṱələ',\n",
       "    'letters': ['b', 'ə', 'r', 'x', 'á', 'ṱ', 'ə', 'l', 'ə'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bàru',\n",
       "    'letters': ['b', 'à', 'r', 'u'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'màra',\n",
       "    'letters': ['m', 'à', 'r', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'maxlèta',\n",
       "    'letters': ['m', 'a', 'x', 'l', 'è', 't', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [',', 'ˈ', ' ']},\n",
       "   {'word': 'ʾátən',\n",
       "    'letters': ['ʾ', 'á', 't', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'dílux',\n",
       "    'letters': ['d', 'í', 'l', 'u', 'x'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾána',\n",
       "    'letters': ['ʾ', 'á', 'n', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bət',\n",
       "    'letters': ['b', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'náplənva',\n",
       "    'letters': ['n', 'á', 'p', 'l', 'ə', 'n', 'v', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'm', 'letters': ['m'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'al',\n",
       "    'letters': ['a', 'l'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'ʾilàna',\n",
       "    'letters': ['ʾ', 'i', 'l', 'à', 'n', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'bas',\n",
       "    'letters': ['b', 'a', 's'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'tánili',\n",
       "    'letters': ['t', 'á', 'n', 'i', 'l', 'i'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'xázən',\n",
       "    'letters': ['x', 'á', 'z', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾána',\n",
       "    'letters': ['ʾ', 'á', 'n', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'ʾíman',\n",
       "    'letters': ['ʾ', 'í', 'm', 'a', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'bət',\n",
       "    'letters': ['b', 'ə', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'mètən',\n",
       "    'letters': ['m', 'è', 't', 'ə', 'n'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['.', 'ˈ', ' ']},\n",
       "   {'word': 'ʾo', 'letters': ['ʾ', 'o'], 'beginnings': [], 'endings': ['-']},\n",
       "   {'word': 'náša',\n",
       "    'letters': ['n', 'á', 'š', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'xzílə',\n",
       "    'letters': ['x', 'z', 'í', 'l', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'k̭at',\n",
       "    'letters': ['k̭', 'a', 't'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['-']},\n",
       "   {'word': 'ʾá', 'letters': ['ʾ', 'á'], 'beginnings': [], 'endings': [' ']},\n",
       "   {'word': 'màlla',\n",
       "    'letters': ['m', 'à', 'l', 'l', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'hónu',\n",
       "    'letters': ['h', 'ó', 'n', 'u'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'xáč̭č̭a',\n",
       "    'letters': ['x', 'á', 'č̭', 'č̭', 'a'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ', '...', ' ']},\n",
       "   {'word': 'basùrələ',\n",
       "    'letters': ['b', 'a', 's', 'ù', 'r', 'ə', 'l', 'ə'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'mə́rrə',\n",
       "    'letters': ['m', 'ə́', 'r', 'r', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [' ']},\n",
       "   {'word': 'k̭àtu',\n",
       "    'letters': ['k̭', 'à', 't', 'u'],\n",
       "    'beginnings': [],\n",
       "    'endings': ['ˈ', ' ']},\n",
       "   {'word': 'maxlèta',\n",
       "    'letters': ['m', 'a', 'x', 'l', 'è', 't', 'a'],\n",
       "    'beginnings': ['⁺'],\n",
       "    'endings': [',', 'ˈ', ' ']},\n",
       "   {'word': 'mə̀drə',\n",
       "    'letters': ['m', 'ə̀', 'd', 'r', 'ə'],\n",
       "    'beginnings': [],\n",
       "    'endings': [',', 'ˈ']}],\n",
       "  'speaker': 'Yulia Davudi'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1\n",
    "len(text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1, n-words\n",
    "len(text[0][0]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'xá', 'letters': ['x', 'á'], 'beginnings': [], 'endings': ['-']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1, word 1\n",
    "text[0][0]['words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = Path('../texts/2.0')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../texts/2.0/Barwar'), PosixPath('../texts/2.0/Urmi_C')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parse On All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dialect ../texts/2.0/Barwar--\n",
      "\n",
      "trying: A Hundred Gold Coins.nena\n",
      "\t√\n",
      "trying: A Man Called Čuxo.nena\n",
      "\t√\n",
      "trying: A Tale of Two Kings.nena\n",
      "\t√\n",
      "trying: A Tale of a Prince and a Princess.nena\n",
      "\t√\n",
      "trying: Baby Leliθa.nena\n",
      "\t√\n",
      "trying: Dəmdəma.nena\n",
      "\t√\n",
      "trying: Gozali and Nozali.nena\n",
      "\t√\n",
      "trying: I Am Worth the Same as a Blind Wolf.nena\n",
      "\t√\n",
      "trying: Man Is Treacherous.nena\n",
      "\t√\n",
      "trying: Measure for Measure.nena\n",
      "\t√\n",
      "trying: Nanno and Jəndo.nena\n",
      "\t√\n",
      "trying: Qaṭina Rescues His Nephew From Leliθa.nena\n",
      "\t√\n",
      "trying: Sour Grapes.nena\n",
      "\t√\n",
      "trying: Tales From the 1001 Nights.nena\n",
      "\t√\n",
      "trying: The Battle With Yuwanəs the Armenian.nena\n",
      "\t√\n",
      "trying: The Bear and the Fox.nena\n",
      "\t√\n",
      "trying: The Brother of Giants.nena\n",
      "\t√\n",
      "trying: The Cat and the Mice.nena\n",
      "\t√\n",
      "trying: The Cooking Pot.nena\n",
      "\t√\n",
      "trying: The Crafty Hireling.nena\n",
      "\t√\n",
      "trying: The Crow and the Cheese.nena\n",
      "\t√\n",
      "trying: The Daughter of the King.nena\n",
      "\t√\n",
      "trying: The Fox and the Lion.nena\n",
      "\t√\n",
      "trying: The Fox and the Miller.nena\n",
      "\t√\n",
      "trying: The Fox and the Stork.nena\n",
      "\t√\n",
      "trying: The Giant’s Cave.nena\n",
      "\t√\n",
      "trying: The Girl and the Seven Brothers.nena\n",
      "\t√\n",
      "trying: The King With Forty Sons.nena\n",
      "\t√\n",
      "trying: The Leliθa From č̭āl.nena\n",
      "\t√\n",
      "trying: The Lion King.nena\n",
      "\t√\n",
      "trying: The Lion With a Swollen Leg.nena\n",
      "\t√\n",
      "trying: The Man Who Cried Wolf.nena\n",
      "\t√\n",
      "trying: The Man Who Wanted to Work.nena\n",
      "\t√\n",
      "trying: The Monk Who Wanted to Know When He Would Die.nena\n",
      "\t√\n",
      "trying: The Monk and the Angel.nena\n",
      "\t√\n",
      "trying: The Priest and the Mullah.nena\n",
      "\t√\n",
      "trying: The Sale of an Ox.nena\n",
      "\t√\n",
      "trying: The Scorpion and the Snake.nena\n",
      "\t√\n",
      "trying: The Selfish Neighbour.nena\n",
      "\t√\n",
      "trying: The Sisisambər Plant.nena\n",
      "\t√\n",
      "trying: The Story With No End.nena\n",
      "\t√\n",
      "trying: The Tale of Farxo and Səttiya.nena\n",
      "\t√\n",
      "trying: The Tale of Mămo and Zine.nena\n",
      "\t√\n",
      "trying: The Tale of Mərza Pămət.nena\n",
      "\t√\n",
      "trying: The Tale of Nasimo.nena\n",
      "\t√\n",
      "trying: The Tale of Parizada, Warda and Nargis.nena\n",
      "\t√\n",
      "trying: The Tale of Rustam (1).nena\n",
      "\t√\n",
      "trying: The Tale of Rustam (2).nena\n",
      "\t√\n",
      "trying: The Wise Daughter of the King.nena\n",
      "\t√\n",
      "trying: The Wise Snake.nena\n",
      "\t√\n",
      "trying: The Wise Young Man.nena\n",
      "\t√\n",
      "trying: Šošət Xere.nena\n",
      "\t√\n",
      "--Dialect ../texts/2.0/Urmi_C--\n",
      "\n",
      "trying: A Close Shave.nena\n",
      "\t√\n",
      "trying: A Cure for a Husband’s Madness.nena\n",
      "\t√\n",
      "trying: A Donkey Knows Best.nena\n",
      "\t√\n",
      "trying: A Dragon in the Well.nena\n",
      "\t√\n",
      "trying: A Dutiful Son.nena\n",
      "\t√\n",
      "trying: A Frog Wants a Husband.nena\n",
      "\t√\n",
      "trying: A Lost Donkey.nena\n",
      "\t√\n",
      "trying: A Lost Ring.nena\n",
      "\t√\n",
      "trying: A Painting of the King of Iran.nena\n",
      "\t√\n",
      "trying: A Pound of Flesh.nena\n",
      "\t√\n",
      "trying: A Sweater to Pay Off a Debt.nena\n",
      "\t√\n",
      "trying: A Thousand Dinars.nena\n",
      "\t√\n",
      "trying: A Visit From Harun Ar-Rashid.nena\n",
      "\t√\n",
      "trying: Agriculture and Village Life.nena\n",
      "\t√\n",
      "trying: Am I Dead?.nena\n",
      "\t√\n",
      "trying: An Orphan Duckling.nena\n",
      "\t√\n",
      "trying: Axiqar.nena\n",
      "\t√\n",
      "trying: Events in 1946 on the Urmi Plain.nena\n",
      "\t√\n",
      "trying: Games.nena\n",
      "\t√\n",
      "trying: Hunting.nena\n",
      "\t√\n",
      "trying: I Have Died.nena\n",
      "\t√\n",
      "trying: Ice for Dinner.nena\n",
      "\t√\n",
      "trying: Is There a Man With No Worries?.nena\n",
      "\t√\n",
      "trying: Kindness to a Donkey.nena\n",
      "\t√\n",
      "trying: Lost Money.nena\n",
      "\t√\n",
      "trying: Mistaken Identity.nena\n",
      "\t√\n",
      "trying: Much Ado About Nothing.nena\n",
      "\t√\n",
      "trying: Nipuxta.nena\n",
      "\t√\n",
      "trying: No Bread Today.nena\n",
      "\t√\n",
      "trying: Problems Lighting a Fire.nena\n",
      "\t√\n",
      "trying: St. Zayya’s Cake Dough.nena\n",
      "\t√\n",
      "trying: Star-Crossed Lovers.nena\n",
      "\t√\n",
      "trying: Stomach Trouble.nena\n",
      "\t√\n",
      "trying: The Adventures of Ashur.nena\n",
      "\t√\n",
      "trying: The Adventures of Two Brothers.nena\n",
      "\t√\n",
      "trying: The Adventures of a Princess.nena\n",
      "\t√\n",
      "trying: The Angel of Death.nena\n",
      "\t√\n",
      "trying: The Assyrians of Armenia.nena\n",
      "\t√\n",
      "trying: The Assyrians of Urmi.nena\n",
      "\t√\n",
      "trying: The Bald Child and the Monsters.nena\n",
      "\t√\n",
      "trying: The Bald Man and the King.nena\n",
      "\t√\n",
      "trying: The Bird and the Fox.nena\n",
      "\t√\n",
      "trying: The Cat’s Dinner.nena\n",
      "\t√\n",
      "trying: The Cow and the Poor Girl.nena\n",
      "\t√\n",
      "trying: The Dead Rise and Return.nena\n",
      "\t√\n",
      "trying: The Fisherman and the Princess.nena\n",
      "\t√\n",
      "trying: The Giant One-Eyed Demon.nena\n",
      "\t√\n",
      "trying: The Little Prince and the Snake.nena\n",
      "\t√\n",
      "trying: The Loan of a Cooking Pot.nena\n",
      "\t√\n",
      "trying: The Man Who Wanted to Complain to God.nena\n",
      "\t√\n",
      "trying: The Old Man and the Fish.nena\n",
      "\t√\n",
      "trying: The Purchase of a Donkey.nena\n",
      "\t√\n",
      "trying: The Snake’s Dilemma.nena\n",
      "\t√\n",
      "trying: The Stupid Carpenter.nena\n",
      "\t√\n",
      "trying: The Wife Who Learns How to Work (2).nena\n",
      "\t√\n",
      "trying: The Wife Who Learns How to Work.nena\n",
      "\t√\n",
      "trying: The Wife’s Condition.nena\n",
      "\t√\n",
      "trying: The Wise Brother.nena\n",
      "\t√\n",
      "trying: The Wise Young Daughter.nena\n",
      "\t√\n",
      "trying: Trickster.nena\n",
      "\t√\n",
      "trying: Two Birds Fall in Love.nena\n",
      "\t√\n",
      "trying: Two Wicked Daughters-In-Law.nena\n",
      "\t√\n",
      "trying: Village Life (2).nena\n",
      "\t√\n",
      "trying: Village Life (3).nena\n",
      "\t√\n",
      "trying: Village Life (4).nena\n",
      "\t√\n",
      "trying: Village Life (5).nena\n",
      "\t√\n",
      "trying: Village Life (6).nena\n",
      "\t√\n",
      "trying: Village Life.nena\n",
      "\t√\n",
      "trying: Vineyards.nena\n",
      "\t√\n",
      "trying: Weddings and Festivals.nena\n",
      "\t√\n",
      "trying: Weddings.nena\n",
      "\t√\n",
      "trying: When Shall I Die?.nena\n",
      "\t√\n",
      "trying: Women Are Stronger Than Men.nena\n",
      "\t√\n",
      "trying: Women Do Things Best.nena\n",
      "\t√\n",
      "126 parsed...\n",
      "0 not parsed...\n"
     ]
    }
   ],
   "source": [
    "dialect2name2parsed = collections.defaultdict(dict)\n",
    "name2parsed = {}\n",
    "name2text = {}\n",
    "not_parsed = []\n",
    "\n",
    "ignore = [\n",
    "\n",
    "#    'Tales From the 1001 Nights.nena',\n",
    "#    'A Cure for a Husband’s Madness.nena',\n",
    "#    'A Dragon in the Well.nena',\n",
    "#     'A Dutiful Son.nena',\n",
    "#     'A Frog Wants a Husband.nena',\n",
    "#     'A Painting of the King of Iran.nena',\n",
    "#     'A Pound of Flesh.nena',\n",
    "#     'A Thousand Dinars.nena',\n",
    "#     'A Visit From Harun Ar-Rashid.nena',\n",
    "#     'Agriculture and Village Life.nena',\n",
    "]\n",
    "\n",
    "for dialect in dialect_dirs:\n",
    "    print(f'--Dialect {dialect}--')\n",
    "    print()\n",
    "    for file in sorted(dialect.glob('*.nena')):\n",
    "        \n",
    "        if file.name in ignore:\n",
    "            print('SKIPPING:', file.name, '\\n')\n",
    "            not_parsed.append(file)\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as infile:\n",
    "            text = infile.read()\n",
    "            name2text[file.name] = text\n",
    "            print(f'trying: {file.name}')\n",
    "            parseit = parser.parse(lexer.tokenize(text))\n",
    "            print(f'\\t√')\n",
    "            name2parsed[file.stem] = parseit\n",
    "            dialect2name2parsed[dialect.stem][file.stem] = parseit\n",
    "                \n",
    "print(len(name2parsed), 'parsed...')\n",
    "print(len(not_parsed), 'not parsed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name2text['Axiqar.nena'][22200-30:22200+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_token(test_str):\n",
    "#     norm_str = unicodedata.normalize('NFD', test_str)\n",
    "#     return list(lexer.tokenize(norm_str))\n",
    "\n",
    "# test_token('nux màlka.ˈ (4) ʾu-ʾímət ṛi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the parsed texts for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dir = Path('../parsed_texts/2.0/')\n",
    "parsed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#dialect2name2parsed['Urmi_C']['Trickster']\n",
    "\n",
    "for dialect, texts in dialect2name2parsed.items():\n",
    "    dialect_dir = parsed_dir.joinpath(dialect)\n",
    "    dialect_dir.mkdir(exist_ok=True)\n",
    "    for text, parsing in texts.items():\n",
    "        text_file = dialect_dir.joinpath(f'{text}.json')\n",
    "        with open(text_file, 'w') as outfile:\n",
    "            json.dump(parsing, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example = unicodedata.normalize('NFD', '''\n",
    "#dialect::Urmi_C\n",
    "#title::When Shall I Die?\n",
    "#encoding::UTF8\n",
    "#informant::Yulia Davudi\n",
    "#interviewer::Geoffrey Khan\n",
    "#place::+Hassar +Baba-čanɟa, N\n",
    "#transcriber::Geoffrey Khan\n",
    "#text_id::A32 \n",
    "#speakers::GK=Geoffrey Khan, CK=Cody Kingham, YD=Yulia Davudi\n",
    "\n",
    "#(1@0:00) xá-yuma \"⁺malla ⁺Nasrádən\" váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə\n",
    "#⁺vàrəva,ˈ mə́rrə ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P: bŏ́ro> \n",
    "#bàbaˈ ʾàtən=daˈ ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux\n",
    "#.ˈ ʾána ⁺šūl-ɟànilə.ˈ náplən nàplən.ˈ (2@0:08) ⁺hàlaˈ ʾo-náša léva xíša xá \n",
    "#⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ \n",
    "#⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ \n",
    "#bas-tánili xázən ʾána ʾíman bət-mètən.ˈ ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu\n",
    "#xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ mə̀drə,ˈ «GK: maxlèta?» ⁺rába \n",
    "#⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "#ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "#(3@0:16, CK) <E:Why hello there!> (4@0:18, YD) ⁺málla! múttəva ... ⁺ṱànaˈ\n",
    "#⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi ⁺rába múttəva ⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ \n",
    "#⁺báyyava ʾask̭áva ⁺ʾùllul.ˈʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ \n",
    "#ʾána dū́n k̭arbúnə k̭a-myàta.ˈ (4@0:20)<E:ok> «CK:yes?» xáč̭č̭a=da sə̀k̭laˈ xa-xìta.ˈ ɟánu mudməxxálə\n",
    "#⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ\n",
    "# xuyravàtuˈ xə́šlun tílun mə̀rrunˈ: ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə \n",
    "# xob-ʾána mìtən.ˈ lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun ʾat-xàya!ˈ \n",
    "# hamzùməvət.ˈ bəšvák̭una ⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺parmùyə.ˈ\n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class NenaLexer(Lexer):\n",
    "#    \n",
    "#    def error(self, t):\n",
    "#        \"\"\"Give warning for bad characters\"\"\"\n",
    "#        print(f\"Illegal character {repr(t.value[0])} @ index {self.index}\")\n",
    "#        self.index += 1\n",
    "#    \n",
    "#    # set of token names as required by\n",
    "#    # the Lexer class\n",
    "#    tokens = {\n",
    "#        LETTER, PUNCT_BEGIN, PUNCT_END, NEWLINES,\n",
    "#        NEWLINE, NEWLINES, ATTRIBUTE, \n",
    "#        FOREIGN_LETTER,\n",
    "#        LINESTAMP, SPAN_START, SPAN_END        \n",
    "#    }\n",
    "\n",
    "#    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "#    @_(r'[a-z0-9_]+ = .*')\n",
    "#    def ATTRIBUTE(self, t):\n",
    "#        field, value = tuple(t.value.split('='))\n",
    "#        t.value = {field.strip(): value.strip()}\n",
    "#        return t\n",
    "#    \n",
    "#    @_(r'\\(\\d+\\@\\d:\\d+\\)\\s*', \n",
    "#       r'\\(\\d+\\)\\s*')\n",
    "#    def LINESTAMP(self, t):\n",
    "#        number = re.findall('^\\((\\d+)', t.value)[0]\n",
    "#        timestamp = re.findall('@(\\d+:\\d+)', t.value)\n",
    "#        if timestamp:\n",
    "#            timestamp = timestamp[0]\n",
    "#        t.value = {'number': number, 'timestamp': timestamp}\n",
    "#        return t\n",
    "\n",
    "#    NEWLINES = r'\\n\\s*\\n\\s*' # i.e. marks text-blocks\n",
    "#    LETTER = alphabet_re    \n",
    "#    PUNCT_BEGIN = punct_begin_re\n",
    "#    PUNCT_END = punct_end_re\n",
    "#    NEWLINE = '\\n\\s*'\n",
    "#        \n",
    "#    # treat the language and speaker tag simultaneously as a \"span\"\n",
    "#    # this optimizes the code quite a bit since both tags\n",
    "#    # behave identically when they are parsed\n",
    "#    @_(r'[<«][A-Za-z?]+:\\s*')\n",
    "#    def SPAN_START(self, t):\n",
    "#        if t.value[0] == '<':\n",
    "#            kind = 'language'\n",
    "#            punct_type = 'exclusive'\n",
    "#        else:\n",
    "#            kind = 'speaker'\n",
    "#            punct_type = 'inclusive'\n",
    "#        value = re.match(r'[<«]([A-Za-z?]+):', t.value).group(1)\n",
    "#        tag = t.value.strip() + ' ' # ensure spacing\n",
    "#        t.value = (tag, kind, value, punct_type) # tag, key, value, punct_type\n",
    "#        return t\n",
    "#        \n",
    "#    SPAN_END = r'[>»]'\n",
    "#    \n",
    "#    # NB: tokens evaluated in order of appearance here\n",
    "#    # thus foreign string matched lastly\n",
    "#    FOREIGN_LETTER = r'[a-zA-ZðÐɟəƏɛƐʾʿθΘ][\\u0300-\\u033d]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def make_word(letters, beginnings=[], endings=[]):\n",
    "#    \"\"\"Return word dictionary\"\"\"\n",
    "#    return {\n",
    "#        'word': ''.join(letters),\n",
    "#        'letters': letters,\n",
    "#        'beginnings': beginnings,\n",
    "#        'endings': endings,\n",
    "#    }\n",
    "\n",
    "#def modify_attribute(words, key, value):\n",
    "#    \"\"\"Modify dict attribute for a list of words\"\"\"\n",
    "#    for word in words:\n",
    "#        word[key] = value\n",
    "#    return words\n",
    "\n",
    "#def format_tag_endings(tag, punct_value, endings=[]):\n",
    "#    \"\"\"Format punctuation around a tag.\n",
    "#    \n",
    "#    Normalizes in case of irregularity. For instance, in the\n",
    "#    cases of both\n",
    "#        words.</> \n",
    "#        words</>.\n",
    "#    the tags will be normalized to either an in/exclusive order.\n",
    "#    \"\"\"\n",
    "#    if punct_value == 'inclusive':\n",
    "#        return endings + [tag]\n",
    "#    elif punct_value == 'exclusive':\n",
    "#        return [tag] + endings\n",
    "#    else:\n",
    "#        raise Exception(f'INVALID punct_value supplied: {punct_value}')\n",
    "#    \n",
    "#class NenaParser(Parser):\n",
    "#    \n",
    "#    #debugfile = 'nena_parser.out'\n",
    "#    tokens = NenaLexer.tokens\n",
    "#    \n",
    "#    def error(self, t):\n",
    "#        raise Exception(f'unexpected {t.type} ({repr(t.value)}) at index {t.index}')\n",
    "#    \n",
    "#    @_('attributes NEWLINES text_block')\n",
    "#    def nena(self, p):\n",
    "#        return [p.attributes, p.text_block]\n",
    "#    \n",
    "#    @_('attributes NEWLINE ATTRIBUTE')\n",
    "#    def attributes(self, p):\n",
    "#        p.attributes.update(p.ATTRIBUTE)\n",
    "#        return p.attributes\n",
    "#    \n",
    "#    @_('NEWLINE ATTRIBUTE', 'ATTRIBUTE')\n",
    "#    def attributes(self, p):\n",
    "#        return p.ATTRIBUTE\n",
    "#    \n",
    "#    @_('text_block NEWLINES paragraph')\n",
    "#    def text_block(self, p):\n",
    "#        return p.text_block + [p.paragraph]\n",
    "#    \n",
    "#    @_('paragraph')\n",
    "#    def text_block(self, p):\n",
    "#        return [p.paragraph]\n",
    "#    \n",
    "#    @_('paragraph line')\n",
    "#    def paragraph(self, p):\n",
    "#        return p.paragraph + [p.line]\n",
    "#    \n",
    "#    @_('line')\n",
    "#    def paragraph(self, p):\n",
    "#        return [p.line]\n",
    "#    \n",
    "#    @_('LINESTAMP span words',\n",
    "#      'LINESTAMP span span words',\n",
    "#      'LINESTAMP span word span words')\n",
    "#    def line(self, p):\n",
    "#        words = []\n",
    "#        for wordtype in list(p)[1:]:\n",
    "#            if type(wordtype) == list: \n",
    "#                words += wordtype\n",
    "#            else:\n",
    "#                words.append(wordtype)\n",
    "#        p.LINESTAMP['words'] = words\n",
    "#        return p.LINESTAMP\n",
    "#    \n",
    "#    @_('LINESTAMP word span words')\n",
    "#    def line(self, p):\n",
    "#        p.LINESTAMP['words'] = [p.word] + p.span + p.words\n",
    "#        return p.LINESTAMP\n",
    "#    \n",
    "#    @_('LINESTAMP words')\n",
    "#    def line(self, p):\n",
    "#        p.LINESTAMP['words'] = p.words\n",
    "#        return p.LINESTAMP\n",
    "#    \n",
    "#    @_('words span')\n",
    "#    def words(self, p):\n",
    "#        return p.words + p.span\n",
    "#    \n",
    "#    @_('SPAN_START letters SPAN_END',\n",
    "#       'SPAN_START letters SPAN_END endings',\n",
    "#       'SPAN_START letters SPAN_END NEWLINE',\n",
    "#       'SPAN_START beginnings letters SPAN_END endings',\n",
    "#       'SPAN_START beginnings letters SPAN_END NEWLINE',\n",
    "#      )\n",
    "#    def span(self, p):\n",
    "#        begin_tag, kind, value, punct_type = p.SPAN_START\n",
    "#        beginnings = [begin_tag] + getattr(p, 'beginnings', [])\n",
    "#        \n",
    "#        # build ends\n",
    "#        trailing_ends = getattr(p, 'endings', [])\n",
    "#        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "#            trailing_ends.append(' ')\n",
    "#        endings = format_tag_endings(p.SPAN_END, punct_type, trailing_ends)\n",
    "#        \n",
    "#        word = make_word(p.letters, beginnings=beginnings, endings=endings)\n",
    "#        word[kind] = value\n",
    "#        return [word]\n",
    "#    \n",
    "#    @_('SPAN_START word letters SPAN_END',\n",
    "#       'SPAN_START word letters SPAN_END endings',\n",
    "#       'SPAN_START word letters SPAN_END NEWLINE',\n",
    "#       'SPAN_START word beginnings letters SPAN_END endings',\n",
    "#       'SPAN_START word beginnings letters SPAN_END NEWLINE',\n",
    "#       'SPAN_START words letters SPAN_END endings',\n",
    "#       'SPAN_START words letters SPAN_END NEWLINE',\n",
    "#       'SPAN_START words beginnings letters SPAN_END endings',\n",
    "#      )\n",
    "#    def span(self, p):\n",
    "#        begin_tag, kind, value, punct_type = p[0]\n",
    "#        \n",
    "#        # compile words\n",
    "#        words = []\n",
    "#        if getattr(p, 'word', None):\n",
    "#            p.word['beginnings'].insert(0, begin_tag)\n",
    "#            words.append(p.word)\n",
    "#        elif getattr(p, 'words', None):\n",
    "#            words.extend(p.words)\n",
    "#            \n",
    "#        # build new word from dangling letters and ends\n",
    "#        trailing_ends = getattr(p, 'endings', [])\n",
    "#        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "#            trailing_ends.append(' ')\n",
    "#        endings = format_tag_endings(p.SPAN_END, punct_type, trailing_ends)\n",
    "#        beginnings = getattr(p, 'beginnings', [])\n",
    "#        words.append(make_word(p.letters, beginnings=beginnings, endings=endings))\n",
    "#        \n",
    "#        return modify_attribute(words, kind, value)\n",
    "#    \n",
    "#    @_('SPAN_START words SPAN_END',\n",
    "#       'SPAN_START words SPAN_END endings',\n",
    "#       'SPAN_START words SPAN_END NEWLINE',\n",
    "#       'SPAN_START word SPAN_END',\n",
    "#       'SPAN_START word SPAN_END endings',\n",
    "#       'SPAN_START word SPAN_END NEWLINE',)\n",
    "#    def span(self, p):\n",
    "#        words = getattr(p, 'words', [p[1]])\n",
    "#        begin_tag, kind, value, punct_type = p[0]\n",
    "#        first_word, last_word = words[0], words[-1]\n",
    "#        first_word['beginnings'].insert(0, begin_tag)\n",
    "#        \n",
    "#        # build ends\n",
    "#        trailing_ends = last_word['endings'] + getattr(p, 'endings', [])\n",
    "#        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "#            trailing_ends.append(' ')        \n",
    "#        last_word['endings'] = format_tag_endings(p[2], punct_type, trailing_ends)\n",
    "#        \n",
    "#        return modify_attribute(words, kind, value)\n",
    "#    \n",
    "#    @_('words word')\n",
    "#    def words(self, p):\n",
    "#        return p.words + [p.word]\n",
    "#    \n",
    "#    @_('word word')\n",
    "#    def words(self, p):\n",
    "#        return [p[0]] + [p[1]]\n",
    "#    \n",
    "#    @_('beginnings letters endings', \n",
    "#       'letters endings',\n",
    "#       'letters NEWLINE',\n",
    "#       'letters NEWLINE endings',\n",
    "#       'beginnings letters NEWLINE',\n",
    "#       'beginnings letters NEWLINE endings',\n",
    "#      )\n",
    "#    def word(self, p):\n",
    "#        beginnings = getattr(p, 'beginnings', [])\n",
    "#        endings =  getattr(p, 'endings', [' '])\n",
    "#        return make_word(p.letters, beginnings, endings)\n",
    "\n",
    "#    @_('PUNCT_BEGIN beginnings')\n",
    "#    def beginnings(self, p):\n",
    "#        return [p.PUNCT_BEGIN] + p.beginnings\n",
    "#    \n",
    "#    @_('PUNCT_BEGIN')\n",
    "#    def beginnings(self, p):\n",
    "#        return [p.PUNCT_BEGIN]\n",
    "#    \n",
    "#    @_('endings NEWLINE')\n",
    "#    def endings(self, p):\n",
    "#        if p.endings[-1] != ' ':\n",
    "#            p.endings.append(' ')\n",
    "#        return p.endings\n",
    "#    \n",
    "#    @_('endings PUNCT_END')\n",
    "#    def endings(self, p):\n",
    "#        return p.endings + [p.PUNCT_END]\n",
    "#    \n",
    "#    @_('PUNCT_END')\n",
    "#    def endings(self, p):\n",
    "#        return [p.PUNCT_END]\n",
    "#        \n",
    "#    @_('LETTER letters', \n",
    "#       'FOREIGN_LETTER letters')\n",
    "#    def letters(self, p):\n",
    "#        return [p[0]] + p[1]\n",
    "#    \n",
    "#    @_('LETTER', \n",
    "#       'FOREIGN_LETTER')\n",
    "#    def letters(self, p):\n",
    "#        return [p[0]]\n",
    "\n",
    "#parser = NenaParser()\n",
    "#test = parser.parse(lexer.tokenize(example))\n",
    "##test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
