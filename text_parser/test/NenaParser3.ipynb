{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nena Parser 3\n",
    "## Pure Python Parser for NENA Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "VERSION = '2.0'\n",
    "PROJECT = Path.home().joinpath('github/CambridgeSemiticsLab/nena_corpus')\n",
    "STANDARDS = PROJECT.joinpath('standards')\n",
    "LEXICONS = STANDARDS.joinpath('lexicons')\n",
    "TEXT_INPUT = PROJECT.joinpath(f'texts/{VERSION}')\n",
    "JSON_OUTPUT = PROJECT.joinpath(f'parsed_texts/{VERSION}')\n",
    "\n",
    "# prepare alphabet and punctuation standards for processing\n",
    "alphabet_std = STANDARDS.joinpath('alphabet.json')\n",
    "punctuation_std = STANDARDS.joinpath('punctuation.json')\n",
    "lang_std = STANDARDS.joinpath('foreign_languages.json')\n",
    "\n",
    "with open(alphabet_std, 'r') as infile:\n",
    "    alphabet_data = json.load(infile)    \n",
    "with open(punctuation_std, 'r') as infile:    \n",
    "    punct_data = json.load(infile)\n",
    "with open(lang_std, 'r') as infile:\n",
    "    lang_data = json.load(infile)\n",
    "\n",
    "alphabet_re = '|'.join(let['decomposed_regex'] for let in alphabet_data)\n",
    "punct_begin_re = '|'.join(punct['decomposed_regex'] for punct in punct_data\n",
    "                            if punct['position'] == 'begin')\n",
    "\n",
    "punct_end_re = '|'.join(punct['decomposed_regex'] for punct in punct_data \n",
    "                            if punct['position'] == 'end')\n",
    "\n",
    "foreign_codes = '|'.join(lang['code'] for lang in lang_data)\n",
    "                               \n",
    "# prepare letter and punctuation data for \n",
    "# matching on a letter-by-letter basis\n",
    "metakeys = {\n",
    "    'punctuation': {'decomposed_string', 'class', 'position', 'modifies'},\n",
    "    'letter': {'decomposed_string', 'class', 'point', 'manner', 'phonation'},\n",
    "}\n",
    "char_metadata = collections.defaultdict(list)\n",
    "for item, data in [('letter', alphabet_data), ('punctuation', punct_data)]:\n",
    "    for chardata in data:\n",
    "        re_pattern = re.compile(chardata['decomposed_regex'])\n",
    "        metadata = {k:v for k,v in chardata.items() if k in metakeys[item]}\n",
    "        char_metadata[item].append((re_pattern, metadata))\n",
    "        \n",
    "        \n",
    "# prepare lexicon matching by dialect\n",
    "dialect2lexicon = collections.defaultdict()\n",
    "dialect2inflects = collections.defaultdict()\n",
    "\n",
    "# load lexicon and inflection data by dialect\n",
    "for dialect_dir in LEXICONS.glob('*'):\n",
    "    \n",
    "    # check paths and existence of relevant data\n",
    "    lexicon_file = dialect_dir.joinpath('lexicon.json')\n",
    "    inflects_file = dialect_dir.joinpath('inflections.json')\n",
    "    if not (lexicon_file.exists() and inflects_file.exists()):\n",
    "        continue\n",
    "        \n",
    "    # load and save files by dialect in dirs\n",
    "    dialect = dialect_dir.name\n",
    "    with open(lexicon_file, 'r') as infile:\n",
    "        dialect2lexicon[dialect] = json.load(infile)\n",
    "    with open(inflects_file, 'r') as infile:\n",
    "        dialect2inflects[dialect] = json.load(infile)\n",
    "        \n",
    "# exclude entry text from lexeme data for now\n",
    "# as it's too long; also exclude form string\n",
    "for dialect, lexs in dialect2lexicon.items():\n",
    "    for lex, ldata in lexs.items():\n",
    "        lexs[lex] = {\n",
    "            k:v for k,v in ldata.items()\n",
    "                if k not in {'entry', 'form'}\n",
    "        }\n",
    "        \n",
    "# compile regexes for lexicon matching\n",
    "for dialect, inflects in dialect2inflects.items():\n",
    "    for inflect in inflects:\n",
    "        inflect['form'] = re.compile(re.escape(inflect['form']))\n",
    "    \n",
    "def normalize_nena(word):\n",
    "    \"\"\"Strip vowel accents from NENA string.\"\"\"\n",
    "    accents = '\\u0300|\\u0301|\\u0304|\\u0306|\\u0308|\\u0303'\n",
    "    norm = unicodedata.normalize('NFD', word) # decompose for accent stripping\n",
    "    norm = re.sub(accents, '', norm) # strip accents\n",
    "    return norm\n",
    "    \n",
    "def parse_word(word, dialect):\n",
    "    \"\"\"Match a word with appropriate parsing data from lexicon.\"\"\"\n",
    "    \n",
    "    if dialect not in dialect2inflects:\n",
    "        return []\n",
    "    \n",
    "    word = normalize_nena(word)\n",
    "    \n",
    "    # find all matching surface forms\n",
    "    matches = []\n",
    "    for i, form in enumerate(dialect2inflects[dialect]):\n",
    "        if form['form'].fullmatch(word):\n",
    "            # filter out 'form' from data\n",
    "            form_data = {\n",
    "                k:v for k,v in form.items()\n",
    "                    if k != 'form'\n",
    "            }\n",
    "            matches.append(form_data)\n",
    "    \n",
    "    # retrieve default lexicon data\n",
    "    parsings = []\n",
    "    for match in matches:\n",
    "        lex_str = match['lex']\n",
    "        lex_data = dialect2lexicon[dialect][lex_str]\n",
    "        lex_data.update(match)\n",
    "        parsings.append(lex_data)\n",
    "        \n",
    "    return parsings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',|\\\\.\\\\.\\\\.|—|;|(?<!\\\\.)\\\\.(?!\\\\.)|\\\\?|!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_puncts = '|'.join(\n",
    "    p['decomposed_regex'] for p in punct_data\n",
    "        if p['modifies'] in {'sentence', 'subsentence'}\n",
    "        and p['class'] == 'separator'\n",
    "        and p['decomposed_regex'] != ':'\n",
    ")\n",
    "\n",
    "\n",
    "sent_puncts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'decomposed_regex': '⁺',\n",
       "  'decomposed_string': '⁺',\n",
       "  'class': 'phonetic',\n",
       "  'position': 'begin',\n",
       "  'modifies': 'stress group',\n",
       "  'codepoints': [8314]},\n",
       " {'decomposed_regex': '(?<= )\"',\n",
       "  'decomposed_string': '\"',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'text',\n",
       "  'position': 'begin',\n",
       "  'codepoints': [34]},\n",
       " {'decomposed_regex': ' ',\n",
       "  'decomposed_string': ' ',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'word',\n",
       "  'position': 'end',\n",
       "  'codepoints': [32]},\n",
       " {'decomposed_regex': '-',\n",
       "  'decomposed_string': '-',\n",
       "  'class': 'connector',\n",
       "  'modifies': 'stress group',\n",
       "  'position': 'end',\n",
       "  'codepoints': [45]},\n",
       " {'decomposed_regex': '=',\n",
       "  'decomposed_string': '=',\n",
       "  'class': 'connector',\n",
       "  'modifies': 'stress group',\n",
       "  'position': 'end',\n",
       "  'codepoints': [61]},\n",
       " {'decomposed_regex': 'ˈ',\n",
       "  'decomposed_string': 'ˈ',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'intonation group',\n",
       "  'position': 'end',\n",
       "  'codepoints': [712]},\n",
       " {'decomposed_regex': ',',\n",
       "  'decomposed_string': ',',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'subsentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [44]},\n",
       " {'decomposed_regex': '\\\\.\\\\.\\\\.',\n",
       "  'decomposed_string': '...',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'subsentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [46, 46, 46]},\n",
       " {'decomposed_regex': ':',\n",
       "  'decomposed_string': ':',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'subsentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [58]},\n",
       " {'decomposed_regex': '—',\n",
       "  'decomposed_string': '—',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'subsentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [8212]},\n",
       " {'decomposed_regex': ';',\n",
       "  'decomposed_string': ';',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'subsentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [59]},\n",
       " {'decomposed_regex': '(?<!\\\\.)\\\\.(?!\\\\.)',\n",
       "  'decomposed_string': '.',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'sentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [46]},\n",
       " {'decomposed_regex': '\\\\?',\n",
       "  'decomposed_string': '?',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'sentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [63]},\n",
       " {'decomposed_regex': '!',\n",
       "  'decomposed_string': '!',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'sentence',\n",
       "  'position': 'end',\n",
       "  'codepoints': [33]},\n",
       " {'decomposed_regex': '(?<! )\"',\n",
       "  'decomposed_string': '\"',\n",
       "  'class': 'separator',\n",
       "  'modifies': 'text',\n",
       "  'position': 'end',\n",
       "  'codepoints': [34]}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Text\n",
    "\n",
    "Below is a dummy text we can use to test the parsers on, written in the\n",
    "[NENA Markup](../docs/nena_format.md) standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unicodedata.normalize('NFD', '''\n",
    "dialect:: Urmi_C\n",
    "title:: When Shall I Die?\n",
    "encoding:: UTF8\n",
    "speakers:: YD=Yulia Davudi, GK=Geoffrey Khan, CK=Cody Kingham\n",
    "place:: +Hassar +Baba-čanɟa, N\n",
    "transcriber:: Geoffrey Khan\n",
    "text_id:: A32 \n",
    "\n",
    "(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə ⁺vàrəva,ˈ mə́rrə\n",
    " ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ\n",
    " ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ ʾána ⁺šūl-ɟànilə.ˈ\n",
    "náplən nàplən.ˈ (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla\n",
    "bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən\n",
    " ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ bas-tánili xázən ʾána ʾíman bət-mètən.ˈ\n",
    "ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ\n",
    "mə̀drə,ˈ (GK) maxlèta? (YD) ⁺rába ⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə \n",
    "⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "\n",
    "(3 0:14) ⁺málla múttəva ... ⁺ṱànaˈ ⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi ⁺rába múttəva \n",
    "⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ ⁺báyyava ʾask̭áva ⁺ʾùllul.ˈ ʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ\n",
    " ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ ʾána dū́n k̭arbúnə k̭a-myàta.ˈ (4 0:19) xáč̭č̭a=da sə̀k̭laˈ\n",
    "xa-xìta.ˈ (CK)<E:Hello this is a test> (YD)ɟánu mudməxxálə ⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ \n",
    "xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ xuyravàtuˈ xə́šlun tílun mə̀rrunˈ \n",
    "ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə xob-ʾána mìtən.ˈ lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun \n",
    "ʾat-xàya!ˈ hamzùməvət.ˈ bəšvák̭una ⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺p̭armùyə.ˈ\n",
    " ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NenaParser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # compile primary regex patterns\n",
    "        self.blocks = re.compile(r'\\n\\n')\n",
    "        \n",
    "    def parse(self, text, dialect):\n",
    "        \"\"\"Parse a NENA format text into structured, analyzed linguistic data.\n",
    "        \n",
    "        Args:\n",
    "            text: a string containing NENA Markup-formatted text.\n",
    "        \n",
    "        Returns:\n",
    "            List of structured, parsed data.\n",
    "        \"\"\"\n",
    "        metadata, text_block = self.get_blocks(text)\n",
    "        metadata_parsed = self.parse_metablock(metadata)\n",
    "        return metadata_parsed\n",
    "        \n",
    "    def get_blocks(self, text):\n",
    "        \"\"\"Split text into metadata and text blocks.\n",
    "        \n",
    "        Args:\n",
    "            text: a NENA markup string consisting of \n",
    "                a metadata block separated from a text block\n",
    "                by two newlines.\n",
    "                \n",
    "        Returns:\n",
    "            two-tuple of (metadata block, text block) strings\n",
    "        \"\"\"\n",
    "        return self.blocks.split(text, 1)\n",
    "\n",
    "    def parse_metablock(self, metablock):\n",
    "        \"\"\"Convert metadata text block into dictionary.\n",
    "\n",
    "        Args: \n",
    "            metablock: a multi-line string where each\n",
    "                line corresponds to a single metadata entry\n",
    "                for the text; e.g.\n",
    "                ```\n",
    "                dialect:: Barwar\n",
    "                title:: A Hundred Gold Coins\n",
    "                ```\n",
    "        Returns:\n",
    "            dictionary of metadata key, value pairs.\n",
    "        \"\"\"\n",
    "        metablock = metablock.strip()\n",
    "        attribs = metablock.split('\\n')\n",
    "        metadata = dict(self.parse_meta_attrib(entry) for entry in attribs)\n",
    "        return metadata\n",
    "\n",
    "    def parse_meta_attrib(self, meta_line):\n",
    "        \"\"\"Parse a line of metadata into a key, value pair.\n",
    "\n",
    "        Args:\n",
    "            meta_line: a string of metadata corresponding to\n",
    "                a single line in metadata block\n",
    "                e.g. \"dialect:: Barwar\"\n",
    "        Returns:\n",
    "            two-tuple of (key, value) for the attribute.\n",
    "        \"\"\"\n",
    "        key, value = meta_line.split('::')\n",
    "        key, value = key.strip(), value.strip()\n",
    "        if key == 'speakers':\n",
    "            value = self.parse_speakers_attrib(value)\n",
    "        return (key, value)\n",
    "    \n",
    "    def parse_speakers_attrib(self, speaker_line):\n",
    "        \"\"\"Parse a speaker attribute line into a dictionary.\n",
    "\n",
    "        Args: \n",
    "            speaker_line: a string containing speaker nickname assignments\n",
    "                e.g. \"GK=Geoffrey Khan, CK=Cody Kingham\"\n",
    "\n",
    "        Returns:\n",
    "            dict where each key is an initial and value is speaker's \n",
    "            name as a string.\n",
    "        \"\"\"\n",
    "        speakers = {}\n",
    "        for speakset in speaker_line.split(','):\n",
    "            initials, speaker = speakset.split('=')\n",
    "            initials, speaker = initials.strip(), speaker.strip()\n",
    "            speakers[initials] = speaker\n",
    "        return speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialect': 'Urmi_C',\n",
       " 'title': 'When Shall I Die?',\n",
       " 'encoding': 'UTF8',\n",
       " 'speakers': {'YD': 'Yulia Davudi',\n",
       "  'GK': 'Geoffrey Khan',\n",
       "  'CK': 'Cody Kingham'},\n",
       " 'place': '+Hassar +Baba-čanɟa, N',\n",
       " 'transcriber': 'Geoffrey Khan',\n",
       " 'text_id': 'A32'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = NenaParser()\n",
    "\n",
    "parser.parse(example, 'Urmi_C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '''\n",
    "(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə ⁺vàrəva,ˈ mə́rrə\n",
    " ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ\n",
    " ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ ʾána ⁺šūl-ɟànilə.ˈ\n",
    "náplən nàplən.ˈ (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla\n",
    "bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən\n",
    " ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ bas-tánili xázən ʾána ʾíman bət-mètən.ˈ\n",
    "ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ\n",
    "mə̀drə,ˈ (GK) maxlèta? (YD) ⁺rába ⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə \n",
    "⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "\n",
    "(3 0:14) ⁺málla múttəva ... ⁺ṱànaˈ ⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi ⁺rába múttəva \n",
    "⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ ⁺báyyava ʾask̭áva ⁺ʾùllul.ˈ ʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ\n",
    " ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ ʾána dū́n k̭arbúnə k̭a-myàta.ˈ (4 0:19) xáč̭č̭a=da sə̀k̭laˈ\n",
    "xa-xìta.ˈ (CK)<E:Hello this is a test> (YD)ɟánu mudməxxálə ⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ \n",
    "xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ xuyravàtuˈ xə́šlun tílun mə̀rrunˈ \n",
    "ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə xob-ʾána mìtən.ˈ lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun \n",
    "ʾat-xàya!ˈ hamzùməvət.ˈ bəšvák̭una ⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺p̭armùyə.ˈ\n",
    "'''\n",
    "\n",
    "test_paragraph = '''\\\n",
    "(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə ⁺vàrəva,ˈ mə́rrə\n",
    " ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ\n",
    " ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ ʾána ⁺šūl-ɟànilə.ˈ\n",
    "náplən nàplən.ˈ (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla\n",
    "bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən\n",
    " ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ bas-tánili xázən ʾána ʾíman bət-mètən.ˈ\n",
    "ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ\n",
    "mə̀drə,ˈ (GK) maxlèta? (YD) ⁺rába ⁺maxlèta. mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə \n",
    "⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ\\\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_textblock(textblock):\n",
    "    \"\"\"Parse a text block into structured data.\n",
    "    \n",
    "    Args:\n",
    "        textblock: a text block in NENA markup format.\n",
    "        \n",
    "    Returns:\n",
    "        List of metadata with parsings.\n",
    "    \"\"\"\n",
    "    paragraphs = split_paragraphs(textblock)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sentences = split_sentences(paragraphs)\n",
    "    \n",
    "def split_paragraphs(textblock):\n",
    "    \"\"\"Split text into paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        textblock: string with NENA markup text where paragraphs\n",
    "            are separated by double newlines.\n",
    "    \n",
    "    Returns:\n",
    "        list of strings where each string is a paragraph.\n",
    "    \"\"\"\n",
    "    return textblock.split('\\n\\n')\n",
    "\n",
    "def split_sentences(paragraph):\n",
    "    \"\"\"Splits off sentences based on punctuation.\n",
    "    \n",
    "    Args:\n",
    "        paragraphs: list of strings where each string corresponds\n",
    "            with a paragraph.\n",
    "            \n",
    "    Returns:\n",
    "        list of strings which correspond with sentences\n",
    "    \"\"\"\n",
    "    sent_mark = \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə ⁺vàrəva,ˈ mə́rrə\n",
      " ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ\n",
      " ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ ʾána ⁺šūl-ɟànilə.ˈ\n",
      "náplən nàplən.ˈ (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla\n",
      "bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən\n",
      " ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ bas-tánili xázən ʾána ʾíman bət-mètən.ˈ\n",
      "ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ\n",
      "mə̀drə,ˈ (GK) maxlèta? (YD) ⁺rába ⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə \n",
      "⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ\n"
     ]
    }
   ],
   "source": [
    "re.split(test_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ',\n",
       " ' xá mən-nášə ⁺vàrəva,ˈ',\n",
       " ' mə́rrə  ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ',\n",
       " ' bət-nàplət.ˈ',\n",
       " ' mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ  ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ',\n",
       " ' ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ',\n",
       " ' ʾána ⁺šūl-ɟànilə.ˈ',\n",
       " ' náplən nàplən.ˈ',\n",
       " ' (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ',\n",
       " ' ⁺málla bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ',\n",
       " ' bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ',\n",
       " ' màraˈ ⁺maxlèta,ˈ',\n",
       " ' ʾátən  ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ',\n",
       " ' bas-tánili xázən ʾána ʾíman bət-mètən.ˈ',\n",
       " ' ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ...',\n",
       " ' ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ',\n",
       " ' mə̀drə,ˈ',\n",
       " ' (GK) maxlèta?',\n",
       " ' (YD) ⁺rába ⁺maxlèta.',\n",
       " ' mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə  ⁺ʾarṱàla,ˈ',\n",
       " ' ʾó-yuma mètət.ˈ',\n",
       " ' ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ',\n",
       " ' ʾó-yuma mètət.ˈ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[''.join(sub_sent) for sub_sent in re.findall(f'(.*?)({sent_puncts})(ˈ)?', test_paragraph.replace('\\n', ' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1 0:00) <E:Ok> xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ',\n",
       " ' xá mən-nášə ⁺vàrəva,ˈ mə́rrə  ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ',\n",
       " ' mə́rrə <P:bŏ́ro> bàbaˈ ʾàtən=daˈ  ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ',\n",
       " ' ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ',\n",
       " ' ʾána ⁺šūl-ɟànilə.ˈ',\n",
       " ' náplən nàplən.ˈ',\n",
       " ' (2 0:08) ⁺hàlaˈ ʾo-náša léva xíša xá ⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ',\n",
       " ' bək̭yámələ ⁺bərxáṱələ ⁺bàru.ˈ',\n",
       " ' màraˈ ⁺maxlèta,ˈ ʾátən  ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ',\n",
       " ' bas-tánili xázən ʾána ʾíman bət-mètən.ˈ',\n",
       " ' ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ mə̀drə,ˈ (GK) maxlèta? (YD) ⁺rába ⁺maxlèta.',\n",
       " ' mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə  ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ',\n",
       " ' ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(f'.*?(?<!\\.)\\.(?!\\.)ˈ?', test_paragraph.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
