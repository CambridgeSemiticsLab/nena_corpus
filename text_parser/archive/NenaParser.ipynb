{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser: A parser for Nena Standard Text format\n",
    "\n",
    "The goal of this parser (under development) is to translate texts in the\n",
    "[Nena Standard Text format][nenamd] (we should find a better name for that)\n",
    "into structured groups of morphemes. Those structured morphemes can then be\n",
    "easily converted to (e.g.) TextFabric format.\n",
    "\n",
    "For the Nena Standard Text parser, we make use of [Sly][sly], a Python\n",
    "implementation of the lex/yacc type of parser generators. (This may soon have\n",
    "to be converted to Sly's predecessor [Ply][ply], as Sly works only with\n",
    "Python 3.6+ and the NENA website runs on Python 3.5 - but that should not be\n",
    "difficult).\n",
    "\n",
    "[nenamd]: https://github.com/CambridgeSemiticsLab/nena_corpus/blob/tomarkdown/docs/text_markup.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[ply]: http://www.dabeaz.com/ply/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPACE', '\\n'),\n",
       " ('TITLE', ('title', 'Gozáli and Nozali')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('ATTRIBUTE', ('text_id', 'A8')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('informant', 'Nanəs Bənyamən')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('place', 'ʾƐn-Nune')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '1'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('HYPHEN', '-'),\n",
       " ('+', '+'),\n",
       " ('LETTER', 'w'),\n",
       " ('LETTER', 'o'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '1'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('COMMENT', '(a-comment)'),\n",
       " ('SPACE', ' '),\n",
       " ('LPAREN_COMMENT', '(GK: '),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'b'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', ' '),\n",
       " ('/', '/'),\n",
       " ('/', '/'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('LETTER', 'b'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', '\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '2'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 's'),\n",
       " ('LETTER', 'o'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '2'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('*', '*'),\n",
       " ('LETTER', 'w'),\n",
       " ('LETTER', 'ó'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('LETTER', 's'),\n",
       " ('*', '*'),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '4'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'n'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'w'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'g'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'h'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('FOOTNOTE', (1, 'First footnote.')),\n",
       " ('SPACE', '\\n'),\n",
       " ('FOOTNOTE', (2, 'Second footnote.')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('FOOTNOTE', (3, 'Third footnote, not referenced in text.')),\n",
       " ('SPACE', '\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    # set of token names\n",
    "    tokens = {\n",
    "        TITLE, ATTRIBUTE, LETTER, NEWLINES, SPACE,\n",
    "        PUNCTUATION, HYPHEN,\n",
    "        LPAREN_COMMENT, LBRACKET_COMMENT, DIGITS,\n",
    "        LANG_MARKER, COMMENT, FOOTNOTE,\n",
    "    }\n",
    "    \n",
    "    literals = {'+', '*', '(', ')', '{', '}', '[', ']', '/', '^'}\n",
    "\n",
    "    # The '(?m)' part turns on multiline matching, which makes\n",
    "    # it possible to use ^ and $ for the start/end of the line.\n",
    "    # Title starts with pound sign. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^\\# .*$')\n",
    "    def TITLE(self, t):\n",
    "        t.value = ('title', t.value[2:])\n",
    "        return t\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^[a-z][a-z0-9_]+: .*$')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        t.value = tuple(t.value.split(': '))\n",
    "        return t\n",
    "    \n",
    "    # Footnote starts with '[^n]: ', where n is a number.\n",
    "    # Returns a 2-tuple (int: fn_sym, str: footnote_text)\n",
    "    @_(r'(?m)^\\[\\^[1-9][0-9]*\\]: .*$')\n",
    "    def FOOTNOTE(self, t):\n",
    "        fn_sym, footnote = t.value.split(maxsplit=1)\n",
    "        t.value = (int(fn_sym[2:-2]), footnote)\n",
    "        return t\n",
    "\n",
    "    # How to get combined Unicode characters to be recognized?\n",
    "    # Matching only Unicode points of letters with pre-combined\n",
    "    # marks can be done with the 'word' class '\\w', but it\n",
    "    # includes digits and underscore. To remove those, negate\n",
    "    # the inverted word class along with digits and underscore:\n",
    "    # '[^\\W\\d_]. But that does not include separate combining\n",
    "    # marks, or the '+' sign.\n",
    "    # One solution would be unicodedata.normalize('NFC', data),\n",
    "    # except that not all combinations have pre-combined Unicode\n",
    "    # points.\n",
    "    # Another solution is to use an external regex engine such as\n",
    "    # `regex` (`pip install regex`), which has better Unicode\n",
    "    # support. However, I would like to avoid extra dependencies.\n",
    "    # Another (less elegant) solution is to make the '+' symbol\n",
    "    # and the combining characters [\\u0300-\\u036F] each its own\n",
    "    # token, which the parser will have to parse into morphemes\n",
    "    # and words.\n",
    "    # Another (also less elegant) solution is to use a 'negative\n",
    "    # lookbehind assertion' for the negation of digits and '_':\n",
    "    # https://stackoverflow.com/a/12349464/9230612\n",
    "    # (?!\\d_)[\\w\\u0300-\\u036F]+\n",
    "    # Because combining marks can never appear before the first\n",
    "    # letter, and because some dialects have a '+' sign at the\n",
    "    # beginning of some words, we prefix an optional '+' symbol\n",
    "    # and an obligatory '[^\\W\\d_]' before the negative lookbehind.\n",
    "#     LETTERS = r'[+]?[^\\W\\d_](?!\\d_)[\\w\\u0300-\\u036F+]*'\n",
    "    # One letter with (or without) combining marks can be matched\n",
    "    # with: [^\\W\\d_][\\u0300-\\u036F]*\n",
    "    # Unfortunately, with python's `re` it seems impossible to repeat\n",
    "    # a group like this. So we will group the letters in the parser.\n",
    "    LETTER = r'[^\\W\\d_][\\u0300-\\u036F]*'\n",
    "    # Newlines: boundaries of paragraphs and metadata are marked\n",
    "    # with two newlines (meaning an empty line). The empty line\n",
    "    # may contain whitespace.\n",
    "    NEWLINES = r'\\n\\s*\\n\\s*'\n",
    "    # Space is any successive number of whitespace symbols.\n",
    "    SPACE = r'\\s+'\n",
    "    # One or more digits, not starting with zero\n",
    "    DIGITS = r'[1-9][0-9]*'\n",
    "    # Line id is any number of digits surrounded by round brackets\n",
    "#     LINE_ID = r'\\([0-9]+\\)'  # TODO convert to int?\n",
    "    # Punctuation is any normal punctuation symbol and vertical bar.\n",
    "    PUNCTUATION = r'[.,?!:;|–]'\n",
    "    # There are two different hyphens, a single one and a double one.\n",
    "    # The double one is the 'equals' sign.\n",
    "    HYPHEN = r'[-=]'\n",
    "    # Language markers are ASCII letter strings surrounded by\n",
    "    # angle brackets.\n",
    "    LANG_MARKER = r'<[A-Za-z]+>'\n",
    "    # A special comment starts with an opening bracket, capital initials\n",
    "    # and a colon.\n",
    "    LPAREN_COMMENT = r'\\([A-Za-z]+: '\n",
    "    LBRACKET_COMMENT = r'\\[[A-Za-z]+: '\n",
    "    # A regular comment is text (at least one character not being a digit)\n",
    "    # which may not contain a colon (otherwise it becomes a special comment/interruption)\n",
    "    COMMENT = r'\\([^:)]*[^:)\\d]+[^:)]*\\)'\n",
    "\n",
    "lexer_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-+word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "bla\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph.\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "[(tok.type, tok.value) for tok in lexer.tokenize(lexer_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "The text is going to be structured into paragraphs, lines, sentences, words, morphemes and characters. These text elements can have features, which are in the Nena Standard Text format indicated by markup (such as `*emphasis*`, `[^n] footnotes` and such. In general, the smallest element those features apply to is the `morpheme` (which, for our purposes, is defined as 'the smallest text element to which external features apply').\n",
    "\n",
    "### Morpheme class\n",
    "\n",
    "To conveniently store the morpheme and its features, we prepare a small `Morpheme` class, to be used by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morpheme:\n",
    "    \n",
    "    def __init__(self, value, trailer='',\n",
    "                 footnotes=None, speaker=None,\n",
    "                 foreign=False, lang=None):\n",
    "        self.value = value  # list of (combined) characters\n",
    "        self.trailer = trailer  # str (TODO: make this a list as well?)\n",
    "        self.footnotes = footnotes if footnotes is not None else {}  # dict\n",
    "        self.speaker = speaker  # str\n",
    "        self.foreign = foreign  # boolean\n",
    "        self.lang = lang  # str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ''.join(self.value)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        sp = f' speaker {self.speaker!r}' if self.speaker else ''\n",
    "        fr = ' foreign' if self.foreign else ''\n",
    "        ln = f' lang {self.lang!r}' if self.lang else ''\n",
    "        fn = f' fn_anc {\",\".join(str(n) for n in self.footnotes)!r}' if self.footnotes else ''\n",
    "        fn = f' fn_anc {self.footnotes!r}' if self.footnotes else ''\n",
    "        return f'<Morpheme {str(self)!r} trailer {self.trailer!r}{sp}{fr}{ln}{fn}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing, and in this case contains a tuple `(heading, paragraphs)`.\n",
    "\n",
    "The value `heading` contains a dictionary with the text metadata. The value `paragraphs` is a list, in which each element contains a list of `lines`. Each element of `lines` is a 2-tuple containing an `int` line identifier, and a list of `line_elements`. The values of `line_elements` are `Morpheme` objects, or 2-tuples with comments in the form `('comment', str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 17 shift/reduce conflicts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'title': 'Gozáli and Nozali',\n",
       "  'text_id': 'A8',\n",
       "  'informant': 'Nanəs Bənyamən',\n",
       "  'place': 'ʾƐn-Nune'},\n",
       " [[(1,\n",
       "    [<Morpheme 'a' trailer '-'>,\n",
       "     <Morpheme '+word' trailer '... ' fn_anc {1: 'First footnote.'}>,\n",
       "     ('comment', 'a-comment'),\n",
       "     <Morpheme 'lalala' trailer ' ' speaker 'GK'>,\n",
       "     <Morpheme 'bla' trailer ' //'>,\n",
       "     <Morpheme 'bla' trailer ' '>]),\n",
       "   (2,\n",
       "    [<Morpheme 'also' trailer ' ' fn_anc {2: 'Second footnote.'}>,\n",
       "     <Morpheme 'wórds' trailer '.' foreign lang 'E'>])],\n",
       "  [(4, [<Morpheme 'new' trailer ' '>, <Morpheme 'paragraph' trailer '.'>])],\n",
       "  ('footnotes', {3: 'Third footnote, not referenced in text.'})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Parser\n",
    "\n",
    "# dict stack to contain morphemes with footnote anchors,\n",
    "# until the corresponding footnote is encountered.\n",
    "fn_anchors = {}\n",
    "\n",
    "class NenaParser(Parser):\n",
    "#     debugfile = 'parser.out'\n",
    "\n",
    "    # Get the token list from the lexer (required)\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    @_('heading NEWLINES paragraphs')\n",
    "    def text(self, p):\n",
    "        return (p.heading, p.paragraphs)\n",
    "    \n",
    "    @_('SPACE TITLE NEWLINES attributes',\n",
    "       'TITLE NEWLINES attributes')\n",
    "    def heading(self, p):\n",
    "        key, value = p.TITLE\n",
    "        heading = {key: value}\n",
    "        heading.update(p.attributes)\n",
    "        return heading\n",
    "    \n",
    "    @_('attributes space ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        p.attributes[key] = value\n",
    "        return p.attributes\n",
    "            \n",
    "    @_('ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        return {key: value}\n",
    "    \n",
    "    @_('paragraphs NEWLINES paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        if p.paragraph is not None:\n",
    "            return p.paragraphs + [p.paragraph]\n",
    "        else:\n",
    "            return p.paragraphs\n",
    "    \n",
    "    @_('paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        return [p.paragraph]\n",
    "    \n",
    "    # paragraph\n",
    "    @_('paragraph line')\n",
    "    def paragraph(self, p):\n",
    "        return p.paragraph + [p.line]\n",
    "    \n",
    "    # paragraph\n",
    "    @_('footnotes')\n",
    "    def paragraph(self, p):\n",
    "        if p.footnotes:\n",
    "            # TODO: issue log warning about\n",
    "            # unreferenced footnotes?\n",
    "            return ('footnotes', p.footnotes)\n",
    "    \n",
    "    # footnotes\n",
    "    @_('footnotes footnote')\n",
    "    def footnotes(self, p):\n",
    "        p.footnotes.update(p.footnote)\n",
    "        return p.footnotes\n",
    "    \n",
    "    # footnotes\n",
    "    @_('footnote')\n",
    "    def footnotes(self, p):\n",
    "        return p.footnote\n",
    "    \n",
    "    @_('FOOTNOTE space NEWLINES',\n",
    "       'FOOTNOTE NEWLINES',\n",
    "       'FOOTNOTE space',\n",
    "       'FOOTNOTE')\n",
    "    def footnote(self, p):\n",
    "        fn_sym, fn_str = p.FOOTNOTE\n",
    "        footnote = {}\n",
    "        try:\n",
    "            # lookup the fn_sym key in the fn_anchors dict,\n",
    "            # and add the footnote to the appropriate morpheme\n",
    "            fn_morpheme = fn_anchors.pop(fn_sym)\n",
    "            fn_morpheme.footnotes[fn_sym] = fn_str\n",
    "        except KeyError:\n",
    "            # This means there is not footnote anchor\n",
    "            # referring to this footnote. So we return\n",
    "            # the footnote to the text\n",
    "            footnote = {fn_sym: fn_str}\n",
    "        return footnote\n",
    "\n",
    "    # lines\n",
    "    @_('line')\n",
    "    def paragraph(self, p):\n",
    "        return [p.line]\n",
    "    \n",
    "    # line\n",
    "    @_('line_id line_elements')\n",
    "    def line(self, p):\n",
    "        return (p.line_id, p.line_elements)\n",
    "    \n",
    "    # line_id\n",
    "    @_('\"(\" DIGITS \")\" SPACE')\n",
    "    def line_id(self, p):\n",
    "        return int(p.DIGITS)\n",
    "\n",
    "    @_('line_elements line_element',\n",
    "       'line_element')\n",
    "    def line_elements(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.line_elements + p.line_element\n",
    "        else:\n",
    "            return p.line_element\n",
    "    \n",
    "    @_('morphemes',\n",
    "       'interruption',\n",
    "       'morphemes_foreign',\n",
    "       'morphemes_language',\n",
    "       'comment')\n",
    "    def line_element(self, p):\n",
    "        return p[0]\n",
    "\n",
    "    # morphemes_language\n",
    "    @_('lang morphemes_foreign morpheme_trailer lang trailer',\n",
    "       'lang morphemes_foreign lang trailer',\n",
    "       'lang morphemes_foreign lang',\n",
    "       'lang morphemes_foreign')\n",
    "    def morphemes_language(self, p):\n",
    "        # check if language markers correspond\n",
    "        if len(p) > 2:\n",
    "            lang = p.lang0\n",
    "            if p.lang0 != p.lang1:\n",
    "                pass  # TODO issue warning: language markers do not correspond\n",
    "        else:\n",
    "            lang = p.lang  # TODO issue warning: missing second language marker\n",
    "        for m in p.morphemes_foreign:\n",
    "            m.lang = lang\n",
    "        if len(p) == 4:\n",
    "            p.morphemes_foreign[-1].trailer += p.trailer\n",
    "        elif len(p) == 5:\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "            p.morphemes_foreign.append(morpheme_trailer)\n",
    "        return p.morphemes_foreign\n",
    "    \n",
    "    # lang\n",
    "    @_('LANG_MARKER')\n",
    "    def lang(self, p):\n",
    "        return p.LANG_MARKER[1:-1]\n",
    "\n",
    "    # morphemes_foreign\n",
    "    # last morpheme may not include trailer\n",
    "    # add trailer after second asterisk to last morpheme\n",
    "    @_('\"*\" morphemes letters \"*\" trailer',\n",
    "       '\"*\" morphemes letters \"*\"',\n",
    "       '\"*\" letters \"*\" trailer',\n",
    "       '\"*\" letters \"*\"')\n",
    "    def morphemes_foreign(self, p):\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "        except KeyError:\n",
    "            trailer = ''\n",
    "        try:\n",
    "            morphemes = p.morphemes\n",
    "        except KeyError:\n",
    "            morphemes = []\n",
    "        morphemes.append(Morpheme(p.letters, trailer=trailer))\n",
    "        for m in morphemes:\n",
    "            m.foreign = True\n",
    "        return morphemes\n",
    "    \n",
    "    # comment\n",
    "    @_('COMMENT trailer',\n",
    "       'COMMENT')\n",
    "    def comment(self, p):\n",
    "        return [('comment', p.COMMENT[1:-1])]\n",
    "\n",
    "    # interruption\n",
    "    @_('LPAREN_COMMENT morphemes \")\" trailer',\n",
    "       'LPAREN_COMMENT morphemes \")\"',\n",
    "       'LBRACKET_COMMENT morphemes \"]\" trailer',\n",
    "       'LBRACKET_COMMENT morphemes \"]\"')\n",
    "    def interruption(self, p):\n",
    "        speaker = p[0][1:-2]\n",
    "        for m in p.morphemes:\n",
    "            m.speaker = speaker\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "            if (p.morphemes[-1].trailer.endswith(' ')\n",
    "                and trailer.startswith(' ')):\n",
    "                trailer = trailer[1:]\n",
    "            p.morphemes[-1].trailer += trailer\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return p.morphemes\n",
    "    \n",
    "    # morphemes\n",
    "    @_('morphemes morpheme_trailer',\n",
    "       'morpheme_trailer')\n",
    "    def morphemes(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.morphemes + [p.morpheme_trailer]\n",
    "        else:\n",
    "            return [p.morpheme_trailer]\n",
    "    \n",
    "    # morpheme_trailer\n",
    "    @_('letters trailer',\n",
    "       'letters HYPHEN',\n",
    "       'letters')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 2:\n",
    "            trailer = p[1]\n",
    "        else:\n",
    "            trailer = ''\n",
    "        return Morpheme(p.letters, trailer=trailer)\n",
    "\n",
    "    # morpheme_trailer with footnote anchor\n",
    "    @_('morpheme_trailer fn_anc trailer',\n",
    "       'morpheme_trailer fn_anc')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 3:\n",
    "            if (p.morpheme_trailer.trailer.endswith(' ')\n",
    "                and p.trailer.startswith(' ')):\n",
    "                p.trailer = p.trailer[1:]\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "        # add dummy value {fn_anc: None} to footnote dict\n",
    "        p.morpheme_trailer.footnotes[p.fn_anc] = None\n",
    "        # add morpheme object to fn_anchors dict,\n",
    "        # for easy access when footnote text is found\n",
    "        fn_anchors[p.fn_anc] = p.morpheme_trailer\n",
    "        return p.morpheme_trailer\n",
    "    \n",
    "    # fn_anc\n",
    "    @_('\"[\" \"^\" DIGITS \"]\"')\n",
    "    def fn_anc(self, p):\n",
    "        return int(p.DIGITS)\n",
    "    \n",
    "    @_('letters LETTER',\n",
    "       'LETTER',\n",
    "       '\"+\"')\n",
    "    def letters(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.letters + [p.LETTER]\n",
    "        else:\n",
    "            return [p[0]]\n",
    "    \n",
    "    # trailer\n",
    "    @_('trailer versebreak',\n",
    "       'trailer linebreak',\n",
    "       'trailer PUNCTUATION',\n",
    "       'trailer space',\n",
    "       'PUNCTUATION',\n",
    "       'space')\n",
    "    def trailer(self, p):\n",
    "        return ''.join(p)\n",
    "    \n",
    "    @_('SPACE')\n",
    "    def space(self, p):\n",
    "        return ' '\n",
    "    \n",
    "    @_('\"/\" \"/\"',\n",
    "       '\"/\" \"/\" space',\n",
    "       '\"/\" \"/\" NEWLINES',\n",
    "       '\"/\" \"/\" space NEWLINES')\n",
    "    def versebreak(self, p):\n",
    "        return '//'\n",
    "    \n",
    "    @_('\"/\"',\n",
    "       '\"/\" space',\n",
    "       '\"/\" NEWLINES',\n",
    "       '\"/\" space NEWLINES')\n",
    "    def linebreak(self, p):\n",
    "        return '/'\n",
    "    \n",
    "parser_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-+word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "bla\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph.\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\"\"\"\n",
    "\n",
    "# demonstration of output results of parser, to be used by generate_TF loop\n",
    "parser = NenaParser()\n",
    "parser.parse(lexer.tokenize(parser_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser output\n",
    "\n",
    "The parser prints a warning that there were shift/reduce conflicts, probably caused by ambiguous whitespace. That is not a problem (although not very elegant, ideally it should be fixed). The parser resolves the conflicts automatically.\n",
    "\n",
    "The output of the example text shows that the parser succeeded to parse it, and structure it into heading, paragraphs, lines and morphemes, with the features stored in the Morpheme object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TextFabric\n",
    "\n",
    "The output of the parser can be very easily converted to TextFabric format, as most elements are already separated and structured, which removes a lot of checking and matching from the conversion script, as it is done by the parser already. Some extra structuring, such as division into sentences, subsentences, prosaic units, and words, must be done in the conversion script, based on the contents of the `trailer` attribute.\n",
    "\n",
    "Below an (incomplete) example of a loop converting the parser output to something TextFabric can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-+word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "bla\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph.\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\"\"\"\n",
    "\n",
    "heading, paragraphs = parser.parse(lexer.tokenize(tf_test))\n",
    "\n",
    "# raw_features['title'][this_text] = heading['title']\n",
    "# ... etc.\n",
    "\n",
    "# initialize counters (will be increased to start from 1)\n",
    "this_paragraph = 0\n",
    "this_line = 0\n",
    "this_sentence = 0\n",
    "this_subsentence = 0\n",
    "this_word = 0\n",
    "this_morpheme = 0\n",
    "this_prosa = 0\n",
    "\n",
    "slot = 0 # i.e. chars\n",
    "\n",
    "# Mark units that are increased upon their 'ending' boundaryas 'ended',\n",
    "# so their counters will be increased on first morpheme\n",
    "sentence_end = True\n",
    "subsentence_end = True\n",
    "prosa_end = True\n",
    "word_end = True\n",
    "\n",
    "for p in paragraphs:\n",
    "    if type(p) is tuple:\n",
    "        # key, value = p  # unreferenced footnotes are passed as paragraph tuples\n",
    "        # and can be ignored\n",
    "        continue\n",
    "    this_paragraph += 1\n",
    "    for line_id, line in p:\n",
    "        this_line += 1\n",
    "        for m in line:\n",
    "            if type(m) is tuple:\n",
    "                # key, value = line_element  # comments are passed as tuples\n",
    "                # to be ignored?\n",
    "                continue\n",
    "            \n",
    "            # increase counters\n",
    "            this_morpheme += 1\n",
    "            \n",
    "            # increase counters of ended units\n",
    "            if sentence_end:\n",
    "                this_sentence += 1\n",
    "            if subsentence_end:\n",
    "                this_subsentence += 1\n",
    "            if prosa_end:\n",
    "                this_prosa += 1\n",
    "            if word_end:\n",
    "                this_word += 1\n",
    "            \n",
    "            for c in m.value:\n",
    "                slot += 1\n",
    "                \n",
    "                # add main character features:\n",
    "                # pretty_c = unicodedata.normalize('NFC', c)  # make pretty utf8 char text\n",
    "                # trans_c = translate(c, transcr_table)  # character in transcription\n",
    "                # raw_features['utf8'][slot] = pretty_c\n",
    "                # raw_features['trans'][slot] = trans_c\n",
    "                \n",
    "                # and other char features from Morpheme object `m`:\n",
    "                # if m.speaker:\n",
    "                #     raw_features['speaker'][slot] = m.speaker\n",
    "                # if m.foreign:\n",
    "                #     raw_features['language'][slot] = m.lang or ''\n",
    "            \n",
    "            # the last character of a `morpheme` gets its `trailer` and `footnotes`:\n",
    "            # raw_features['trailer'][slot] = m.trailer.replace('|', '\\u02c8')\n",
    "            # if any(m.footnotes.values()):\n",
    "            #     raw_features['footnotes'][slot] = '\\n'.join(m.footnotes.values())\n",
    "                \n",
    "            # check for unit ends\n",
    "            if (any(c in m.trailer for c in '.!?')\n",
    "                or m.trailer.endswith('//')):\n",
    "                sentence_end = True\n",
    "                subsentence_end = True\n",
    "            if (any(c in m.trailer for c in ',;:')\n",
    "                or m.trailer.endswith('/')):\n",
    "                subsentence_end = True\n",
    "            if '|' in m.trailer:\n",
    "                prosa_end = True\n",
    "            if m.trailer not in ('-', '=', ''):\n",
    "                word_end = True\n",
    "                # m.trailer == '' should only occur at end of paragraph.\n",
    "                # TODO issue a warning if it occurs elsewhere? (Better in parser?)\n",
    "                \n",
    "    # end of paragraph also ends sentence, subsentence, prosa, and word units\n",
    "    sentence_end = True\n",
    "    subsentence_end = True\n",
    "    prosa_end = True\n",
    "    word_end = True        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- [x] ~~implement 'foreign' marker `*`~~\n",
    "- [x] ~~implement language marker `<Marker>`~~\n",
    "- [x] ~~implement line and verse breaks `/` and `//`~~\n",
    "- [x] ~~implement footnotes~~\n",
    "\n",
    "ISSUES\n",
    "\n",
    "The parser does not enforce all parts of the grammar. For example, as verse and line breaks are just appended to the trailer, nothing will stop it from adding other trailer elements (save whitespace) after it. There is also no check to see whether the two `LANG_MARKER`s have the same value. A `+` sign must appear as the first character in a `morpheme`, but that just means that a `+` in the middle of a morpheme breaks it into two morphemes, instead of invalidating it. Undoubtedly there are more issues like this.\n",
    "\n",
    "Paragraphs in which the first line lacks a `line_id` break the parser. That is true for e.g. the first line of the text in which the `line_id` is absent, or for poetic style text with no `//` verse break marker but with empty line dividing verses. This could (should?) be handled by fixing the issue (default `line_id=1` for first line, default `//` verse break for empty lines within `line`), and issuing a warning notifying the user of the automatic fix.\n",
    "\n",
    "Footnotes can only be one line and the string is not processed (e.g. markup like `*emphasis*` is kept as is).\n",
    "\n",
    "Footnote anchors can now only occur after a `morpheme`, not after other things like `comment` or `interruption`. (note to self: possible solution: include `fn_anc` in `morphemes` instead of `morpheme_trailer`, and put `comment` in `trailer`).\n",
    "\n",
    "Comments and unreferenced footnotes are returned as tuples for now, and have to be filtered out in the loop.\n",
    "\n",
    "QUESTIONS\n",
    "\n",
    "Some questions require answers for implementation. They need not be definitive answers for now, but they should be motivated somehow (even if the motivation is 'random choice'), so it will be clear later why it is done in one way or another.\n",
    "\n",
    "- How to store hyphen? Now it is stored as a character in a word occuring between morphemes (I think).\n",
    "  \n",
    "  Should it be the trailer of the morpheme?\n",
    "\n",
    "\n",
    "- How to split sentences?\n",
    "\n",
    "  Now sentences are split on .?! and subsentences on ,\n",
    "  There are other symbols: ;:– and even .. ... ..., .... ..... (If I recall correctly). Should those split\n",
    "  sentences or subsentences?\n",
    "\n",
    "\n",
    "- What to do with poetic line breaks and sentence/paragraph boundaries?\n",
    "\n",
    "  I think a 'poem' should not be divided into paragraphs. I suggest that a line break '/' is a subsentence division, and a verse break '//' a sentence division (even when in the source it is followed by an empty line). If there is a verse number in between, that automatically starts a new sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
