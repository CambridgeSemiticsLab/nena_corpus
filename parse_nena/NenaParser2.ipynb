{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser: A parser for Nena Standard Text format\n",
    "\n",
    "The goal of this parser is to parse texts written in the plaintext\n",
    "[NENA markup format][nenamarkup] and deliver a structured\n",
    "list of words and their features, as well as paragraphing and line \n",
    "marks, which can be stored in a data format such as [Text-Fabric][textfabric], \n",
    "[Text-as-Graph][textasgraph] or (less optimally), XML or other hierarchical \n",
    "structures. \n",
    "\n",
    "For the Nena Markup parser, we make use of [Sly][sly], a Python implementation \n",
    "of the lex/yacc type of parser generators.\n",
    "\n",
    "[nenamarkup]: ../docs/nena_format.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[textfabric]: https://github.com/annotation/text-fabric\n",
    "[textasgraph]: https://www.balisage.net/Proceedings/vol19/print/Dekker01/BalisageVol19-Dekker01.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sly import Lexer\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Text\n",
    "\n",
    "Below is a dummy text we can use to test the parsers on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "\n",
    "dialect: Urmi_C\n",
    "title: When Shall I Die?\n",
    "encoding: UTF8\n",
    "informant: Yulia Davudi\n",
    "interviewer: Geoffrey Khan\n",
    "place: +Hassar +Baba-čanɟa, N\n",
    "transcriber: Geoffrey Khan\n",
    "text_id: A32 \n",
    "\n",
    "(1@0:00) xá-yuma ⁺malla ⁺Nasrádən váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə \n",
    "⁺vàrəva,ˈ mə́rrə ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P>bŏ́ro<P> \n",
    "bàbaˈ ʾàtən=daˈ ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ \n",
    "ʾána ⁺šūl-ɟànilə.ˈ náplən nàplən.ˈ (2@0:08) ⁺hàlaˈ ʾo-náša léva xíša xá \n",
    "⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ \n",
    "⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ \n",
    "bas-tánili xázən ʾána ʾíman bət-mètən.ˈ ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu \n",
    "xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ mə̀drə,ˈ \n",
    "<<Geoffrey Khan: maxlèta?>> ⁺rába ⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux \n",
    "⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ \n",
    "ʾó-yuma mètət.ˈ \n",
    "\n",
    "(3@0:16) ⁺málla múttəva ... ⁺ṱànaˈ ⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi \n",
    "⁺rába múttəva ⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ ⁺báyyava ʾask̭áva ⁺ʾùllul.ˈ\n",
    "ʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ ʾána dū́n\n",
    "k̭arbúnə k̭a-myàta.ˈ (4@0:20) xáč̭č̭a=da sə̀k̭laˈ xa-xìta.ˈ ɟánu mudməxxálə\n",
    "⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ\n",
    " xuyravàtuˈ xə́šlun tílun mə̀rrunˈ ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə \n",
    " xob-ʾána mìtən.ˈ lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun ʾat-xàya!ˈ \n",
    " hamzùməvət.ˈ bəšvák̭una ⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺p̂armùyə.ˈ\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a text with NENA Markup\n",
    "\n",
    "Below is a representation of the tree-like structure of a NENA standard text file. This is the structure that the parser must recognize and reproduce.\n",
    "\n",
    "`+` is used to represent one or more elements.\n",
    "\n",
    "```\n",
    "text\n",
    "  |\n",
    "  metadata block\n",
    "  |  |\n",
    "  |  +attribute\n",
    "  | \n",
    "  text block\n",
    "    |\n",
    "    +paragraph\n",
    "      |   \n",
    "      +line\n",
    "        |\n",
    "        +word\n",
    "          |\n",
    "          +letter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These items will be returned in the following Pythonic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('dialect', 'Urmi_C'), ('title', 'When Shall I Die?'), ('encoding', 'UTF8')],\n",
       " [[[('number', '1'),\n",
       "    ('timestamp', '0:00'),\n",
       "    ('words',\n",
       "     [{'text': 'xá',\n",
       "       'begin': '',\n",
       "       'end': '-',\n",
       "       'lang': 'NENA',\n",
       "       'letters': ('x', 'á')},\n",
       "      {'text': 'bŏ́ro',\n",
       "       'begin': '<P>',\n",
       "       'end': '<P> ',\n",
       "       'lang': 'P',\n",
       "       'letters': ('b', 'ŏ́', 'r', 'o')}])]]]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ # text\n",
    "    [ # metadata block\n",
    "        ('dialect', 'Urmi_C'),\n",
    "        ('title', 'When Shall I Die?'),\n",
    "        ('encoding', 'UTF8'),\n",
    "    ],\n",
    "    [ # text block\n",
    "        [ # paragraph\n",
    "            [ # line\n",
    "                ('number', '1'),\n",
    "                ('timestamp', '0:00'),\n",
    "                ('words', [\n",
    "                        { # word\n",
    "                            'text':'xá',\n",
    "                            'begin':'',\n",
    "                            'end':'-',\n",
    "                            'lang':'NENA', \n",
    "                            'letters':('x','á'),\n",
    "                        },\n",
    "                        # ...\n",
    "                        { # foreign word\n",
    "                            'text':'bŏ́ro',\n",
    "                            'begin':'<P>',\n",
    "                            'end': '<P> ',\n",
    "                            'lang': 'P', \n",
    "                            'letters':('b','ŏ́','r','o'),\n",
    "                        }, \n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_punct = '+' '\\u207A'\n",
    "end_punct = '.' ',' '?' '!' ':' ';' '–' '—' '\\-' '\\u02c8' '='\n",
    "letters = fr'[^\\W\\d_{end_punct}{begin_punct}][\\u0300-\\u036F]*'\n",
    "\n",
    "# The '(?m)' part turns on multiline matching, which makes\n",
    "# it possible to use ^ and $ for the start/end of the line.\n",
    "\n",
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    # set of token names\n",
    "    tokens = {\n",
    "        NEWLINE, NEWLINES, ATTRIBUTE, \n",
    "        BEGIN, LETTER, END,\n",
    "        LANG_MARKER, LINENUMBER,\n",
    "        TIMESTAMP, SPEAKER_START,\n",
    "        SPEAKER_END, SPACE,\n",
    "    }\n",
    "    \n",
    "    # Text blocks and paragraphs are marked off with 2 newlines\n",
    "    # spaces are allowed\n",
    "    NEWLINES = r'\\n\\s*\\n\\s*'\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'[a-z][a-z0-9_]+: .*')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        t.value = tuple(t.value.split(': '))\n",
    "        return t\n",
    "    \n",
    "    @_(r'\\(\\d+\\)', \n",
    "       r'\\(\\d+')\n",
    "    def LINENUMBER(self, t):\n",
    "        t.value = re.findall('\\d+', t.value).pop()\n",
    "        return t\n",
    "    \n",
    "    @_(r'\\@\\d:\\d+\\)')\n",
    "    def TIMESTAMP(self, t):\n",
    "        t.value = re.match(r'@(\\d:\\d+)\\)', t.value).group(1)\n",
    "        return t\n",
    "    \n",
    "    BEGIN = fr'[{begin_punct}]'\n",
    "    LETTER = letters\n",
    "    END = fr'[{end_punct}]'\n",
    "    NEWLINE = r'\\n'\n",
    "    SPACE = r'\\s'\n",
    "    \n",
    "    # Language markers are ASCII letter strings \n",
    "    # surrounded by angle brackets.\n",
    "    LANG_MARKER = r'<[A-Za-z]+>'\n",
    "    \n",
    "    @_(r'>>')\n",
    "    def SPEAKER_END(self, t):\n",
    "        return t\n",
    "\n",
    "    @_(r'<<[a-z\\sA-Z]+: ')\n",
    "    def SPEAKER_START(self, t):\n",
    "        t.value = re.match('<<([a-z\\sA-Z]+): ', t.value).group(1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "\n",
    "tokens = [(tok.type, tok.value) for tok in lexer.tokenize(example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NEWLINES', '\\n\\n'),\n",
       " ('ATTRIBUTE', ('dialect', 'Urmi_C')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('title', 'When Shall I Die?')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('encoding', 'UTF8')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('informant', 'Yulia Davudi')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('interviewer', 'Geoffrey Khan')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('place', '+Hassar +Baba-čanɟa, N')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('transcriber', 'Geoffrey Khan')),\n",
       " ('NEWLINE', '\\n'),\n",
       " ('ATTRIBUTE', ('text_id', 'A32 ')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('LINENUMBER', '1'),\n",
       " ('TIMESTAMP', '0:00'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'x'),\n",
       " ('LETTER', 'á'),\n",
       " ('END', '-'),\n",
       " ('LETTER', 'y'),\n",
       " ('LETTER', 'u'),\n",
       " ('LETTER', 'm'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', ' '),\n",
       " ('BEGIN', '⁺'),\n",
       " ('LETTER', 'm'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', ' '),\n",
       " ('BEGIN', '⁺'),\n",
       " ('LETTER', 'N'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 's'),\n",
       " ('LETTER', 'r')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Parser\n",
    "\n",
    "# dict stack to contain footnote anchors,\n",
    "# until the corresponding footnote is encountered.\n",
    "fn_anchors = {}\n",
    "\n",
    "class NenaParser(Parser):\n",
    "    \n",
    "    debugfile = 'parser.out'\n",
    "\n",
    "    # Get the token list from the lexer (required)\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of output results of parser, to be used by generate_TF loop\n",
    "parser = NenaParser()\n",
    "parser.parse(lexer.tokenize(parser_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser output\n",
    "\n",
    "The parser prints a warning that there were shift/reduce conflicts, probably caused by ambiguous whitespace. That is not a problem (although not very elegant, ideally it should be fixed). The parser resolves the conflicts automatically.\n",
    "\n",
    "The output of the example text shows that the parser succeeded to parse it, and structure it into heading, paragraphs, lines and morphemes, with the features stored in the Morpheme object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = Path('../nena/0.01')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parse On All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2parsed = {}\n",
    "name2text = {}\n",
    "not_parsed = []\n",
    "\n",
    "ignore = [\n",
    "    #'The Adventures Of Two Brothers.nena', # FIX BY MOVING UNEMPHASIZED OUT\n",
    "]\n",
    "\n",
    "for dialect in dialect_dirs:\n",
    "    print(f'--Dialect {dialect}--')\n",
    "    print()\n",
    "    for file in sorted(dialect.glob('*.nena')):\n",
    "        \n",
    "        if file.name in ignore:\n",
    "            print('SKIPPING:', file.name, '\\n')\n",
    "            not_parsed.append(file)\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as infile:\n",
    "            text = infile.read()\n",
    "            name2text[file.name] = text\n",
    "            print(f'trying: {file.name}')\n",
    "            parseit = parser.parse(lexer.tokenize(text))\n",
    "            print(f'\\t√')\n",
    "            name2parsed[file.name] = parseit\n",
    "                \n",
    "print(len(name2parsed), 'parsed...')\n",
    "print(len(not_parsed), 'not parsed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name2text['A Man Called Čuxo.nena'][7100:7120]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
