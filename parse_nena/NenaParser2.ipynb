{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser: A parser for Nena Standard Text format\n",
    "\n",
    "The goal of this parser is to parse texts written in the plaintext\n",
    "[NENA markup format][nenamarkup] and deliver a structured\n",
    "list of words and their features, as well as paragraphing and line \n",
    "marks, which can be stored in a data format such as [Text-Fabric][textfabric], \n",
    "[Text-as-Graph][textasgraph] or (less optimally), XML or other hierarchical \n",
    "structures. \n",
    "\n",
    "For the Nena Markup parser, we make use of [Sly][sly], a Python implementation \n",
    "of the lex/yacc type of parser generators.\n",
    "\n",
    "[nenamarkup]: ../docs/nena_format.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[textfabric]: https://github.com/annotation/text-fabric\n",
    "[textasgraph]: https://www.balisage.net/Proceedings/vol19/print/Dekker01/BalisageVol19-Dekker01.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from sly import Lexer, Parser\n",
    "import unicodedata\n",
    "from pprint import pprint\n",
    "\n",
    "# prepare alphabet and punctuation standards for processing\n",
    "alphabet_std = '../standards/alphabet.json'\n",
    "punctuation_std = '../standards/punctuation.json' \n",
    "lang_std = '../standards/foreign_languages.json'\n",
    "\n",
    "with open(alphabet_std, 'r') as infile:\n",
    "    alphabet_data = {\n",
    "        re.compile(data['decomposed_regex']):data \n",
    "            for data in json.load(infile)\n",
    "    }\n",
    "with open(punctuation_std, 'r') as infile:\n",
    "    punct_data = {\n",
    "        re.compile(data['regex']):data \n",
    "            for data in json.load(infile)\n",
    "    }\n",
    "with open(lang_std, 'r') as infile:\n",
    "    foreign_data = set(lang['code'] for lang in json.load(infile))\n",
    "    \n",
    "# compile regexes for matching\n",
    "alphabet_re = '|'.join(data['decomposed_regex'] for letter,data in alphabet_data.items())\n",
    "\n",
    "punct_begin_re = '|'.join(\n",
    "    data['regex'] for punct, data in punct_data.items()\n",
    "        if data['position'] == 'begin'\n",
    ")\n",
    "punct_end_re = '|'.join(\n",
    "    data['regex'] for punct, data in punct_data.items()\n",
    "        if data['position'] == 'end'\n",
    ")\n",
    "\n",
    "foreign_codes = '|'.join(foreign_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestLexer(Lexer):\n",
    "#     tokens = {LETTER, SPACE}\n",
    "#     LETTER = '[A-Za-z]'\n",
    "#     SPACE = '\\s'\n",
    "    \n",
    "# class TestParser(Parser):\n",
    "    \n",
    "#     debugfile = 'test_parser.out'\n",
    "#     tokens = TestLexer.tokens\n",
    "    \n",
    "#     @_('word words')\n",
    "#     def words(self, p):\n",
    "#         return [p.word] + p.words\n",
    "    \n",
    "#     @_('word SPACE word')\n",
    "#     def words(self, p):\n",
    "#         return [p.word]\n",
    "    \n",
    "#     @_('letters')\n",
    "#     def word(self, p):\n",
    "#         return {\n",
    "#             'letters': p.letters,\n",
    "#             'punct': '',\n",
    "#         }\n",
    "    \n",
    "#     @_('LETTER letters')\n",
    "#     def letters(self, p):\n",
    "#         return [p.LETTER] + p.letters \n",
    "    \n",
    "#     @_('LETTER')\n",
    "#     def letters(self, p):\n",
    "#         return [p[0]]\n",
    "    \n",
    "    \n",
    "# test_lexer = TestLexer()\n",
    "# test_parser = TestParser()\n",
    "# test_string = 'This is'\n",
    "# test_parser.parse(test_lexer.tokenize(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parser debugging for TestParser written to test_parser.out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'letters': ['T', 'h', 'i', 's'], 'punc': (' ',)},\n",
       " {'letters': ['i', 's'], 'punc': (' ',)},\n",
       " {'letters': ['a'], 'punc': (' ',)},\n",
       " {'letters': ['t', 'e', 's', 't']}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLexer(Lexer):\n",
    "    tokens = {LETTER, SPACE}\n",
    "    LETTER = '[A-Za-z]'\n",
    "    SPACE = '\\s'\n",
    "    \n",
    "    @_('\\s')\n",
    "    def SPACE(self, t):\n",
    "        t.value = (t.value,)\n",
    "        return t\n",
    "    \n",
    "class TestParser(Parser):\n",
    "    debugfile = 'test_parser.out'\n",
    "    tokens = TestLexer.tokens\n",
    "    \n",
    "    @_('word SPACE words')\n",
    "    def words(self, p):\n",
    "        p.word.update({\"punc\": p.SPACE})\n",
    "        return [p.word] + p.words\n",
    "    \n",
    "    @_('word')\n",
    "    def words(self, p):\n",
    "        return [p.word]\n",
    "    \n",
    "    @_('letters')\n",
    "    def word(self, p):\n",
    "        return {\"letters\":  p.letters}\n",
    "    \n",
    "    @_('LETTER letters')\n",
    "    def letters(self, p):\n",
    "        return [p.LETTER] + p.letters \n",
    "    \n",
    "    @_('LETTER')\n",
    "    def letters(self, p):\n",
    "        return [p[0]]\n",
    "    \n",
    "test_lexer = TestLexer()\n",
    "test_parser = TestParser()\n",
    "test_string = 'This is a test'\n",
    "test_parser.parse(test_lexer.tokenize(test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Text\n",
    "\n",
    "Below is a dummy text we can use to test the parsers on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = unicodedata.normalize('NFD', '''\n",
    "dialect: Urmi_C\n",
    "title: When Shall I Die?\n",
    "encoding: UTF8\n",
    "informant: Yulia Davudi\n",
    "interviewer: Geoffrey Khan\n",
    "place: +Hassar +Baba-čanɟa, N\n",
    "transcriber: Geoffrey Khan\n",
    "text_id: A32 \n",
    "\n",
    "(1@0:00) xá-yuma \"⁺malla ⁺Nasrádən\" váyələ tíva ⁺ʾal-k̭èsa.ˈ xá mən-nášə \n",
    "⁺vàrəva,ˈ mə́rrə ⁺màllaˈ ʾátən ʾo-k̭ésa pràmut,ˈ bət-nàplət.ˈ mə́rrə <P: bŏ́ro> \n",
    "bàbaˈ ʾàtən=daˈ ⁺šúla lə̀tluxˈ tíyyət b-dìyyi k̭ítət.ˈ ⁺šúk̭ si-⁺bar-⁺šùlux.ˈ \n",
    "ʾána ⁺šūl-ɟànilə.ˈ náplən nàplən.ˈ (2@0:08) ⁺hàlaˈ ʾo-náša léva xíša xá \n",
    "⁺ʾəsrá ⁺pasulyày,ˈ ⁺málla bitáyələ drúm ⁺ʾal-⁺ʾàrra.ˈ bək̭yámələ ⁺bərxáṱələ \n",
    "⁺bàru.ˈ màraˈ ⁺maxlèta,ˈ ʾátən ⁺dílux ʾána bət-náplənva m-⁺al-ʾilàna.ˈ \n",
    "bas-tánili xázən ʾána ʾíman bət-mètən.ˈ ʾo-náša xzílə k̭at-ʾá ⁺màllaˈ hónu \n",
    "xáč̭č̭a ... ⁺basùrələˈ mə́rrə k̭àtuˈ ⁺maxlèta,ˈ mə̀drə,ˈ «GK: maxlèta?» ⁺rába \n",
    "⁺maxlèta.ˈ mə́rrə k̭at-ʾíman xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "ʾó-yumət xmártux ⁺ṱlá ɟáhə ⁺ʾarṱàla,ˈ ʾó-yuma mètət.ˈ \n",
    "\n",
    "(3@0:16) ⁺málla múttəva ... ⁺ṱànaˈ ⁺yak̭úyra ⁺ʾal-xmàrta.ˈ ⁺ṱànaˈ mə́ndi \n",
    "⁺rába múttəva ⁺ʾal-xmàrtaˈ ʾu-xmàrtaˈ ⁺báyyava ʾask̭áva ⁺ʾùllul.ˈ\n",
    "ʾu-bas-pòxa ⁺plə́ṱlə mənnó.ˈ ṱə̀r,ˈ ⁺riṱàla.ˈ ⁺málla mə́rrə ʾàha,ˈ ʾána dū́n\n",
    "k̭arbúnə k̭a-myàta.ˈ (4@0:20) xáč̭č̭a=da sə̀k̭laˈ xa-xìta.ˈ ɟánu mudməxxálə\n",
    "⁺ʾal-⁺ʾàrra.ˈ mə̀rrəˈ xína ⁺dā́n mòtila.ˈ ʾē=t-d-⁺ṱlàˈ ⁺málla mə̀tlə.ˈ nàšə,ˈ\n",
    " xuyravàtuˈ xə́šlun tílun mə̀rrunˈ ʾa mù-vadət? k̭a-mú=ivət ⁺tàmma?ˈ mə́rrə \n",
    " xob-ʾána mìtən.ˈ lá bəxzáyətun k̭at-mìtən!ˈ lá mə́rrun ʾat-xàya!ˈ \n",
    " hamzùməvət.ˈ bəšvák̭una ⁺tàmaˈ màraˈ xmàrələ,ˈ lélə ⁺parmùyə.ˈ\n",
    " ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a text with NENA Markup\n",
    "\n",
    "Below is a representation of the tree-like structure of a NENA standard text file. This is the structure that the parser must recognize and reproduce.\n",
    "\n",
    "`+` is used to represent one or more elements.\n",
    "\n",
    "```\n",
    "text\n",
    "  |\n",
    "  metadata block\n",
    "  |  |\n",
    "  |  +attribute\n",
    "  | \n",
    "  text block\n",
    "    |\n",
    "    +paragraph\n",
    "      |   \n",
    "      +line\n",
    "        |\n",
    "        +word\n",
    "          |\n",
    "          +letter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These items will be returned in the following Pythonic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = [ # text\n",
    "    [ # metadata block\n",
    "        {\n",
    "            'dialect': 'Urmi_C',\n",
    "            'title': 'When Shall I Die?',\n",
    "            'encoding': 'UTF8',\n",
    "        }\n",
    "    ],\n",
    "    [ # text block\n",
    "        [ # paragraph\n",
    "            { # line\n",
    "                'number': '1', \n",
    "                'timestamp': '0:00',\n",
    "                'words': [\n",
    "                    { # word\n",
    "                        'text':'xá',\n",
    "                        'begin':'',\n",
    "                        'end':'-',\n",
    "                        'lang':'NENA', \n",
    "                        'letters':('x','á'),\n",
    "                    },\n",
    "                    # ...\n",
    "                    { # foreign word\n",
    "                        'text':'bŏ́ro',\n",
    "                        'begin':'<P:',\n",
    "                        'end': '> ',\n",
    "                        'lang': 'P', \n",
    "                        'letters':('b','ŏ́','r','o'),\n",
    "                    }, \n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    def error(self, t):\n",
    "        \"\"\"Give warning for bad characters\"\"\"\n",
    "        print(f\"Illegal character {repr(t.value[0])} @ index {self.index}\")\n",
    "        self.index += 1\n",
    "    \n",
    "    # set of token names as required by\n",
    "    # the Lexer class\n",
    "    tokens = {\n",
    "        LETTER, PUNCT_BEGIN, PUNCT_END, NEWLINES,\n",
    "        NEWLINE, NEWLINES, ATTRIBUTE, \n",
    "        FOREIGN_LETTER,\n",
    "        LINESTAMP, SPAN_START, SPAN_END        \n",
    "    }\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'[a-z][a-z0-9_]+: .*')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        field, value = tuple(t.value.split(': '))\n",
    "        t.value = {field.strip(): value.strip()}\n",
    "        return t\n",
    "    \n",
    "    @_(r'\\(\\d+\\@\\d:\\d+\\)\\s*', \n",
    "       r'\\(\\d+\\)\\s*')\n",
    "    def LINESTAMP(self, t):\n",
    "        number = re.findall('^\\((\\d+)', t.value)[0]\n",
    "        timestamp = re.findall('@(\\d+:\\d+)', t.value)\n",
    "        if timestamp:\n",
    "            timestamp = timestamp[0]\n",
    "        t.value = {'number': number, 'timestamp': timestamp}\n",
    "        return t\n",
    "\n",
    "    NEWLINES = r'\\n\\s*\\n\\s*' # i.e. marks text-blocks\n",
    "    LETTER = alphabet_re    \n",
    "    PUNCT_BEGIN = punct_begin_re\n",
    "    PUNCT_END = punct_end_re\n",
    "    NEWLINE = '\\n\\s*'\n",
    "        \n",
    "    # treat the language and speaker tag simultaneously as a \"span\"\n",
    "    # this optimizes the code quite a bit since both tags\n",
    "    # behave identically when they are parsed\n",
    "    @_(r'[<«][A-Za-z]+:\\s*')\n",
    "    def SPAN_START(self, t):\n",
    "        if t.value[0] == '<':\n",
    "            kind = 'language'\n",
    "            punct_type = 'exclusive'\n",
    "        else:\n",
    "            kind = 'speaker'\n",
    "            punct_type = 'inclusive'\n",
    "        value = re.match(r'[<«]([A-Za-z]+):', t.value).group(1)\n",
    "        tag = t.value.strip() + ' ' # ensure spacing\n",
    "        t.value = (tag, kind, value, punct_type) # tag, key, value, punct_type\n",
    "        return t\n",
    "        \n",
    "    SPAN_END = r'[>»]'\n",
    "    \n",
    "    # NB: tokens evaluated in order of appearance here\n",
    "    # thus foreign string matched lastly\n",
    "    FOREIGN_LETTER = r'[a-zA-ZðÐɟəƏɛƐʾʿθΘ][\\u0300-\\u033d]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "tokens = [(tok.type, tok.value) for tok in lexer.tokenize(example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'dialect': 'Urmi_C'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'title': 'When Shall I Die?'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'encoding': 'UTF8'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'informant': 'Yulia Davudi'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'interviewer': 'Geoffrey Khan'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'place': '+Hassar +Baba-čanɟa, N'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'transcriber': 'Geoffrey Khan'}),\n",
      " ('NEWLINE', '\\n'),\n",
      " ('ATTRIBUTE', {'text_id': 'A32'}),\n",
      " ('NEWLINES', '\\n\\n'),\n",
      " ('LINESTAMP', {'number': '1', 'timestamp': '0:00'}),\n",
      " ('LETTER', 'x'),\n",
      " ('LETTER', 'á'),\n",
      " ('PUNCT_END', '-'),\n",
      " ('LETTER', 'y'),\n",
      " ('LETTER', 'u'),\n",
      " ('LETTER', 'm'),\n",
      " ('LETTER', 'a'),\n",
      " ('PUNCT_END', ' '),\n",
      " ('PUNCT_BEGIN', '\"'),\n",
      " ('PUNCT_BEGIN', '⁺'),\n",
      " ('LETTER', 'm'),\n",
      " ('LETTER', 'a')]\n"
     ]
    }
   ],
   "source": [
    "pprint(tokens[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word(letters, beginnings=[], endings=[]):\n",
    "    \"\"\"Return word dictionary\"\"\"\n",
    "    return {\n",
    "        'word': ''.join(letters),\n",
    "        'letters': letters,\n",
    "        'beginnings': beginnings,\n",
    "        'endings': endings,\n",
    "    }\n",
    "\n",
    "def modify_attribute(words, key, value):\n",
    "    \"\"\"Modify dict attribute for a list of words\"\"\"\n",
    "    for word in words:\n",
    "        word[key] = value\n",
    "    return words\n",
    "\n",
    "def format_tag_endings(tag, punct_value, endings=[]):\n",
    "    \"\"\"Format punctuation around a tag.\n",
    "    \n",
    "    Normalizes in case of irregularity. For instance, in the\n",
    "    cases of both\n",
    "        words.</> \n",
    "        words</>.\n",
    "    the tags will be normalized to either an in/exclusive order.\n",
    "    \"\"\"\n",
    "    if punct_value == 'inclusive':\n",
    "        return endings + [tag]\n",
    "    elif punct_value == 'exclusive':\n",
    "        return [tag] + endings\n",
    "    else:\n",
    "        raise Exception(f'INVALID punct_value supplied: {punct_value}')\n",
    "    \n",
    "class NenaParser(Parser):\n",
    "    \n",
    "    #debugfile = 'nena_parser.out'\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    def error(self, t):\n",
    "        raise Exception(f'unexpected {t.type} ({repr(t.value[0])}) at index {t.index}')\n",
    "    \n",
    "    @_('attributes NEWLINES text_block')\n",
    "    def nena(self, p):\n",
    "        return [p.attributes, p.text_block]\n",
    "    \n",
    "    @_('attributes NEWLINE ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        p.attributes.update(p.ATTRIBUTE)\n",
    "        return p.attributes\n",
    "    \n",
    "    @_('NEWLINE ATTRIBUTE', 'ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        return p.ATTRIBUTE\n",
    "    \n",
    "    @_('text_block NEWLINES paragraph')\n",
    "    def text_block(self, p):\n",
    "        return p.text_block + [p.paragraph]\n",
    "    \n",
    "    @_('paragraph')\n",
    "    def text_block(self, p):\n",
    "        return [p.paragraph]\n",
    "    \n",
    "    @_('paragraph line')\n",
    "    def paragraph(self, p):\n",
    "        return p.paragraph + [p.line]\n",
    "    \n",
    "    @_('line')\n",
    "    def paragraph(self, p):\n",
    "        return [p.line]\n",
    "    \n",
    "    @_('LINESTAMP words')\n",
    "    def line(self, p):\n",
    "        p.LINESTAMP['words'] = p.words\n",
    "        return p.LINESTAMP\n",
    "        \n",
    "    @_('words span')\n",
    "    def words(self, p):\n",
    "        return p.words + p.span\n",
    "    \n",
    "    @_('SPAN_START letters SPAN_END',\n",
    "       'SPAN_START letters SPAN_END endings',\n",
    "       'SPAN_START letters SPAN_END NEWLINE',)\n",
    "    def span(self, p):\n",
    "        begin_tag, kind, value, punct_type = p[0]\n",
    "        beginnings = begin_tag\n",
    "        \n",
    "        # build ends\n",
    "        trailing_ends = getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')\n",
    "        endings = format_tag_endings(p[2], punct_type, trailing_ends)\n",
    "        \n",
    "        word = make_word(p.letters, beginnings=beginnings, endings=endings)\n",
    "        word[kind] = value\n",
    "        return [word]\n",
    "    \n",
    "    @_('SPAN_START word letters SPAN_END',\n",
    "       'SPAN_START word letters SPAN_END endings',\n",
    "       'SPAN_START word letters SPAN_END NEWLINE',)\n",
    "    def span(self, p):\n",
    "        begin_tag, kind, value, punct_type = p[0]        \n",
    "        p.word['beginnings'].insert(0, begin_tag)\n",
    "        \n",
    "        # build ends\n",
    "        trailing_ends = getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')\n",
    "        endings = format_tag_endings(p[3], punct_type, trailing_ends)\n",
    "        \n",
    "        new_word = make_word(p.letters, endings=endings)\n",
    "        return modify_attribute([p.word, new_word], kind, value)\n",
    "    \n",
    "    @_('SPAN_START words SPAN_END',\n",
    "       'SPAN_START words SPAN_END endings',\n",
    "       'SPAN_START words SPAN_END NEWLINE',\n",
    "       'SPAN_START word SPAN_END',\n",
    "       'SPAN_START word SPAN_END endings',\n",
    "       'SPAN_START word SPAN_END NEWLINE',)\n",
    "    def span(self, p):\n",
    "        words = getattr(p, 'words', [p[1]])\n",
    "        begin_tag, kind, value, punct_type = p[0]\n",
    "        first_word, last_word = words[0], words[-1]\n",
    "        first_word['beginnings'].insert(0, begin_tag)\n",
    "        \n",
    "        # build ends\n",
    "        trailing_ends = last_word['endings'] + getattr(p, 'endings', [])\n",
    "        if getattr(p, 'NEWLINE', '') and not ''.join(trailing_ends).endswith(' '):\n",
    "            trailing_ends.append(' ')        \n",
    "        last_word['endings'] = format_tag_endings(p[2], punct_type, trailing_ends)\n",
    "        \n",
    "        return modify_attribute(words, kind, value)\n",
    "    \n",
    "    @_('words word')\n",
    "    def words(self, p):\n",
    "        return p.words + [p.word]\n",
    "    \n",
    "    @_('word word')\n",
    "    def words(self, p):\n",
    "        return [p[0]] + [p[1]]\n",
    "    \n",
    "    @_('beginnings letters endings', \n",
    "       'letters endings',\n",
    "       'letters NEWLINE',)\n",
    "    def word(self, p):\n",
    "        beginnings = getattr(p, 'beginnings', [])\n",
    "        endings =  getattr(p, 'endings', [' '])\n",
    "        return make_word(p.letters, beginnings, endings)\n",
    "\n",
    "    @_('PUNCT_BEGIN beginnings')\n",
    "    def beginnings(self, p):\n",
    "        return [p.PUNCT_BEGIN] + p.beginnings\n",
    "    \n",
    "    @_('PUNCT_BEGIN')\n",
    "    def beginnings(self, p):\n",
    "        return [p.PUNCT_BEGIN]\n",
    "    \n",
    "    @_('endings NEWLINE')\n",
    "    def endings(self, p):\n",
    "        if p.endings[-1] != ' ':\n",
    "            p.endings.append(' ')\n",
    "        return p.endings\n",
    "    \n",
    "    @_('endings PUNCT_END')\n",
    "    def endings(self, p):\n",
    "        return p.endings + [p.PUNCT_END]\n",
    "    \n",
    "    @_('PUNCT_END')\n",
    "    def endings(self, p):\n",
    "        return [p.PUNCT_END]\n",
    "        \n",
    "    @_('LETTER letters', \n",
    "       'FOREIGN_LETTER letters')\n",
    "    def letters(self, p):\n",
    "        return [p[0]] + p[1]\n",
    "    \n",
    "    @_('LETTER', \n",
    "       'FOREIGN_LETTER')\n",
    "    def letters(self, p):\n",
    "        return [p[0]]\n",
    "\n",
    "parser = NenaParser()\n",
    "test = parser.parse(lexer.tokenize(example))\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, text = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialect': 'Urmi_C',\n",
       " 'title': 'When Shall I Die?',\n",
       " 'encoding': 'UTF8',\n",
       " 'informant': 'Yulia Davudi',\n",
       " 'interviewer': 'Geoffrey Khan',\n",
       " 'place': '+Hassar +Baba-čanɟa, N',\n",
       " 'transcriber': 'Geoffrey Khan',\n",
       " 'text_id': 'A32'}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-paragraphs\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, n-lines\n",
    "len(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1\n",
    "len(text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1, n-words\n",
    "len(text[0][0]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'xá', 'letters': ['x', 'á'], 'beginnings': [], 'endings': ['-']}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph 1, line 1, word 1\n",
    "text[0][0]['words'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = Path('../nena/0.01')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parse On All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dialect ../nena/0.01/Barwar--\n",
      "\n",
      "trying: A Hundred Gold Coins.nena\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a0f9fb11d9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mname2text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'trying: {file.name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mparseit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t√'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mname2parsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "name2parsed = {}\n",
    "name2text = {}\n",
    "not_parsed = []\n",
    "\n",
    "ignore = [\n",
    "    #'The Adventures Of Two Brothers.nena', # FIX BY MOVING UNEMPHASIZED OUT\n",
    "]\n",
    "\n",
    "for dialect in dialect_dirs:\n",
    "    print(f'--Dialect {dialect}--')\n",
    "    print()\n",
    "    for file in sorted(dialect.glob('*.nena')):\n",
    "        \n",
    "        if file.name in ignore:\n",
    "            print('SKIPPING:', file.name, '\\n')\n",
    "            not_parsed.append(file)\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as infile:\n",
    "            text = infile.read()\n",
    "            name2text[file.name] = text\n",
    "            print(f'trying: {file.name}')\n",
    "            parseit = parser.parse(lexer.tokenize(text))\n",
    "            print(f'\\t√')\n",
    "            name2parsed[file.name] = parseit\n",
    "                \n",
    "print(len(name2parsed), 'parsed...')\n",
    "print(len(not_parsed), 'not parsed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name2text['A Man Called Čuxo.nena'][7100:7120]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
