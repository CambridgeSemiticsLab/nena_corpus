{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser: A parser for Nena Standard Text format\n",
    "\n",
    "The goal of this parser (under development) is to translate texts in the\n",
    "[Nena Standard Text format][nenamd] (we should find a better name for that)\n",
    "into structured groups of morphemes. Those structured morphemes can then be\n",
    "easily converted to (e.g.) TextFabric format.\n",
    "\n",
    "For the Nena Standard Text parser, we make use of [Sly][sly], a Python\n",
    "implementation of the lex/yacc type of parser generators. (This may soon have\n",
    "to be converted to Sly's predecessor [Ply][ply], as Sly works only with\n",
    "Python 3.6+ and the NENA website runs on Python 3.5 - but that should not be\n",
    "difficult).\n",
    "\n",
    "[nenamd]: https://github.com/CambridgeSemiticsLab/nena_corpus/blob/tomarkdown/docs/text_markup.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[ply]: http://www.dabeaz.com/ply/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPACE', '\\n'),\n",
       " ('TITLE', ('title', 'Gozáli and Nozali')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('ATTRIBUTE', ('text_id', 'A8')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('informant', 'Nanəs Bənyamən')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('place', 'ʾƐn-Nune')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '1'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('HYPHEN', '-'),\n",
       " ('LETTER', '⁺w'),\n",
       " ('LETTER', 'o'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '1'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('COMMENT', '(a-comment)'),\n",
       " ('SPACE', ' '),\n",
       " ('LPAREN_COMMENT', '(GK:'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'b'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', ' '),\n",
       " ('/', '/'),\n",
       " ('/', '/'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('LETTER', 'H'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'o'),\n",
       " ('SPACE', '\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '2'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 's'),\n",
       " ('LETTER', 'o'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '2'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('*', '*'),\n",
       " ('LETTER', 'w'),\n",
       " ('LETTER', 'ó'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('LETTER', 's'),\n",
       " ('*', '*'),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('SPACE', ' '),\n",
       " ('LBRACKET_COMMENT', '[CK:'),\n",
       " ('SPACE', ' \\n'),\n",
       " ('LETTER', 'b'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'o'),\n",
       " ('LETTER', 'k'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'n'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'c'),\n",
       " ('LETTER', 'o'),\n",
       " ('LETTER', 'm'),\n",
       " ('LETTER', 'm'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'n'),\n",
       " ('LETTER', 't'),\n",
       " (']', ']'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '4'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('PUNCTUATION', '‘'),\n",
       " ('LETTER', 'n'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'w'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'g'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'h'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'S'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'y'),\n",
       " ('SPACE', ' '),\n",
       " ('PUNCTUATION', '—'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'w'),\n",
       " ('LETTER', 'h'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 't'),\n",
       " ('PUNCTUATION', '?'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'à'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('FOOTNOTE', (1, 'First footnote\\ncontinued.')),\n",
       " ('SPACE', '\\n'),\n",
       " ('FOOTNOTE', (2, 'Second footnote.')),\n",
       " ('SPACE', '\\n'),\n",
       " ('FOOTNOTE', (3, 'Third footnote, not referenced in text.\\n'))]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    # set of token names\n",
    "    tokens = {\n",
    "        TITLE, ATTRIBUTE, LETTER, NEWLINES, SPACE,\n",
    "        PUNCTUATION, HYPHEN,\n",
    "        LPAREN_COMMENT, LBRACKET_COMMENT, DIGITS,\n",
    "        LANG_MARKER, COMMENT, FOOTNOTE\n",
    "    }\n",
    "    \n",
    "    # NB \\u207A == superscript +\n",
    "    literals = {'*', '(', ')', '{', '}', '[', ']', '/', '^'}\n",
    "\n",
    "    # The '(?m)' part turns on multiline matching, which makes\n",
    "    # it possible to use ^ and $ for the start/end of the line.\n",
    "    # Title starts with pound sign. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^\\# .*$')\n",
    "    def TITLE(self, t):\n",
    "        t.value = ('title', t.value[2:])\n",
    "        return t\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^[a-z][a-z0-9_]+: .*$')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        t.value = tuple(t.value.split(': '))\n",
    "        return t\n",
    "    \n",
    "    # Footnote starts with '[^n]: ', where n is a number.\n",
    "    # Returns a 2-tuple (int: fn_sym, str: footnote_text)\n",
    "    @_(r'(?m)^\\[\\^[1-9][0-9]*\\]: \\D*$')\n",
    "    def FOOTNOTE(self, t):\n",
    "        fn_sym, footnote = t.value.split(maxsplit=1)\n",
    "        t.value = (int(fn_sym[2:-2]), footnote)\n",
    "        return t\n",
    "\n",
    "    # How to get combined Unicode characters to be recognized?\n",
    "    # Matching only Unicode points of letters with pre-combined\n",
    "    # marks can be done with the 'word' class '\\w', but it\n",
    "    # includes digits and underscore. To remove those, negate\n",
    "    # the inverted word class along with digits and underscore:\n",
    "    # '[^\\W\\d_]. But that does not include separate combining\n",
    "    # marks, or the '+' sign.\n",
    "    # One solution would be unicodedata.normalize('NFC', data),\n",
    "    # except that not all combinations have pre-combined Unicode\n",
    "    # points.\n",
    "    # Another solution is to use an external regex engine such as\n",
    "    # `regex` (`pip install regex`), which has better Unicode\n",
    "    # support. However, I would like to avoid extra dependencies.\n",
    "    # Another (less elegant) solution is to make the '+' symbol\n",
    "    # and the combining characters [\\u0300-\\u036F] each its own\n",
    "    # token, which the parser will have to parse into morphemes\n",
    "    # and words.\n",
    "    # Another (also less elegant) solution is to use a 'negative\n",
    "    # lookbehind assertion' for the negation of digits and '_':\n",
    "    # https://stackoverflow.com/a/12349464/9230612\n",
    "    # (?!\\d_)[\\w\\u0300-\\u036F]+\n",
    "    # Because combining marks can never appear before the first\n",
    "    # letter, and because some dialects have a '+' sign at the\n",
    "    # beginning of some words, we prefix an optional '+' symbol\n",
    "    # and an obligatory '[^\\W\\d_]' before the negative lookbehind.\n",
    "    \n",
    "    # One letter with (or without) combining marks can be matched\n",
    "    # with: [^\\W\\d_][\\u0300-\\u036F]*\n",
    "    # We also add a superscript plus (U-207A) as part of a letter, \n",
    "    # since this char is not a letter on its own, but rather\n",
    "    # modifies the quality of a consonant\n",
    "    LETTER = r'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*'\n",
    "    \n",
    "    # we try to make a LETTERS token:\n",
    "#     LETTERS = r'[+]?[^\\W\\d_](?!\\d_)[\\w\\u0300-\\u036F+]*'\n",
    "    # Unfortunately, with python's `re` it seems impossible to repeat\n",
    "    # a group like this. So we will group the letters in the parser.\n",
    "    \n",
    "    # Newlines: boundaries of paragraphs and metadata are marked\n",
    "    # with two newlines (meaning an empty line). The empty line\n",
    "    # may contain whitespace.\n",
    "    NEWLINES = r'\\n\\s*\\n\\s*'\n",
    "    \n",
    "    # Space is any successive number of whitespace symbols.\n",
    "    SPACE = r'\\s+'\n",
    "    # One or more digits, not starting with zero\n",
    "    DIGITS = r'[1-9][0-9]*'\n",
    "    # Line id is any number of digits surrounded by round brackets\n",
    "#     LINE_ID = r'\\([0-9]+\\)'  # TODO convert to int?\n",
    "    # Punctuation is any normal punctuation symbol and vertical bar.\n",
    "    # as well as a long hyphen (—)\n",
    "    PUNCTUATION = r'[.,?!:;–\\u02c8\\u2014\\u2019\\u2018]'\n",
    "    # There are two different hyphens, a single one and a double one.\n",
    "    # The double one is the 'equals' sign.\n",
    "    HYPHEN = r'[-=]'\n",
    "    # Language markers are ASCII letter strings surrounded by\n",
    "    # angle brackets.\n",
    "    LANG_MARKER = r'<[A-Za-z]+>'\n",
    "    # A special comment starts with an opening bracket, capital initials\n",
    "    # and a colon.\n",
    "    LPAREN_COMMENT = r'\\([A-Za-z]+:'\n",
    "    LBRACKET_COMMENT = r'\\[[A-Za-z]+:'\n",
    "    # A regular comment is text (at least one character not being a digit)\n",
    "    # which may not contain a colon (otherwise it becomes a special comment/interruption)\n",
    "    COMMENT = r'\\([^:)]*[^:)\\d]+[^:)]*\\)'\n",
    "\n",
    "lexer_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-\\u207Aword...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "Hello\n",
    "(2) also[^2] <E>*wórds*<E> [CK: \n",
    "broken comment]\n",
    "\n",
    "(4) ‘new paragraph. Say — what? a\\u0300\n",
    "\n",
    "[^1]: First footnote\n",
    "continued.\n",
    "[^2]: Second footnote.\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "[(tok.type, tok.value) for tok in lexer.tokenize(lexer_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a `.nena` file\n",
    "\n",
    "Below is a representation of the tree-like structure of a NENA standard text file. This is the structure that the parser must recognize and reproduce.\n",
    "\n",
    "```\n",
    "text\n",
    "  |\n",
    "  heading\n",
    "  |  |\n",
    "  |  attributes\n",
    "  |    |\n",
    "  |    attribute (e.g. title, informant, etc.)\n",
    "  | \n",
    "  paragraphs\n",
    "    |\n",
    "    paragraph\n",
    "    |  |\n",
    "    |  lines\n",
    "    |    |\n",
    "    |    line\n",
    "    |      |\n",
    "    |      line elements (in any order)\n",
    "    |        |\n",
    "    |        word elements\n",
    "    |        |  |\n",
    "    |        |  morpheme normal (+metadata, e.g. trailer, etc.)\n",
    "    |        |  |\n",
    "    |        |  morpheme foreign (+metadata)\n",
    "    |        |  |\n",
    "    |        |  morpheme language (+metadata)\n",
    "    |        |\n",
    "    |        footnote\n",
    "    |        |\n",
    "    |        comment\n",
    "    |        |\n",
    "    |        interruption\n",
    "    |      \n",
    "    orphaned footnote (processed later)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morpheme class\n",
    "\n",
    "To conveniently store the morpheme and its features, we prepare a small `Morpheme` class, to be used by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morpheme:\n",
    "    \n",
    "    def __init__(self, value, trailer='',\n",
    "                 footnotes=None, speaker=None,\n",
    "                 foreign=False, lang=None):\n",
    "        self.value = value  # list of (combined) characters\n",
    "        self.trailer = trailer  # str (TODO: make this a list as well?)\n",
    "        self.footnotes = footnotes if footnotes is not None else {}  # dict\n",
    "        self.speaker = speaker  # str\n",
    "        self.foreign = foreign  # boolean\n",
    "        self.lang = lang  # str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ''.join(self.value)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        sp = f' speaker {self.speaker!r}' if self.speaker else ''\n",
    "        fr = ' foreign' if self.foreign else ''\n",
    "        ln = f' lang {self.lang!r}' if self.lang else ''\n",
    "        fn = f' fn_anc {\",\".join(str(n) for n in self.footnotes)!r}' if self.footnotes else ''\n",
    "        fn = f' fn_anc {self.footnotes!r}' if self.footnotes else ''\n",
    "        return f'<Morpheme {str(self)!r} trailer {self.trailer!r}{sp}{fr}{ln}{fn}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing, and in this case contains a tuple `(heading, paragraphs)`.\n",
    "\n",
    "The value `heading` contains a dictionary with the text metadata. The value `paragraphs` is a list, in which each element contains a list of `lines`. Each element of `lines` is a 2-tuple containing an `int` line identifier, and a list of `line_elements`. The values of `line_elements` are `Morpheme` objects, or 2-tuples with comments in the form `('comment', str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 17 shift/reduce conflicts\n",
      "Parser debugging for NenaParser written to parser.out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'title': 'Gozáli and Nozali',\n",
       "  'text_id': 'A8',\n",
       "  'informant': 'Nanəs Bənyamən',\n",
       "  'place': 'ʾƐn-Nune'},\n",
       " [[(1,\n",
       "    [<Morpheme 'a' trailer '-'>,\n",
       "     <Morpheme '⁺test' trailer ' '>,\n",
       "     <Morpheme 'word' trailer '... ' fn_anc {1: 'The '}>,\n",
       "     ('comment', 'a-comment'),\n",
       "     <Morpheme 'lalala' trailer ' ' speaker 'GK'>,\n",
       "     <Morpheme 'fd' trailer ' ' speaker 'GK'>,\n",
       "     <Morpheme 'bla' trailer ' //'>,\n",
       "     <Morpheme 'blatwo' trailer ' '>]),\n",
       "   (2,\n",
       "    [<Morpheme 'also' trailer ' ' fn_anc {2: 'Second footnote.\\ncontinued.'}>,\n",
       "     <Morpheme 'wórds' trailer '.' foreign lang 'E'>,\n",
       "     <Morpheme 'i' trailer '.'>,\n",
       "     <Morpheme 'ˈ' trailer ' '>,\n",
       "     <Morpheme 'b' trailer '-' speaker 'GK'>,\n",
       "     <Morpheme 'mú' trailer ' ' speaker 'GK'>,\n",
       "     <Morpheme 'bəcnàšəva' trailer '? ' speaker 'GK'>,\n",
       "     <Morpheme 'b' trailer '-'>,\n",
       "     <Morpheme 'mù' trailer ''>])],\n",
       "  [(3, [<Morpheme 'more' trailer ' '>, <Morpheme 'wordsčx' trailer ' '>]),\n",
       "   (4,\n",
       "    [<Morpheme 'new' trailer '-'>,\n",
       "     <Morpheme 'paragraph' trailer ' — '>,\n",
       "     <Morpheme 'pause' trailer '. '>,\n",
       "     <Morpheme 't' trailer '-'>,\n",
       "     <Morpheme 'wéwa' trailer ' ... '>,\n",
       "     <Morpheme 'ʾáyya' trailer ' '>,\n",
       "     <Morpheme 'tséntr' trailer '-' foreign lang 'R'>,\n",
       "     <Morpheme 'ət' trailer ''>])],\n",
       "  ('footnotes', {3: 'Third footnote, not referenced in text.\\n\\n'})])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Parser\n",
    "\n",
    "# dict stack to contain footnote anchors,\n",
    "# until the corresponding footnote is encountered.\n",
    "fn_anchors = {}\n",
    "\n",
    "class NenaParser(Parser):\n",
    "    \n",
    "    debugfile = 'parser.out'\n",
    "\n",
    "    # Get the token list from the lexer (required)\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    def error(self, t):\n",
    "        #print('ERROR:')\n",
    "        #print(f'\\tunexpected string {repr(t.value[0])} at index {t.index}')\n",
    "        raise Exception(f'unexpected string {repr(t.value[0])} at index {t.index}')\n",
    "    \n",
    "    @_('heading NEWLINES paragraphs')\n",
    "    def text(self, p):\n",
    "        return (p.heading, p.paragraphs)\n",
    "    \n",
    "    # -- HEADING --\n",
    "    \n",
    "    @_('SPACE TITLE NEWLINES attributes',\n",
    "       'TITLE NEWLINES attributes')\n",
    "    def heading(self, p):\n",
    "        key, value = p.TITLE\n",
    "        heading = {key: value}\n",
    "        heading.update(p.attributes)\n",
    "        return heading\n",
    "    \n",
    "    @_('attributes space ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        p.attributes[key] = value\n",
    "        return p.attributes \n",
    "    \n",
    "    @_('ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        return {key: value}\n",
    "    \n",
    "    # -- PARAGRAPHS --\n",
    "    \n",
    "    @_('paragraphs NEWLINES paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        # handle cases of null footnotes\n",
    "        if p.paragraph is not None:\n",
    "            return p.paragraphs + [p.paragraph]\n",
    "        else:\n",
    "            return p.paragraphs\n",
    "        \n",
    "    @_('paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        return [p.paragraph]\n",
    "    \n",
    "    # paragraph\n",
    "    @_('paragraph line')\n",
    "    def paragraph(self, p):\n",
    "        return p.paragraph + [p.line]\n",
    "    \n",
    "    # paragraph from orphaned footnotes\n",
    "    @_('footnotes')\n",
    "    def paragraph(self, p):\n",
    "        if p.footnotes:\n",
    "            # TODO: issue log warning about\n",
    "            # unreferenced footnotes?\n",
    "            return ('footnotes', p.footnotes)\n",
    "    \n",
    "    # -- FOOTNOTES -- \n",
    "    \n",
    "    @_('footnotes footnote')\n",
    "    def footnotes(self, p):\n",
    "        p.footnotes.update(p.footnote)\n",
    "        return p.footnotes\n",
    "    \n",
    "    @_('footnote')\n",
    "    def footnotes(self, p):\n",
    "        return p.footnote\n",
    "    \n",
    "    @_('FOOTNOTE space NEWLINES',\n",
    "       'FOOTNOTE NEWLINES',\n",
    "       'FOOTNOTE space',\n",
    "       'FOOTNOTE')\n",
    "    def footnote(self, p):\n",
    "        fn_sym, fn_str = p.FOOTNOTE\n",
    "        footnote = {}\n",
    "        try:\n",
    "            # lookup the fn_sym key in the fn_anchors dict,\n",
    "            # and add the footnote to the appropriate morpheme\n",
    "            fn_morpheme = fn_anchors.pop(fn_sym)\n",
    "            fn_morpheme.footnotes[fn_sym] = fn_str\n",
    "        except KeyError:\n",
    "            # This means there is not footnote anchor\n",
    "            # referring to this footnote. So we return\n",
    "            # the footnote to the text\n",
    "            footnote = {fn_sym: fn_str}\n",
    "        return footnote\n",
    "\n",
    "    # -- LINES --\n",
    "    \n",
    "    @_('line')\n",
    "    def paragraph(self, p):\n",
    "        return [p.line]\n",
    "    \n",
    "    @_('line_id line_elements')\n",
    "    def line(self, p):\n",
    "        return (p.line_id, p.line_elements)\n",
    "    \n",
    "    @_('\"(\" DIGITS \")\" SPACE')\n",
    "    def line_id(self, p):\n",
    "        return int(p.DIGITS)\n",
    "\n",
    "    @_('line_elements line_element',\n",
    "       'line_element')\n",
    "    def line_elements(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.line_elements + p.line_element\n",
    "        else:\n",
    "            return p.line_element\n",
    "    \n",
    "    # -- MORPHEMES -- \n",
    "    \n",
    "    @_('morphemes',\n",
    "       'fn_anchor',\n",
    "       'interruption',\n",
    "       'morphemes_foreign',\n",
    "       'morphemes_language',\n",
    "       'comment')\n",
    "    def line_element(self, p):\n",
    "        return p[0]\n",
    "\n",
    "    # morphemes_language\n",
    "    @_('lang morphemes_foreign morpheme_trailer lang trailer',\n",
    "       'lang morphemes_foreign lang trailer',\n",
    "       'lang morphemes_foreign lang',\n",
    "       'lang morphemes_foreign')\n",
    "    def morphemes_language(self, p):\n",
    "        # check if language markers correspond\n",
    "        if len(p) > 2:\n",
    "            lang = p.lang0\n",
    "            if p.lang0 != p.lang1:\n",
    "                pass  # TODO issue warning: language markers do not correspond\n",
    "        else:\n",
    "            lang = p.lang  # TODO issue warning: missing second language marker\n",
    "        for m in p.morphemes_foreign:\n",
    "            m.lang = lang\n",
    "        if len(p) == 4:\n",
    "            p.morphemes_foreign[-1].trailer += p.trailer\n",
    "        elif len(p) == 5:\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "            p.morphemes_foreign.append(morpheme_trailer)\n",
    "        return p.morphemes_foreign\n",
    "    \n",
    "    # lang\n",
    "    @_('LANG_MARKER')\n",
    "    def lang(self, p):\n",
    "        return p.LANG_MARKER[1:-1]\n",
    "\n",
    "    # morphemes_foreign\n",
    "    # last morpheme may not include trailer\n",
    "    # add trailer after second asterisk to last morpheme\n",
    "    @_('\"*\" morphemes letters \"*\" trailer',\n",
    "       '\"*\" morphemes letters \"*\"',\n",
    "       '\"*\" letters \"*\" trailer',\n",
    "       '\"*\" letters \"*\"',\n",
    "      )\n",
    "    def morphemes_foreign(self, p):\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "        except KeyError:\n",
    "            trailer = ''\n",
    "        try:\n",
    "            morphemes = p.morphemes\n",
    "        except KeyError:\n",
    "            morphemes = []\n",
    "        morphemes.append(Morpheme(p.letters, trailer=trailer))\n",
    "        for m in morphemes:\n",
    "            m.foreign = True\n",
    "        return morphemes\n",
    "    \n",
    "    # comment\n",
    "    @_('COMMENT trailer',\n",
    "       'COMMENT')\n",
    "    def comment(self, p):\n",
    "        return [('comment', p.COMMENT[1:-1])]\n",
    "\n",
    "    # interruption\n",
    "    @_('LPAREN_COMMENT space morphemes \")\" trailer',\n",
    "       'LPAREN_COMMENT space morphemes \")\"',\n",
    "       'LBRACKET_COMMENT space morphemes \"]\" trailer',\n",
    "       'LBRACKET_COMMENT space morphemes \"]\"')\n",
    "    def interruption(self, p):\n",
    "        speaker = p[0][1:-1]\n",
    "        for m in p.morphemes:\n",
    "            m.speaker = speaker\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "            if (p.morphemes[-1].trailer.endswith(' ')\n",
    "                and trailer.startswith(' ')):\n",
    "                trailer = trailer[1:]\n",
    "            p.morphemes[-1].trailer += trailer\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return p.morphemes\n",
    "    \n",
    "    # morphemes\n",
    "    @_('morphemes morpheme_trailer',\n",
    "       'morpheme_trailer')\n",
    "    def morphemes(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.morphemes + [p.morpheme_trailer]\n",
    "        else:\n",
    "            return [p.morpheme_trailer]\n",
    "    \n",
    "    # -- MORPHEME ATTRIBUTES --\n",
    "    \n",
    "    # morpheme_trailer\n",
    "    @_('letters trailer',\n",
    "       'letters')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 2:\n",
    "            trailer = p[1]\n",
    "        else:\n",
    "            trailer = ''\n",
    "        return Morpheme(p.letters, trailer=trailer)\n",
    "\n",
    "    # morpheme_trailer with footnote anchor\n",
    "    @_('morpheme_trailer fn_anchor trailer',\n",
    "       'morpheme_trailer fn_anchor')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 3:\n",
    "            if (p.morpheme_trailer.trailer.endswith(' ')\n",
    "                and p.trailer.startswith(' ')):\n",
    "                p.trailer = p.trailer[1:]\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "        # add dummy value {fn_anc: None} to footnote dict\n",
    "        p.morpheme_trailer.footnotes[p.fn_anchor] = None\n",
    "        # add morpheme object to fn_anchors dict,\n",
    "        # for easy access when footnote text is found\n",
    "        fn_anchors[p.fn_anchor] = p.morpheme_trailer\n",
    "        return p.morpheme_trailer\n",
    "    \n",
    "    # --VARIOUS--\n",
    "    \n",
    "    @_('\"[\" \"^\" DIGITS \"]\"')\n",
    "    def fn_anchor(self, p):\n",
    "        return int(p.DIGITS)\n",
    "        \n",
    "    @_('letters LETTER')\n",
    "    def letters(self, p):\n",
    "        return p.letters + [p.LETTER]\n",
    "    \n",
    "    @_('LETTER')\n",
    "    def letters(self, p):\n",
    "        return [p[0]]\n",
    "    \n",
    "    # trailer\n",
    "    @_('trailer versebreak',\n",
    "       'trailer linebreak',\n",
    "       'trailer PUNCTUATION',\n",
    "       'trailer space',\n",
    "       'PUNCTUATION',\n",
    "       'space',\n",
    "       'HYPHEN',\n",
    "      )\n",
    "    def trailer(self, p):\n",
    "        return ''.join(p)\n",
    "    \n",
    "    # -- LITERALS --\n",
    "    \n",
    "    # reduce any number of spaces (\\s+)\n",
    "    # to a single space (' ')\n",
    "    @_('SPACE')\n",
    "    def space(self, p):\n",
    "        return ' '\n",
    "    \n",
    "    @_('\"/\" \"/\"',\n",
    "       '\"/\" \"/\" space',\n",
    "       '\"/\" \"/\" NEWLINES',\n",
    "       '\"/\" \"/\" space NEWLINES')\n",
    "    def versebreak(self, p):\n",
    "        return '//'\n",
    "    \n",
    "    @_('\"/\"',\n",
    "       '\"/\" space',\n",
    "       '\"/\" NEWLINES',\n",
    "       '\"/\" space NEWLINES')\n",
    "    def linebreak(self, p):\n",
    "        return '/'\n",
    "    \n",
    "parser_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-\\u207Atest word...[^1] (a-comment) [GK: lalala fd] bla //\n",
    "\n",
    "blatwo\n",
    "(2) also[^2] <E>*wórds*<E>.i.ˈ [GK:\n",
    "b-mú bəcnàšəva?] b-mù\n",
    "\n",
    "(3) more wordsčx\n",
    "(4) new-paragraph — pause. t-wéwa ... ʾáyya  <R>*tséntr*<R>-ət\n",
    "\n",
    "[^1]: The \n",
    "[^2]: Second footnote.\n",
    "continued.\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# demonstration of output results of parser, to be used by generate_TF loop\n",
    "parser = NenaParser()\n",
    "parser.parse(lexer.tokenize(parser_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(4)',\n",
       " 'mə̀rrəˈ',\n",
       " 'ʾáha',\n",
       " 'ɟári',\n",
       " 'ʾàvəˈ',\n",
       " 'ʾo-nášət',\n",
       " 'ʾána',\n",
       " '⁺byàyunˈ',\n",
       " 'k̭at-lá-ʾavilə',\n",
       " '⁺ʾā̀x,ˈ',\n",
       " 'k̭at-lá-ʾavilə',\n",
       " 'xə̀šša,ˈ',\n",
       " 'lá-ʾavilə',\n",
       " 'taxmànta,ˈ',\n",
       " 'ʾáha',\n",
       " 'ɟánu',\n",
       " 'xá-ʾaxča',\n",
       " 'laxùyma,ˈ',\n",
       " 'xá-ʾaxča',\n",
       " 'zùyza,ˈ',\n",
       " 'ʾá',\n",
       " 'duccána',\n",
       " '⁺ɟùrta,ˈ',\n",
       " 'paláxə',\n",
       " 'xut-ʾìdu.ˈ',\n",
       " 'bas-ʾáha',\n",
       " 'lə̀tlə',\n",
       " 'xə́šša.ˈ',\n",
       " 'p-sáp̂rən',\n",
       " 'xázzən',\n",
       " 'mu-p̂ṱ-òya.ˈ']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'(4)  mə̀rrəˈ ʾáha ɟári ʾàvəˈ ʾo-nášət ʾána ⁺byàyunˈ k̭at-lá-ʾavilə ⁺ʾā̀x,ˈ k̭at-lá-ʾavilə xə̀šša,ˈ lá-ʾavilə taxmànta,ˈ ʾáha ɟánu xá-ʾaxča laxùyma,ˈ xá-ʾaxča zùyza,ˈ ʾá duccána ⁺ɟùrta,ˈ paláxə xut-ʾìdu.ˈ bas-ʾáha lə̀tlə xə́šša.ˈ p-sáp̂rən xázzən mu-p̂ṱ-òya.ˈ'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser output\n",
    "\n",
    "The parser prints a warning that there were shift/reduce conflicts, probably caused by ambiguous whitespace. That is not a problem (although not very elegant, ideally it should be fixed). The parser resolves the conflicts automatically.\n",
    "\n",
    "The output of the example text shows that the parser succeeded to parse it, and structure it into heading, paragraphs, lines and morphemes, with the features stored in the Morpheme object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = Path('../nena/0.01')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parse On All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dialect ../nena/0.01/Barwar--\n",
      "\n",
      "trying: A Hundred Gold Coins.nena\n",
      "\t√\n",
      "trying: A Man Called Čuxo.nena\n",
      "\t√\n",
      "trying: A Tale Of A Prince And A Princess.nena\n",
      "\t√\n",
      "trying: A Tale Of Two Kings.nena\n",
      "\t√\n",
      "trying: Baby Leliθa.nena\n",
      "\t√\n",
      "trying: Dəmdəma.nena\n",
      "\t√\n",
      "trying: Gozali And Nozali.nena\n",
      "\t√\n",
      "trying: I Am Worth The Same As A Blind Wolf.nena\n",
      "\t√\n",
      "trying: Man Is Treacherous.nena\n",
      "\t√\n",
      "trying: Measure For Measure.nena\n",
      "\t√\n",
      "trying: Nanno And Jəndo.nena\n",
      "\t√\n",
      "trying: Qaṭina Rescues His Nephew From Leliθa.nena\n",
      "\t√\n",
      "trying: Sour Grapes.nena\n",
      "\t√\n",
      "trying: Tales From The 1001 Nights.nena\n",
      "\t√\n",
      "trying: The Battle With Yuwanəs The Armenian.nena\n",
      "\t√\n",
      "trying: The Bear And The Fox.nena\n",
      "\t√\n",
      "trying: The Brother Of Giants.nena\n",
      "\t√\n",
      "trying: The Cat And The Mice.nena\n",
      "\t√\n",
      "trying: The Cooking Pot.nena\n",
      "\t√\n",
      "trying: The Crafty Hireling.nena\n",
      "\t√\n",
      "trying: The Crow And The Cheese.nena\n",
      "\t√\n",
      "trying: The Daughter Of The King.nena\n",
      "\t√\n",
      "trying: The Fox And The Lion.nena\n",
      "\t√\n",
      "trying: The Fox And The Miller.nena\n",
      "\t√\n",
      "trying: The Fox And The Stork.nena\n",
      "\t√\n",
      "trying: The Giant’s Cave.nena\n",
      "\t√\n",
      "trying: The Girl And The Seven Brothers.nena\n",
      "\t√\n",
      "trying: The King With Forty Sons.nena\n",
      "\t√\n",
      "trying: The Leliθa From Č̭āl.nena\n",
      "\t√\n",
      "trying: The Lion King.nena\n",
      "\t√\n",
      "trying: The Lion With A Swollen Leg.nena\n",
      "\t√\n",
      "trying: The Man Who Cried Wolf.nena\n",
      "\t√\n",
      "trying: The Man Who Wanted To Work.nena\n",
      "\t√\n",
      "trying: The Monk And The Angel.nena\n",
      "\t√\n",
      "trying: The Monk Who Wanted To Know When He Would Die.nena\n",
      "\t√\n",
      "trying: The Priest And The Mullah.nena\n",
      "\t√\n",
      "trying: The Sale Of An Ox.nena\n",
      "\t√\n",
      "trying: The Scorpion And The Snake.nena\n",
      "\t√\n",
      "trying: The Selfish Neighbour.nena\n",
      "\t√\n",
      "trying: The Sisisambər Plant.nena\n",
      "\t√\n",
      "trying: The Story With No End.nena\n",
      "\t√\n",
      "trying: The Tale Of Farxo And Səttiya.nena\n",
      "\t√\n",
      "trying: The Tale Of Mămo And Zine.nena\n",
      "\t√\n",
      "trying: The Tale Of Mərza Pămət.nena\n",
      "\t√\n",
      "trying: The Tale Of Nasimo.nena\n",
      "\t√\n",
      "trying: The Tale Of Parizada, Warda And Nargis.nena\n",
      "\t√\n",
      "trying: The Tale Of Rustam (1).nena\n",
      "\t√\n",
      "trying: The Tale Of Rustam (2).nena\n",
      "\t√\n",
      "trying: The Wise Daughter Of The King.nena\n",
      "\t√\n",
      "trying: The Wise Snake.nena\n",
      "\t√\n",
      "trying: The Wise Young Man.nena\n",
      "\t√\n",
      "trying: Šošət Xere.nena\n",
      "\t√\n",
      "--Dialect ../nena/0.01/Urmi_C--\n",
      "\n",
      "trying: A Close Shave.nena\n",
      "\t√\n",
      "trying: A Cure For A Husband’s Madness.nena\n",
      "\t√\n",
      "trying: A Donkey Knows Best.nena\n",
      "\t√\n",
      "trying: A Dragon In The Well.nena\n",
      "\t√\n",
      "trying: A Dutiful Son.nena\n",
      "\t√\n",
      "trying: A Frog Wants A Husband.nena\n",
      "\t√\n",
      "trying: A Lost Donkey.nena\n",
      "\t√\n",
      "trying: A Lost Ring.nena\n",
      "\t√\n",
      "trying: A Painting Of The King Of Iran.nena\n",
      "\t√\n",
      "trying: A Pound Of Flesh.nena\n",
      "\t√\n",
      "trying: A Sweater To Pay Off A Debt.nena\n",
      "\t√\n",
      "trying: A Thousand Dinars.nena\n",
      "\t√\n",
      "trying: A Visit From Harun Ar-rashid.nena\n",
      "\t√\n",
      "trying: Agriculture And Village Life.nena\n",
      "\t√\n",
      "trying: Am I Dead?.nena\n",
      "\t√\n",
      "trying: An Orphan Duckling.nena\n",
      "\t√\n",
      "trying: Axiqar.nena\n",
      "\t√\n",
      "trying: Events In 1946 On The Urmi Plain.nena\n",
      "\t√\n",
      "trying: Games.nena\n",
      "\t√\n",
      "trying: Hunting.nena\n",
      "\t√\n",
      "trying: I Have Died.nena\n",
      "\t√\n",
      "trying: Ice For Dinner.nena\n",
      "\t√\n",
      "trying: Is There A Man With No Worries?.nena\n",
      "\t√\n",
      "trying: Kindness To A Donkey.nena\n",
      "\t√\n",
      "trying: Lost Money.nena\n",
      "\t√\n",
      "trying: Mistaken Identity.nena\n",
      "\t√\n",
      "trying: Much Ado About Nothing.nena\n",
      "\t√\n",
      "trying: Nipuxta.nena\n",
      "\t√\n",
      "trying: No Bread Today.nena\n",
      "\t√\n",
      "trying: Problems Lighting A Fire.nena\n",
      "\t√\n",
      "trying: St. Zayya’s Cake Dough.nena\n",
      "\t√\n",
      "trying: Star-crossed Lovers.nena\n",
      "\t√\n",
      "trying: Stomach Trouble.nena\n",
      "\t√\n",
      "trying: The Adventures Of A Princess.nena\n",
      "\t√\n",
      "trying: The Adventures Of Ashur.nena\n",
      "\t√\n",
      "trying: The Adventures Of Two Brothers.nena\n",
      "\t√\n",
      "trying: The Angel Of Death.nena\n",
      "\t√\n",
      "trying: The Assyrians Of Armenia.nena\n",
      "\t√\n",
      "trying: The Assyrians Of Urmi.nena\n",
      "\t√\n",
      "trying: The Bald Child And The Monsters.nena\n",
      "\t√\n",
      "trying: The Bald Man And The King.nena\n",
      "\t√\n",
      "trying: The Bird And The Fox.nena\n",
      "\t√\n",
      "trying: The Cat’s Dinner.nena\n",
      "\t√\n",
      "trying: The Cow And The Poor Girl.nena\n",
      "\t√\n",
      "trying: The Dead Rise And Return.nena\n",
      "\t√\n",
      "trying: The Fisherman And The Princess.nena\n",
      "\t√\n",
      "trying: The Giant One-eyed Demon.nena\n",
      "\t√\n",
      "trying: The Little Prince And The Snake.nena\n",
      "\t√\n",
      "trying: The Loan Of A Cooking Pot.nena\n",
      "\t√\n",
      "trying: The Man Who Wanted To Complain To God.nena\n",
      "\t√\n",
      "trying: The Old Man And The Fish.nena\n",
      "\t√\n",
      "trying: The Purchase Of A Donkey.nena\n",
      "\t√\n",
      "trying: The Snake’s Dilemma.nena\n",
      "\t√\n",
      "trying: The Stupid Carpenter.nena\n",
      "\t√\n",
      "trying: The Wife Who Learns How To Work (2).nena\n",
      "\t√\n",
      "trying: The Wife Who Learns How To Work.nena\n",
      "\t√\n",
      "trying: The Wife’s Condition.nena\n",
      "\t√\n",
      "trying: The Wise Brother.nena\n",
      "\t√\n",
      "trying: The Wise Young Daughter.nena\n",
      "\t√\n",
      "trying: Trickster.nena\n",
      "\t√\n",
      "trying: Two Birds Fall In Love.nena\n",
      "\t√\n",
      "trying: Two Wicked Daughters-in-law.nena\n",
      "\t√\n",
      "trying: Village Life (2).nena\n",
      "\t√\n",
      "trying: Village Life (3).nena\n",
      "\t√\n",
      "trying: Village Life (4).nena\n",
      "\t√\n",
      "trying: Village Life (5).nena\n",
      "\t√\n",
      "trying: Village Life (6).nena\n",
      "\t√\n",
      "trying: Village Life.nena\n",
      "\t√\n",
      "trying: Vineyards.nena\n",
      "\t√\n",
      "trying: Weddings And Festivals.nena\n",
      "\t√\n",
      "trying: Weddings.nena\n",
      "\t√\n",
      "trying: When Shall I Die?.nena\n",
      "\t√\n",
      "trying: Women Are Stronger Than Men.nena\n",
      "\t√\n",
      "trying: Women Do Things Best.nena\n",
      "\t√\n",
      "126 parsed...\n",
      "0 not parsed...\n"
     ]
    }
   ],
   "source": [
    "parsed = []\n",
    "name2text = {}\n",
    "not_parsed = []\n",
    "\n",
    "ignore = [\n",
    "    #'The Adventures Of Two Brothers.nena', # FIX BY MOVING UNEMPHASIZED OUT\n",
    "]\n",
    "\n",
    "for dialect in dialect_dirs:\n",
    "    print(f'--Dialect {dialect}--')\n",
    "    print()\n",
    "    for file in sorted(dialect.glob('*.nena')):\n",
    "        \n",
    "        if file.name in ignore:\n",
    "            print('SKIPPING:', file.name, '\\n')\n",
    "            not_parsed.append(file)\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as infile:\n",
    "            text = infile.read()\n",
    "            name2text[file.name] = text\n",
    "            print(f'trying: {file.name}')\n",
    "            parseit = parser.parse(lexer.tokenize(text))\n",
    "            print(f'\\t√')\n",
    "            parsed.append(parseit)\n",
    "                \n",
    "print(len(parsed), 'parsed...')\n",
    "print(len(not_parsed), 'not parsed...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TextFabric\n",
    "\n",
    "The output of the parser can be very easily converted to TextFabric format, as most elements are already separated and structured, which removes a lot of checking and matching from the conversion script, as it is done by the parser already. Some extra structuring, such as division into sentences, subsentences, prosaic units, and words, must be done in the conversion script, based on the contents of the `trailer` attribute.\n",
    "\n",
    "Below an (incomplete) example of a loop converting the parser output to something TextFabric can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-+word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "bla\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph.\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\"\"\"\n",
    "\n",
    "heading, paragraphs = parser.parse(lexer.tokenize(tf_test))\n",
    "\n",
    "# raw_features['title'][this_text] = heading['title']\n",
    "# ... etc.\n",
    "\n",
    "# initialize counters (will be increased to start from 1)\n",
    "this_paragraph = 0\n",
    "this_line = 0\n",
    "this_sentence = 0\n",
    "this_subsentence = 0\n",
    "this_word = 0\n",
    "this_morpheme = 0\n",
    "this_prosa = 0\n",
    "\n",
    "slot = 0 # i.e. chars\n",
    "\n",
    "# Mark units that are increased upon their 'ending' boundaryas 'ended',\n",
    "# so their counters will be increased on first morpheme\n",
    "sentence_end = True\n",
    "subsentence_end = True\n",
    "prosa_end = True\n",
    "word_end = True\n",
    "\n",
    "for p in paragraphs:\n",
    "    if type(p) is tuple:\n",
    "        # key, value = p  # unreferenced footnotes are passed as paragraph tuples\n",
    "        # and can be ignored\n",
    "        continue\n",
    "    this_paragraph += 1\n",
    "    for line_id, line in p:\n",
    "        this_line += 1\n",
    "        for m in line:\n",
    "            if type(m) is tuple:\n",
    "                # key, value = line_element  # comments are passed as tuples\n",
    "                # to be ignored?\n",
    "                continue\n",
    "            \n",
    "            # increase counters\n",
    "            this_morpheme += 1\n",
    "            \n",
    "            # increase counters of ended units\n",
    "            if sentence_end:\n",
    "                this_sentence += 1\n",
    "            if subsentence_end:\n",
    "                this_subsentence += 1\n",
    "            if prosa_end:\n",
    "                this_prosa += 1\n",
    "            if word_end:\n",
    "                this_word += 1\n",
    "            \n",
    "            for c in m.value:\n",
    "                slot += 1\n",
    "                \n",
    "                # add main character features:\n",
    "                # pretty_c = unicodedata.normalize('NFC', c)  # make pretty utf8 char text\n",
    "                # trans_c = translate(c, transcr_table)  # character in transcription\n",
    "                # raw_features['utf8'][slot] = pretty_c\n",
    "                # raw_features['trans'][slot] = trans_c\n",
    "                \n",
    "                # and other char features from Morpheme object `m`:\n",
    "                # if m.speaker:\n",
    "                #     raw_features['speaker'][slot] = m.speaker\n",
    "                # if m.foreign:\n",
    "                #     raw_features['language'][slot] = m.lang or ''\n",
    "            \n",
    "            # the last character of a `morpheme` gets its `trailer` and `footnotes`:\n",
    "            # raw_features['trailer'][slot] = m.trailer.replace('|', '\\u02c8')\n",
    "            # if any(m.footnotes.values()):\n",
    "            #     raw_features['footnotes'][slot] = '\\n'.join(m.footnotes.values())\n",
    "                \n",
    "            # check for unit ends\n",
    "            if (any(c in m.trailer for c in '.!?')\n",
    "                or m.trailer.endswith('//')):\n",
    "                sentence_end = True\n",
    "                subsentence_end = True\n",
    "            if (any(c in m.trailer for c in ',;:')\n",
    "                or m.trailer.endswith('/')):\n",
    "                subsentence_end = True\n",
    "            if '|' in m.trailer:\n",
    "                prosa_end = True\n",
    "            if m.trailer not in ('-', '=', ''):\n",
    "                word_end = True\n",
    "                # m.trailer == '' should only occur at end of paragraph.\n",
    "                # TODO issue a warning if it occurs elsewhere? (Better in parser?)\n",
    "                \n",
    "    # end of paragraph also ends sentence, subsentence, prosa, and word units\n",
    "    sentence_end = True\n",
    "    subsentence_end = True\n",
    "    prosa_end = True\n",
    "    word_end = True        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- [x] ~~implement 'foreign' marker `*`~~\n",
    "- [x] ~~implement language marker `<Marker>`~~\n",
    "- [x] ~~implement line and verse breaks `/` and `//`~~\n",
    "- [x] ~~implement footnotes~~\n",
    "\n",
    "ISSUES\n",
    "\n",
    "The parser does not enforce all parts of the grammar. For example, as verse and line breaks are just appended to the trailer, nothing will stop it from adding other trailer elements (save whitespace) after it. There is also no check to see whether the two `LANG_MARKER`s have the same value. A `+` sign must appear as the first character in a `morpheme`, but that just means that a `+` in the middle of a morpheme breaks it into two morphemes, instead of invalidating it. Undoubtedly there are more issues like this.\n",
    "\n",
    "Paragraphs in which the first line lacks a `line_id` break the parser. That is true for e.g. the first line of the text in which the `line_id` is absent, or for poetic style text with no `//` verse break marker but with empty line dividing verses. This could (should?) be handled by fixing the issue (default `line_id=1` for first line, default `//` verse break for empty lines within `line`), and issuing a warning notifying the user of the automatic fix.\n",
    "\n",
    "Footnotes can only be one line and the string is not processed (e.g. markup like `*emphasis*` is kept as is).\n",
    "\n",
    "Footnote anchors can now only occur after a `morpheme`, not after other things like `comment` or `interruption`. (note to self: possible solution: include `fn_anc` in `morphemes` instead of `morpheme_trailer`, and put `comment` in `trailer`).\n",
    "\n",
    "Comments and unreferenced footnotes are returned as tuples for now, and have to be filtered out in the loop.\n",
    "\n",
    "QUESTIONS\n",
    "\n",
    "Some questions require answers for implementation. They need not be definitive answers for now, but they should be motivated somehow (even if the motivation is 'random choice'), so it will be clear later why it is done in one way or another.\n",
    "\n",
    "- How to store hyphen? Now it is stored as a character in a word occuring between morphemes (I think).\n",
    "  \n",
    "  Should it be the trailer of the morpheme?\n",
    "\n",
    "\n",
    "- How to split sentences?\n",
    "\n",
    "  Now sentences are split on .?! and subsentences on ,\n",
    "  There are other symbols: ;:– and even .. ... ..., .... ..... (If I recall correctly). Should those split\n",
    "  sentences or subsentences?\n",
    "\n",
    "\n",
    "- What to do with poetic line breaks and sentence/paragraph boundaries?\n",
    "\n",
    "  I think a 'poem' should not be divided into paragraphs. I suggest that a line break '/' is a subsentence division, and a verse break '//' a sentence division (even when in the source it is followed by an empty line). If there is a verse number in between, that automatically starts a new sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
