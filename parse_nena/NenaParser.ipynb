{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NenaParser: A parser for Nena Standard Text format\n",
    "\n",
    "The goal of this parser (under development) is to translate texts in the\n",
    "[Nena Standard Text format][nenamd] (we should find a better name for that)\n",
    "into structured groups of morphemes. Those structured morphemes can then be\n",
    "easily converted to (e.g.) TextFabric format.\n",
    "\n",
    "For the Nena Standard Text parser, we make use of [Sly][sly], a Python\n",
    "implementation of the lex/yacc type of parser generators. (This may soon have\n",
    "to be converted to Sly's predecessor [Ply][ply], as Sly works only with\n",
    "Python 3.6+ and the NENA website runs on Python 3.5 - but that should not be\n",
    "difficult).\n",
    "\n",
    "[nenamd]: https://github.com/CambridgeSemiticsLab/nena_corpus/blob/tomarkdown/docs/text_markup.md\n",
    "[sly]: https://sly.readthedocs.io/en/latest/\n",
    "[ply]: http://www.dabeaz.com/ply/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "\n",
    "The parser needs as its input 'tokens', which are predefined units of characters. These are provided by the 'lexer'. In Sly (and Ply), tokens are defined as regular expressions, of which the matching string is returned as the token value. If the token is defined as a function (with its regular expression as argument to the `@_` decorator), then the returned value (among other things) can be manipulated. For more detailed information, [see the documentation][slydocs].\n",
    "\n",
    "[slydocs]: https://sly.readthedocs.io/en/latest/sly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SPACE', '\\n'),\n",
       " ('TITLE', ('title', 'Gozáli and Nozali')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('ATTRIBUTE', ('text_id', 'A8')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('informant', 'Nanəs Bənyamən')),\n",
       " ('SPACE', '\\n'),\n",
       " ('ATTRIBUTE', ('place', 'ʾƐn-Nune')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '1'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('HYPHEN', '-'),\n",
       " ('LETTER', '⁺w'),\n",
       " ('LETTER', 'o'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '1'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('COMMENT', '(a-comment)'),\n",
       " ('SPACE', ' '),\n",
       " ('LPAREN_COMMENT', '(GK: '),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'b'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'a'),\n",
       " ('SPACE', ' '),\n",
       " ('/', '/'),\n",
       " ('/', '/'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('LETTER', 'H'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 'o'),\n",
       " ('SPACE', '\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '2'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'l'),\n",
       " ('LETTER', 's'),\n",
       " ('LETTER', 'o'),\n",
       " ('[', '['),\n",
       " ('^', '^'),\n",
       " ('DIGITS', '2'),\n",
       " (']', ']'),\n",
       " ('SPACE', ' '),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('*', '*'),\n",
       " ('LETTER', 'w'),\n",
       " ('LETTER', 'ó'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'd'),\n",
       " ('LETTER', 's'),\n",
       " ('*', '*'),\n",
       " ('LANG_MARKER', '<E>'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('(', '('),\n",
       " ('DIGITS', '4'),\n",
       " (')', ')'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'n'),\n",
       " ('LETTER', 'e'),\n",
       " ('LETTER', 'w'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'g'),\n",
       " ('LETTER', 'r'),\n",
       " ('LETTER', 'a'),\n",
       " ('LETTER', 'p'),\n",
       " ('LETTER', 'h'),\n",
       " ('PUNCTUATION', '.'),\n",
       " ('SPACE', ' '),\n",
       " ('LETTER', 'à'),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('FOOTNOTE', (1, 'First footnote.')),\n",
       " ('SPACE', '\\n'),\n",
       " ('FOOTNOTE', (2, 'Second footnote.')),\n",
       " ('NEWLINES', '\\n\\n'),\n",
       " ('FOOTNOTE', (3, 'Third footnote, not referenced in text.')),\n",
       " ('SPACE', '\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Lexer\n",
    "\n",
    "class NenaLexer(Lexer):\n",
    "    \n",
    "    # set of token names\n",
    "    tokens = {\n",
    "        TITLE, ATTRIBUTE, LETTER, NEWLINES, SPACE,\n",
    "        PUNCTUATION, HYPHEN,\n",
    "        LPAREN_COMMENT, LBRACKET_COMMENT, DIGITS,\n",
    "        LANG_MARKER, COMMENT, FOOTNOTE\n",
    "    }\n",
    "    \n",
    "    # NB \\u207A == superscript +\n",
    "    literals = {'*', '(', ')', '{', '}', '[', ']', '/', '^'}\n",
    "\n",
    "    # The '(?m)' part turns on multiline matching, which makes\n",
    "    # it possible to use ^ and $ for the start/end of the line.\n",
    "    # Title starts with pound sign. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^\\# .*$')\n",
    "    def TITLE(self, t):\n",
    "        t.value = ('title', t.value[2:])\n",
    "        return t\n",
    "\n",
    "    # Attribute starts key and colon. Returns 2-tuple (key, value).\n",
    "    @_(r'(?m)^[a-z][a-z0-9_]+: .*$')\n",
    "    def ATTRIBUTE(self, t):\n",
    "        t.value = tuple(t.value.split(': '))\n",
    "        return t\n",
    "    \n",
    "    # Footnote starts with '[^n]: ', where n is a number.\n",
    "    # Returns a 2-tuple (int: fn_sym, str: footnote_text)\n",
    "    @_(r'(?m)^\\[\\^[1-9][0-9]*\\]: .*$')\n",
    "    def FOOTNOTE(self, t):\n",
    "        fn_sym, footnote = t.value.split(maxsplit=1)\n",
    "        t.value = (int(fn_sym[2:-2]), footnote)\n",
    "        return t\n",
    "\n",
    "    # How to get combined Unicode characters to be recognized?\n",
    "    # Matching only Unicode points of letters with pre-combined\n",
    "    # marks can be done with the 'word' class '\\w', but it\n",
    "    # includes digits and underscore. To remove those, negate\n",
    "    # the inverted word class along with digits and underscore:\n",
    "    # '[^\\W\\d_]. But that does not include separate combining\n",
    "    # marks, or the '+' sign.\n",
    "    # One solution would be unicodedata.normalize('NFC', data),\n",
    "    # except that not all combinations have pre-combined Unicode\n",
    "    # points.\n",
    "    # Another solution is to use an external regex engine such as\n",
    "    # `regex` (`pip install regex`), which has better Unicode\n",
    "    # support. However, I would like to avoid extra dependencies.\n",
    "    # Another (less elegant) solution is to make the '+' symbol\n",
    "    # and the combining characters [\\u0300-\\u036F] each its own\n",
    "    # token, which the parser will have to parse into morphemes\n",
    "    # and words.\n",
    "    # Another (also less elegant) solution is to use a 'negative\n",
    "    # lookbehind assertion' for the negation of digits and '_':\n",
    "    # https://stackoverflow.com/a/12349464/9230612\n",
    "    # (?!\\d_)[\\w\\u0300-\\u036F]+\n",
    "    # Because combining marks can never appear before the first\n",
    "    # letter, and because some dialects have a '+' sign at the\n",
    "    # beginning of some words, we prefix an optional '+' symbol\n",
    "    # and an obligatory '[^\\W\\d_]' before the negative lookbehind.\n",
    "    \n",
    "    # One letter with (or without) combining marks can be matched\n",
    "    # with: [^\\W\\d_][\\u0300-\\u036F]*\n",
    "    # We also add a superscript plus (U-207A) as part of a letter, \n",
    "    # since this char is not a letter on its own, but rather\n",
    "    # modifies the quality of a consonant\n",
    "    LETTER = r'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*'\n",
    "    \n",
    "    # we try to make a LETTERS token:\n",
    "#     LETTERS = r'[+]?[^\\W\\d_](?!\\d_)[\\w\\u0300-\\u036F+]*'\n",
    "    # Unfortunately, with python's `re` it seems impossible to repeat\n",
    "    # a group like this. So we will group the letters in the parser.\n",
    "    \n",
    "    # Newlines: boundaries of paragraphs and metadata are marked\n",
    "    # with two newlines (meaning an empty line). The empty line\n",
    "    # may contain whitespace.\n",
    "    NEWLINES = r'\\n\\s*\\n\\s*'\n",
    "    # Space is any successive number of whitespace symbols.\n",
    "    SPACE = r'\\s+'\n",
    "    # One or more digits, not starting with zero\n",
    "    DIGITS = r'[1-9][0-9]*'\n",
    "    # Line id is any number of digits surrounded by round brackets\n",
    "#     LINE_ID = r'\\([0-9]+\\)'  # TODO convert to int?\n",
    "    # Punctuation is any normal punctuation symbol and vertical bar.\n",
    "    PUNCTUATION = r'[.,?!:;–\\u02c8]'\n",
    "    # There are two different hyphens, a single one and a double one.\n",
    "    # The double one is the 'equals' sign.\n",
    "    HYPHEN = r'[-=]'\n",
    "    # Language markers are ASCII letter strings surrounded by\n",
    "    # angle brackets.\n",
    "    LANG_MARKER = r'<[A-Za-z]+>'\n",
    "    # A special comment starts with an opening bracket, capital initials\n",
    "    # and a colon.\n",
    "    LPAREN_COMMENT = r'\\([A-Za-z]+: '\n",
    "    LBRACKET_COMMENT = r'\\[[A-Za-z]+: '\n",
    "    # A regular comment is text (at least one character not being a digit)\n",
    "    # which may not contain a colon (otherwise it becomes a special comment/interruption)\n",
    "    COMMENT = r'\\([^:)]*[^:)\\d]+[^:)]*\\)'\n",
    "\n",
    "lexer_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-\\u207Aword...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "Hello\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph. a\\u0300\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# demonstration of output results of lexer, to be used by parser below\n",
    "lexer = NenaLexer()\n",
    "[(tok.type, tok.value) for tok in lexer.tokenize(lexer_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a `.nena` file\n",
    "\n",
    "Below is a representation of the tree-like structure of a NENA standard text file. This is the structure that the parser must recognize and reproduce.\n",
    "\n",
    "```\n",
    "text\n",
    "  |\n",
    "  heading\n",
    "  |  |\n",
    "  |  attributes\n",
    "  |    |\n",
    "  |    attribute (e.g. title, informant, etc.)\n",
    "  | \n",
    "  paragraphs\n",
    "    |\n",
    "    paragraph\n",
    "    |  |\n",
    "    |  lines\n",
    "    |    |\n",
    "    |    line\n",
    "    |      |\n",
    "    |      line elements (in any order)\n",
    "    |        |\n",
    "    |        word elements\n",
    "    |        |  |\n",
    "    |        |  morpheme normal (+metadata, e.g. trailer, etc.)\n",
    "    |        |  |\n",
    "    |        |  morpheme foreign (+metadata)\n",
    "    |        |  |\n",
    "    |        |  morpheme language (+metadata)\n",
    "    |        |\n",
    "    |        footnote\n",
    "    |        |\n",
    "    |        comment\n",
    "    |        |\n",
    "    |        interruption\n",
    "    |      \n",
    "    orphaned footnote (processed later)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morpheme class\n",
    "\n",
    "To conveniently store the morpheme and its features, we prepare a small `Morpheme` class, to be used by the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morpheme:\n",
    "    \n",
    "    def __init__(self, value, trailer='',\n",
    "                 footnotes=None, speaker=None,\n",
    "                 foreign=False, lang=None):\n",
    "        self.value = value  # list of (combined) characters\n",
    "        self.trailer = trailer  # str (TODO: make this a list as well?)\n",
    "        self.footnotes = footnotes if footnotes is not None else {}  # dict\n",
    "        self.speaker = speaker  # str\n",
    "        self.foreign = foreign  # boolean\n",
    "        self.lang = lang  # str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ''.join(self.value)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        sp = f' speaker {self.speaker!r}' if self.speaker else ''\n",
    "        fr = ' foreign' if self.foreign else ''\n",
    "        ln = f' lang {self.lang!r}' if self.lang else ''\n",
    "        fn = f' fn_anc {\",\".join(str(n) for n in self.footnotes)!r}' if self.footnotes else ''\n",
    "        fn = f' fn_anc {self.footnotes!r}' if self.footnotes else ''\n",
    "        return f'<Morpheme {str(self)!r} trailer {self.trailer!r}{sp}{fr}{ln}{fn}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser\n",
    "\n",
    "The parser processes the tokens provided by the lexer, and tries to combine them into structured units. Those units are defined in the methods of the `NenaParser` class, with the patterns passed as arguments to the `@_` decorator.\n",
    "\n",
    "The top unit (in this case, `text`) is returned as the result of the parsing, and in this case contains a tuple `(heading, paragraphs)`.\n",
    "\n",
    "The value `heading` contains a dictionary with the text metadata. The value `paragraphs` is a list, in which each element contains a list of `lines`. Each element of `lines` is a 2-tuple containing an `int` line identifier, and a list of `line_elements`. The values of `line_elements` are `Morpheme` objects, or 2-tuples with comments in the form `('comment', str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 17 shift/reduce conflicts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'title': 'Gozáli and Nozali',\n",
       "  'text_id': 'A8',\n",
       "  'informant': 'Nanəs Bənyamən',\n",
       "  'place': 'ʾƐn-Nune'},\n",
       " [[(1,\n",
       "    [<Morpheme 'a' trailer '-'>,\n",
       "     <Morpheme '⁺test' trailer ' '>,\n",
       "     <Morpheme 'word' trailer '... ' fn_anc {1: 'First footnote.'}>,\n",
       "     ('comment', 'a-comment'),\n",
       "     <Morpheme 'lalala' trailer ' ' speaker 'GK'>,\n",
       "     <Morpheme 'bla' trailer ' //'>,\n",
       "     <Morpheme 'blatwo' trailer ' '>]),\n",
       "   (2,\n",
       "    [<Morpheme 'also' trailer ' ' fn_anc {2: 'Second footnote.'}>,\n",
       "     <Morpheme 'wórds' trailer '.' foreign lang 'E'>])],\n",
       "  [(4,\n",
       "    [<Morpheme 'new' trailer '-'>,\n",
       "     <Morpheme 'paragraph' trailer '. '>,\n",
       "     <Morpheme 'tséntr' trailer '-' foreign lang 'R'>,\n",
       "     <Morpheme 'ət' trailer ''>])],\n",
       "  ('footnotes', {3: 'Third footnote, not referenced in text.'})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sly import Parser\n",
    "\n",
    "# dict stack to contain footnote anchors,\n",
    "# until the corresponding footnote is encountered.\n",
    "fn_anchors = {}\n",
    "\n",
    "class NenaParser(Parser):\n",
    "    \n",
    "    # debugfile = 'parser.out'\n",
    "\n",
    "    # Get the token list from the lexer (required)\n",
    "    tokens = NenaLexer.tokens\n",
    "    \n",
    "    def error(self, t):\n",
    "        raise Exception(f'unexpected string {repr(t.value[0])} at index {t.index}')\n",
    "    \n",
    "    @_('heading NEWLINES paragraphs')\n",
    "    def text(self, p):\n",
    "        return (p.heading, p.paragraphs)\n",
    "    \n",
    "    # -- HEADING --\n",
    "    \n",
    "    @_('SPACE TITLE NEWLINES attributes',\n",
    "       'TITLE NEWLINES attributes')\n",
    "    def heading(self, p):\n",
    "        key, value = p.TITLE\n",
    "        heading = {key: value}\n",
    "        heading.update(p.attributes)\n",
    "        return heading\n",
    "    \n",
    "    @_('attributes space ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        p.attributes[key] = value\n",
    "        return p.attributes \n",
    "    \n",
    "    @_('ATTRIBUTE')\n",
    "    def attributes(self, p):\n",
    "        key, value = p.ATTRIBUTE\n",
    "        return {key: value}\n",
    "    \n",
    "    # -- PARAGRAPHS --\n",
    "    \n",
    "    @_('paragraphs NEWLINES paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        # handle cases of null footnotes\n",
    "        if p.paragraph is not None:\n",
    "            return p.paragraphs + [p.paragraph]\n",
    "        else:\n",
    "            return p.paragraphs\n",
    "        \n",
    "    @_('paragraph')\n",
    "    def paragraphs(self, p):\n",
    "        return [p.paragraph]\n",
    "    \n",
    "    # paragraph\n",
    "    @_('paragraph line')\n",
    "    def paragraph(self, p):\n",
    "        return p.paragraph + [p.line]\n",
    "    \n",
    "    # paragraph from orphaned footnotes\n",
    "    @_('footnotes')\n",
    "    def paragraph(self, p):\n",
    "        if p.footnotes:\n",
    "            # TODO: issue log warning about\n",
    "            # unreferenced footnotes?\n",
    "            return ('footnotes', p.footnotes)\n",
    "    \n",
    "    # -- FOOTNOTES -- \n",
    "    \n",
    "    @_('footnotes footnote')\n",
    "    def footnotes(self, p):\n",
    "        p.footnotes.update(p.footnote)\n",
    "        return p.footnotes\n",
    "    \n",
    "    @_('footnote')\n",
    "    def footnotes(self, p):\n",
    "        return p.footnote\n",
    "    \n",
    "    @_('FOOTNOTE space NEWLINES',\n",
    "       'FOOTNOTE NEWLINES',\n",
    "       'FOOTNOTE space',\n",
    "       'FOOTNOTE')\n",
    "    def footnote(self, p):\n",
    "        fn_sym, fn_str = p.FOOTNOTE\n",
    "        footnote = {}\n",
    "        try:\n",
    "            # lookup the fn_sym key in the fn_anchors dict,\n",
    "            # and add the footnote to the appropriate morpheme\n",
    "            fn_morpheme = fn_anchors.pop(fn_sym)\n",
    "            fn_morpheme.footnotes[fn_sym] = fn_str\n",
    "        except KeyError:\n",
    "            # This means there is not footnote anchor\n",
    "            # referring to this footnote. So we return\n",
    "            # the footnote to the text\n",
    "            footnote = {fn_sym: fn_str}\n",
    "        return footnote\n",
    "\n",
    "    # -- LINES --\n",
    "    \n",
    "    @_('line')\n",
    "    def paragraph(self, p):\n",
    "        return [p.line]\n",
    "    \n",
    "    @_('line_id line_elements')\n",
    "    def line(self, p):\n",
    "        return (p.line_id, p.line_elements)\n",
    "    \n",
    "    @_('\"(\" DIGITS \")\" SPACE')\n",
    "    def line_id(self, p):\n",
    "        return int(p.DIGITS)\n",
    "\n",
    "    @_('line_elements line_element',\n",
    "       'line_element')\n",
    "    def line_elements(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.line_elements + p.line_element\n",
    "        else:\n",
    "            return p.line_element\n",
    "    \n",
    "    # -- MORPHEMES -- \n",
    "    \n",
    "    @_('morphemes',\n",
    "       'fn_anchor',\n",
    "       'interruption',\n",
    "       'morphemes_foreign',\n",
    "       'morphemes_language',\n",
    "       'comment')\n",
    "    def line_element(self, p):\n",
    "        return p[0]\n",
    "\n",
    "    # morphemes_language\n",
    "    @_('lang morphemes_foreign morpheme_trailer lang trailer',\n",
    "       'lang morphemes_foreign lang trailer',\n",
    "       'lang morphemes_foreign lang',\n",
    "       'lang morphemes_foreign')\n",
    "    def morphemes_language(self, p):\n",
    "        # check if language markers correspond\n",
    "        if len(p) > 2:\n",
    "            lang = p.lang0\n",
    "            if p.lang0 != p.lang1:\n",
    "                pass  # TODO issue warning: language markers do not correspond\n",
    "        else:\n",
    "            lang = p.lang  # TODO issue warning: missing second language marker\n",
    "        for m in p.morphemes_foreign:\n",
    "            m.lang = lang\n",
    "        if len(p) == 4:\n",
    "            p.morphemes_foreign[-1].trailer += p.trailer\n",
    "        elif len(p) == 5:\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "            p.morphemes_foreign.append(morpheme_trailer)\n",
    "        return p.morphemes_foreign\n",
    "    \n",
    "    # lang\n",
    "    @_('LANG_MARKER')\n",
    "    def lang(self, p):\n",
    "        return p.LANG_MARKER[1:-1]\n",
    "\n",
    "    # morphemes_foreign\n",
    "    # last morpheme may not include trailer\n",
    "    # add trailer after second asterisk to last morpheme\n",
    "    @_('\"*\" morphemes letters \"*\" trailer',\n",
    "       '\"*\" morphemes letters \"*\"',\n",
    "       '\"*\" letters \"*\" trailer',\n",
    "       '\"*\" letters \"*\"',\n",
    "      )\n",
    "    def morphemes_foreign(self, p):\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "        except KeyError:\n",
    "            trailer = ''\n",
    "        try:\n",
    "            morphemes = p.morphemes\n",
    "        except KeyError:\n",
    "            morphemes = []\n",
    "        morphemes.append(Morpheme(p.letters, trailer=trailer))\n",
    "        for m in morphemes:\n",
    "            m.foreign = True\n",
    "        return morphemes\n",
    "    \n",
    "    # comment\n",
    "    @_('COMMENT trailer',\n",
    "       'COMMENT')\n",
    "    def comment(self, p):\n",
    "        return [('comment', p.COMMENT[1:-1])]\n",
    "\n",
    "    # interruption\n",
    "    @_('LPAREN_COMMENT morphemes \")\" trailer',\n",
    "       'LPAREN_COMMENT morphemes \")\"',\n",
    "       'LBRACKET_COMMENT morphemes \"]\" trailer',\n",
    "       'LBRACKET_COMMENT morphemes \"]\"')\n",
    "    def interruption(self, p):\n",
    "        speaker = p[0][1:-2]\n",
    "        for m in p.morphemes:\n",
    "            m.speaker = speaker\n",
    "        try:\n",
    "            trailer = p.trailer\n",
    "            if (p.morphemes[-1].trailer.endswith(' ')\n",
    "                and trailer.startswith(' ')):\n",
    "                trailer = trailer[1:]\n",
    "            p.morphemes[-1].trailer += trailer\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return p.morphemes\n",
    "    \n",
    "    # morphemes\n",
    "    @_('morphemes morpheme_trailer',\n",
    "       'morpheme_trailer')\n",
    "    def morphemes(self, p):\n",
    "        if len(p) == 2:\n",
    "            return p.morphemes + [p.morpheme_trailer]\n",
    "        else:\n",
    "            return [p.morpheme_trailer]\n",
    "    \n",
    "    # -- MORPHEME ATTRIBUTES --\n",
    "    \n",
    "    # morpheme_trailer\n",
    "    @_('letters trailer',\n",
    "       'letters')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 2:\n",
    "            trailer = p[1]\n",
    "        else:\n",
    "            trailer = ''\n",
    "        return Morpheme(p.letters, trailer=trailer)\n",
    "\n",
    "    # morpheme_trailer with footnote anchor\n",
    "    @_('morpheme_trailer fn_anchor trailer',\n",
    "       'morpheme_trailer fn_anchor')\n",
    "    def morpheme_trailer(self, p):\n",
    "        if len(p) == 3:\n",
    "            if (p.morpheme_trailer.trailer.endswith(' ')\n",
    "                and p.trailer.startswith(' ')):\n",
    "                p.trailer = p.trailer[1:]\n",
    "            p.morpheme_trailer.trailer += p.trailer\n",
    "        # add dummy value {fn_anc: None} to footnote dict\n",
    "        p.morpheme_trailer.footnotes[p.fn_anchor] = None\n",
    "        # add morpheme object to fn_anchors dict,\n",
    "        # for easy access when footnote text is found\n",
    "        fn_anchors[p.fn_anchor] = p.morpheme_trailer\n",
    "        return p.morpheme_trailer\n",
    "    \n",
    "    # --VARIOUS--\n",
    "    \n",
    "    @_('\"[\" \"^\" DIGITS \"]\"')\n",
    "    def fn_anchor(self, p):\n",
    "        return int(p.DIGITS)\n",
    "        \n",
    "    @_('letters LETTER')\n",
    "    def letters(self, p):\n",
    "        return p.letters + [p.LETTER]\n",
    "    \n",
    "    @_('LETTER')\n",
    "    def letters(self, p):\n",
    "        return [p[0]]\n",
    "    \n",
    "    # trailer\n",
    "    @_('trailer versebreak',\n",
    "       'trailer linebreak',\n",
    "       'trailer PUNCTUATION',\n",
    "       'trailer space',\n",
    "       'PUNCTUATION',\n",
    "       'space',\n",
    "       'HYPHEN'\n",
    "      )\n",
    "    def trailer(self, p):\n",
    "        return ''.join(p)\n",
    "    \n",
    "    # -- LITERALS --\n",
    "    \n",
    "    # reduce any number of spaces (\\s+)\n",
    "    # to a single space (' ')\n",
    "    @_('SPACE')\n",
    "    def space(self, p):\n",
    "        return ' '\n",
    "    \n",
    "    @_('\"/\" \"/\"',\n",
    "       '\"/\" \"/\" space',\n",
    "       '\"/\" \"/\" NEWLINES',\n",
    "       '\"/\" \"/\" space NEWLINES')\n",
    "    def versebreak(self, p):\n",
    "        return '//'\n",
    "    \n",
    "    @_('\"/\"',\n",
    "       '\"/\" space',\n",
    "       '\"/\" NEWLINES',\n",
    "       '\"/\" space NEWLINES')\n",
    "    def linebreak(self, p):\n",
    "        return '/'\n",
    "    \n",
    "parser_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-\\u207Atest word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "blatwo\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new-paragraph. <R>*tséntr*<R>-ət\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "[^3]: Third footnote, not referenced in text.\n",
    "\"\"\"\n",
    "\n",
    "# demonstration of output results of parser, to be used by generate_TF loop\n",
    "parser = NenaParser()\n",
    "parser.parse(lexer.tokenize(parser_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(lexer.tokenize(parser_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser output\n",
    "\n",
    "The parser prints a warning that there were shift/reduce conflicts, probably caused by ambiguous whitespace. That is not a problem (although not very elegant, ideally it should be fixed). The parser resolves the conflicts automatically.\n",
    "\n",
    "The output of the example text shows that the parser succeeded to parse it, and structure it into heading, paragraphs, lines and morphemes, with the features stored in the Morpheme object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Real Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = Path('../nena_corpus/nena/0.01')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(next(dialect_dirs[1].glob('*')), 'r') as infile:\n",
    "    toy = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list((tok.type, tok.value) for tok in lexer.tokenize(toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.parse(lexer.tokenize(toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Parse On All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dialect ../nena_corpus/nena/0.01/Barwar--\n",
      "\n",
      "trying: A Hundred Gold Coins.nena\n",
      "\t√\n",
      "trying: A Man Called Čuxo.nena\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "unexpected string '1' at index 329",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e72bd4d1f7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mname2text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'trying: {file.name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mparseit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t√'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1935\u001b[0m                         \u001b[0merrtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookahead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m                     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1938\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                         \u001b[0;31m# User must have done some kind of panic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a71f3f7cd29b>\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'unexpected string {repr(t.value[0])} at index {t.index}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'heading NEWLINES paragraphs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: unexpected string '1' at index 329"
     ]
    }
   ],
   "source": [
    "parsed = []\n",
    "name2text = {}\n",
    "\n",
    "for dialect in dialect_dirs:\n",
    "    print(f'--Dialect {dialect}--')\n",
    "    print()\n",
    "    for file in sorted(dialect.glob('*.nena')):\n",
    "        with open(file, 'r') as infile:\n",
    "            text = infile.read()\n",
    "            name2text[file.name] = text\n",
    "            print(f'trying: {file.name}')\n",
    "            parseit = parser.parse(lexer.tokenize(text))\n",
    "            print(f'\\t√')\n",
    "            parsed.append(parseit)\n",
    "                \n",
    "print('success rate:')\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[^1]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[^{}]'.format(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ə̀slən,ˈ gu-Bɛ̀rwər.ˈ ṛiyət-ʾə̀rwɛwa,ˈ baqàrɛwa.ˈ kùlla-məndi wéwa.ˈ\n",
      "făqìrɛwa.ˈ šə́mme díye nášət máθa y-amríwale Čùxo.ˈ1 ʾáwwa kú-mdit mṣawə́θwa\n",
      "Čùxo,ˈ Čùxo,ˈ Čùxo.ˈ\n",
      "(2) xá-yoma qə́ryɛle mə́re ṭla-réšət màθaˈ ʾáwwa šə́mma mšaxəlpùle.ˈ ʾáwwa šə́mmi\n",
      "Čùxo,ˈ Čùxo,ˈ Čùxo.ˈ mattúl\n"
     ]
    }
   ],
   "source": [
    "see = name2text['A Man Called Čuxo.nena'][200:500]\n",
    "print(see)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TextFabric\n",
    "\n",
    "The output of the parser can be very easily converted to TextFabric format, as most elements are already separated and structured, which removes a lot of checking and matching from the conversion script, as it is done by the parser already. Some extra structuring, such as division into sentences, subsentences, prosaic units, and words, must be done in the conversion script, based on the contents of the `trailer` attribute.\n",
    "\n",
    "Below an (incomplete) example of a loop converting the parser output to something TextFabric can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test = \"\"\"\n",
    "# Gozáli and Nozali\n",
    "\n",
    "text_id: A8\n",
    "informant: Nanəs Bənyamən\n",
    "place: ʾƐn-Nune\n",
    "\n",
    "(1) a-+word...[^1] (a-comment) (GK: lalala) bla //\n",
    "\n",
    "bla\n",
    "(2) also[^2] <E>*wórds*<E>.\n",
    "\n",
    "(4) new paragraph.\n",
    "\n",
    "[^1]: First footnote.\n",
    "[^2]: Second footnote.\n",
    "\"\"\"\n",
    "\n",
    "heading, paragraphs = parser.parse(lexer.tokenize(tf_test))\n",
    "\n",
    "# raw_features['title'][this_text] = heading['title']\n",
    "# ... etc.\n",
    "\n",
    "# initialize counters (will be increased to start from 1)\n",
    "this_paragraph = 0\n",
    "this_line = 0\n",
    "this_sentence = 0\n",
    "this_subsentence = 0\n",
    "this_word = 0\n",
    "this_morpheme = 0\n",
    "this_prosa = 0\n",
    "\n",
    "slot = 0 # i.e. chars\n",
    "\n",
    "# Mark units that are increased upon their 'ending' boundaryas 'ended',\n",
    "# so their counters will be increased on first morpheme\n",
    "sentence_end = True\n",
    "subsentence_end = True\n",
    "prosa_end = True\n",
    "word_end = True\n",
    "\n",
    "for p in paragraphs:\n",
    "    if type(p) is tuple:\n",
    "        # key, value = p  # unreferenced footnotes are passed as paragraph tuples\n",
    "        # and can be ignored\n",
    "        continue\n",
    "    this_paragraph += 1\n",
    "    for line_id, line in p:\n",
    "        this_line += 1\n",
    "        for m in line:\n",
    "            if type(m) is tuple:\n",
    "                # key, value = line_element  # comments are passed as tuples\n",
    "                # to be ignored?\n",
    "                continue\n",
    "            \n",
    "            # increase counters\n",
    "            this_morpheme += 1\n",
    "            \n",
    "            # increase counters of ended units\n",
    "            if sentence_end:\n",
    "                this_sentence += 1\n",
    "            if subsentence_end:\n",
    "                this_subsentence += 1\n",
    "            if prosa_end:\n",
    "                this_prosa += 1\n",
    "            if word_end:\n",
    "                this_word += 1\n",
    "            \n",
    "            for c in m.value:\n",
    "                slot += 1\n",
    "                \n",
    "                # add main character features:\n",
    "                # pretty_c = unicodedata.normalize('NFC', c)  # make pretty utf8 char text\n",
    "                # trans_c = translate(c, transcr_table)  # character in transcription\n",
    "                # raw_features['utf8'][slot] = pretty_c\n",
    "                # raw_features['trans'][slot] = trans_c\n",
    "                \n",
    "                # and other char features from Morpheme object `m`:\n",
    "                # if m.speaker:\n",
    "                #     raw_features['speaker'][slot] = m.speaker\n",
    "                # if m.foreign:\n",
    "                #     raw_features['language'][slot] = m.lang or ''\n",
    "            \n",
    "            # the last character of a `morpheme` gets its `trailer` and `footnotes`:\n",
    "            # raw_features['trailer'][slot] = m.trailer.replace('|', '\\u02c8')\n",
    "            # if any(m.footnotes.values()):\n",
    "            #     raw_features['footnotes'][slot] = '\\n'.join(m.footnotes.values())\n",
    "                \n",
    "            # check for unit ends\n",
    "            if (any(c in m.trailer for c in '.!?')\n",
    "                or m.trailer.endswith('//')):\n",
    "                sentence_end = True\n",
    "                subsentence_end = True\n",
    "            if (any(c in m.trailer for c in ',;:')\n",
    "                or m.trailer.endswith('/')):\n",
    "                subsentence_end = True\n",
    "            if '|' in m.trailer:\n",
    "                prosa_end = True\n",
    "            if m.trailer not in ('-', '=', ''):\n",
    "                word_end = True\n",
    "                # m.trailer == '' should only occur at end of paragraph.\n",
    "                # TODO issue a warning if it occurs elsewhere? (Better in parser?)\n",
    "                \n",
    "    # end of paragraph also ends sentence, subsentence, prosa, and word units\n",
    "    sentence_end = True\n",
    "    subsentence_end = True\n",
    "    prosa_end = True\n",
    "    word_end = True        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- [x] ~~implement 'foreign' marker `*`~~\n",
    "- [x] ~~implement language marker `<Marker>`~~\n",
    "- [x] ~~implement line and verse breaks `/` and `//`~~\n",
    "- [x] ~~implement footnotes~~\n",
    "\n",
    "ISSUES\n",
    "\n",
    "The parser does not enforce all parts of the grammar. For example, as verse and line breaks are just appended to the trailer, nothing will stop it from adding other trailer elements (save whitespace) after it. There is also no check to see whether the two `LANG_MARKER`s have the same value. A `+` sign must appear as the first character in a `morpheme`, but that just means that a `+` in the middle of a morpheme breaks it into two morphemes, instead of invalidating it. Undoubtedly there are more issues like this.\n",
    "\n",
    "Paragraphs in which the first line lacks a `line_id` break the parser. That is true for e.g. the first line of the text in which the `line_id` is absent, or for poetic style text with no `//` verse break marker but with empty line dividing verses. This could (should?) be handled by fixing the issue (default `line_id=1` for first line, default `//` verse break for empty lines within `line`), and issuing a warning notifying the user of the automatic fix.\n",
    "\n",
    "Footnotes can only be one line and the string is not processed (e.g. markup like `*emphasis*` is kept as is).\n",
    "\n",
    "Footnote anchors can now only occur after a `morpheme`, not after other things like `comment` or `interruption`. (note to self: possible solution: include `fn_anc` in `morphemes` instead of `morpheme_trailer`, and put `comment` in `trailer`).\n",
    "\n",
    "Comments and unreferenced footnotes are returned as tuples for now, and have to be filtered out in the loop.\n",
    "\n",
    "QUESTIONS\n",
    "\n",
    "Some questions require answers for implementation. They need not be definitive answers for now, but they should be motivated somehow (even if the motivation is 'random choice'), so it will be clear later why it is done in one way or another.\n",
    "\n",
    "- How to store hyphen? Now it is stored as a character in a word occuring between morphemes (I think).\n",
    "  \n",
    "  Should it be the trailer of the morpheme?\n",
    "\n",
    "\n",
    "- How to split sentences?\n",
    "\n",
    "  Now sentences are split on .?! and subsentences on ,\n",
    "  There are other symbols: ;:– and even .. ... ..., .... ..... (If I recall correctly). Should those split\n",
    "  sentences or subsentences?\n",
    "\n",
    "\n",
    "- What to do with poetic line breaks and sentence/paragraph boundaries?\n",
    "\n",
    "  I think a 'poem' should not be divided into paragraphs. I suggest that a line break '/' is a subsentence division, and a verse break '//' a sentence division (even when in the source it is followed by an empty line). If there is a verse number in between, that automatically starts a new sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
