{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Barwar lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from html_to_nena import html_elements, Text, element_totext, normalize_styles, text_tostring, str_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace substrings provided in `replace`\n",
    "# if replace:\n",
    "#     s = str_replace(s, replace)\n",
    "replace = {\n",
    "    # standardizing substutions\n",
    "    '\\u2011': '\\u002d',  # non-breaking hyphen to normal hyphen-minus\n",
    "    '\\u01dd': '\\u0259',  # 'ǝ' \"turned e\" to 'ə' schwa\n",
    "    '\\uf1ea': '\\u003d',  # SIL deprecated double hyphen to '=' equals sign\n",
    "    '\\u2026': '...',  # '…' horizontal ellipsis to three dots\n",
    "    'J\\u0335': '\\u0248',  # 'J' + short combining stroke to 'Ɉ' J with stroke\n",
    "    'J\\u0336': '\\u0248',  # J' + long combining stroke to 'Ɉ' J with stroke\n",
    "    '<y>': '\\u02b8',  # superscript small letter y\n",
    "    # special corrections for lexicon\n",
    "    '*\\u02b8*': '\\u02b8',  # superscript small letter y in italic context\n",
    "    '<|>': '\\u02c8',  # superscript vertical bar\n",
    "    '*\\u02c8 *': '\\u02c8 ',  # superscript vertical bar in italic text\n",
    "    '*\\u02c8': '\\u02c8*',  # superscript vertical bar after italic text\n",
    "    ' |** ': '** | ',  # vertical bar accidentally marked bold\n",
    "    ' **|** ': ' | ',  # vertical bar accidentally marked bold\n",
    "    ' *| ': ' | *',  # vertical bar accidentally marked italic\n",
    "    ' | (': ' (',  # superfluous vertical bar before numbered translations\n",
    "    '**; **': '; ', # unmarked punctuation between marked text\n",
    "    '*, *': ', ', # unmarked punctuation between marked text\n",
    "    '**.**': '.', # dot accidentally marked bold\n",
    "}\n",
    "\n",
    "replace = {re.escape(pattern): replace for pattern, replace in replace.items()}\n",
    "\n",
    "filename = 'bar glossary general.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the strings that can make up the grammatical categories of words\n",
    "# in a non-capturing group (starting with ?:) of alternatives, sorted longest first\n",
    "grm_cats = '(?:abs|and|adj|adv|cst|cs|fpl|fs|f|imper|interj|invar|mod|ms|m|num|n|part|pl|prep|pron|sing|tant|tan)'\n",
    "\n",
    "languages = '(?:A|E|K|P|Ṭiy|Ṭiy|T|Urm|C.Syr)'\n",
    "\n",
    "def parselemma(s):\n",
    "        \n",
    "    raw_entry = s\n",
    "        \n",
    "    # first try to strip off numbered list of meanings: (1) ... (2) ... etc\n",
    "    result = re.split(' (\\(\\d+\\)) ', s)\n",
    "    # if not found, try other pattern for numbered list: 1. ... 2. ... etc\n",
    "    # (in Barwar lexicon: only for one lemma: 'kapora')\n",
    "    if len(result) == 1:\n",
    "        result = re.split(' (\\d+\\.) ', s)\n",
    "    # if there are any meanings, recombine with their numbers in list of strings;\n",
    "    # otherwise, empty list\n",
    "    meanings = [f'{result[i]} {result[i+1]}' for i in range(1, len(result), 2)]\n",
    "    # reassign s to part of string before numbered meanings (if any)\n",
    "    s = result[0]\n",
    "    \n",
    "    # then try to strip off any examples, separated from the lemma by '|'\n",
    "    result = re.split('\\s*\\|\\s*', s, 1)\n",
    "    # examples will be empty string if there are none\n",
    "    examples = result[1] if len(result) > 1 else ''\n",
    "    # reassign s to part of string before examples (if any)\n",
    "    s = result[0]\n",
    "    \n",
    "    # split on the first bold marker `**` to get the translation.\n",
    "    # since bold and italic text messes up the markers,\n",
    "    # cannot just select from start to end.\n",
    "    result = re.split('\\s*(\\*\\*)\\s*', s, 1)\n",
    "    if len(result) > 1:\n",
    "        # small fix for when opening brackets of translation aren't marked bold\n",
    "        if result[0].endswith(' ('):\n",
    "            result[0] = result[0][:-2]\n",
    "            result[2] = '(' + result[2]\n",
    "        trans = result[1] + result[2]\n",
    "    else:\n",
    "        trans = ''\n",
    "    s = result[0]\n",
    "    \n",
    "    # split on ' → ' for references\n",
    "    result = re.split('\\s*→\\s*', s, 1)\n",
    "    # reference will be empty string if there is none\n",
    "    reference = result[1] if len(result) > 1 else ''\n",
    "    # reassign s to part of string before reference (if any)\n",
    "    s = result[0]\n",
    "    \n",
    "    # now, we will continue by looking at the start of the string for the lemma head\n",
    "    result = re.split('^\\*([^*]+)\\*\\s*', s)\n",
    "    if len(result) == 1:\n",
    "        raise ValueError('No lemma at start of string:', s)\n",
    "    lemma = result[1]\n",
    "    s = result[2]\n",
    "    \n",
    "    # some lemmata have a roman numeral following it (though it seems unnecessary)\n",
    "    result = re.split('^\\(([ivx]+)\\)\\s*', s)\n",
    "    numeral = result[1] if len(result) > 1 else ''\n",
    "    # last element of result contains remaining string, whether numeral is found or not\n",
    "    s = result[-1]\n",
    "    \n",
    "    # now s should start with a grammatical category of a word, such as 'n.f.'.\n",
    "    # remove grammatical categories from the start of the string until there are no more\n",
    "#     grm_desc = ''\n",
    "#     while s:\n",
    "#         result = re.split(f'^({grm_cats}(?:[ /.,]+|$))', s)\n",
    "#         if len(result) == 1:\n",
    "#             break\n",
    "#         # if there is a match, element 0 is always empty (since match is from start of string);\n",
    "#         # element 1 contains the match, element 2 the remainder of the string\n",
    "#         grm_desc += result[1]\n",
    "#         s = result[2]\n",
    "#     grm_desc = grm_desc.rstrip()\n",
    "    # like above, but with repeated non-capturing group instead of while loop\n",
    "    result = re.split(f'^((?:{grm_cats}(?:[.,/ ]+|$))+)', s)\n",
    "    # element 0 is always empty (bcs of '^' in regex); if match, element 1 contains match\n",
    "    grm_desc = result[1].rstrip() if len(result) > 1 else ''\n",
    "    # last element of result contains remaining string, whether match is found or not\n",
    "    s = result[-1]\n",
    "        \n",
    "    # split on round brackets to get annotations for lemma.\n",
    "    # even elements in position >= 2 should be empty\n",
    "    # e.g.: '*word* n.m. (pl. *words*) (E.)' results in:\n",
    "    # ['*word* n.m.', 'pl. *words*', '', 'E.', '']\n",
    "    result = re.split('\\s*\\(([^)]+)\\)\\s*', s)\n",
    "    # list of annotations, if any, else empty\n",
    "    annotations = [result[i] for i in range(1, len(result), 2)]\n",
    "    \n",
    "    # list of things in between (should all be empty strings! just checking!)\n",
    "    inbetweens = [result[i] for i in range(2, len(result), 2) if result[i]]\n",
    "    \n",
    "    # everything after lemma and before annotations is the tail\n",
    "    tail = result[0]\n",
    "    \n",
    "    # extract forms from annotations\n",
    "    resultlist = []\n",
    "    forms = []\n",
    "    for e in annotations:\n",
    "        # split word forms in format 'f. *worda, wordb*, pl. *wordc*' from start of string\n",
    "        result = re.split(fr'^((?:(?:{grm_cats}\\.?\\s+)+\\*[^*]+\\*(?:[,;]?\\s|$))+)', e)\n",
    "        match = result[1] if len(result) > 1 else None\n",
    "        resultlist.append(result[-1])\n",
    "        if match:\n",
    "            # split matched string into seperate word forms and their grammatical decribers\n",
    "            result = re.split(fr'((?:{grm_cats}\\.?\\s+)+)\\*([^*]+)\\*(?:[,;]?\\s|$)', match)\n",
    "            # the above example results in:\n",
    "            # ['', 'f.', 'worda, wordb', '', 'pl.', 'ʾwordc', '']\n",
    "            # The first regex selecting the whole string makes sure that nothing can occur\n",
    "            # in the elements 0, 3, 6 etc., so we can safely skip those\n",
    "            forms += [(result[i].rstrip(), result[i+1]) for i in range(1, len(result), 3)]\n",
    "    annotations = [e for e in resultlist if e]\n",
    "\n",
    "    # extract languages from annotations\n",
    "    resultlist = []\n",
    "    lang = ''\n",
    "    for e in annotations:\n",
    "        # make sure that only one language string is returned\n",
    "        if lang:\n",
    "            resultlist.append(e)\n",
    "            continue \n",
    "        result = re.split(fr'^((?:{languages}\\.)(?:/{languages}\\.)*)', e)\n",
    "        if not lang and len(result) > 1:\n",
    "            lang = result[1]\n",
    "            break \n",
    "        resultlist.append(result[-1])\n",
    "    annotations = [e for e in resultlist if e]\n",
    "    \n",
    "    # TODO most stuff is extracted. Some stuff however does not fit the regexes.\n",
    "    # (Remains of) unconventional notations can be seen in the tail, annotations\n",
    "    # and (especially) the inbetweens variables. Some of these may require manual\n",
    "    # corrections, others may be parsed by more sophisticated regexes.\n",
    "    # (e.g.: alternative forms in the notation: 'sing. *wordform* f.')\n",
    "    \n",
    "    # TODO Both `meanings` and `examples` are mostly structured to some degree.\n",
    "    # They could be parsed further (e.g. `meanings` usually contain examples).\n",
    "    \n",
    "    # TODO The `grm_desc` should be parsed further, now they are just strings,\n",
    "    # e.g. 'n.m.' for the head word, and 'f.' for an alternative form.\n",
    "    # It should be clearer that the head word is e.g. noun, masc, sing and that\n",
    "    # the alternative form is e.g. noun, fem, sing.\n",
    "    \n",
    "    parsed = {\n",
    "        'lemma': lemma,\n",
    "        'numeral': numeral,\n",
    "        'grm_desc': grm_desc,\n",
    "        'forms': forms,\n",
    "        'trans': trans,\n",
    "        'tail': tail,\n",
    "        'lang': lang,\n",
    "        'annotations': annotations,\n",
    "        'inbetweens': inbetweens,\n",
    "        'ref': reference,\n",
    "        'examples': examples,\n",
    "        'meanings': meanings,\n",
    "        'raw_entry': raw_entry,\n",
    "    }\n",
    "    parsed = {k:v for k,v in parsed.items() if v}\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmata = []\n",
    "\n",
    "for e in html_elements(filename):\n",
    "    t = Text(element_totext(e))\n",
    "    t = normalize_styles(t, can_have_emphasis=lambda c: not c.isspace())\n",
    "    s = text_tostring(t)\n",
    "    s = str_replace(s, replace)\n",
    "\n",
    "    if not s or s.isdigit():\n",
    "        continue\n",
    "    elif not s.startswith('*'):\n",
    "        continue\n",
    "        if s.startswith('/'):\n",
    "            print('##', s)\n",
    "        else:\n",
    "            print('#', s)\n",
    "    else:\n",
    "        parsed = parselemma(s)\n",
    "        #lemma_str = '\\n'.join(f'{k}: {v!r}' for k, v in parsed.items() if v)\n",
    "        lemmata.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bar glossary general.json', 'w') as outfile:\n",
    "    json.dump(lemmata, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'pažgir',\n",
       " 'grm_desc': 'n.f.',\n",
       " 'forms': [('pl.', 'pažgire')],\n",
       " 'trans': '**towel**',\n",
       " 'lang': 'K.',\n",
       " 'raw_entry': '*pažgir* n.f. (pl. *pažgire*) (K.) **towel**'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_form('pažgir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bads = [l for l in lemmata if {'annotations', 'inbetweens', 'tail'} & set(l.keys())]\n",
    "\n",
    "len(bads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
