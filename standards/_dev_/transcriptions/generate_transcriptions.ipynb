{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Transcription Formats\n",
    "\n",
    "Transcriptions should be a simple regex pattern to string mapping within a dictionary.\n",
    "Where each regex pattern corresponds with an acceptable letter or punctuator in the \n",
    "NENA standards library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "corpus_dir = Path.home().joinpath('github/CambridgeSemiticsLab/nena_corpus')\n",
    "standards_dir = corpus_dir.joinpath('standards')\n",
    "alphabet_json = standards_dir.joinpath('alphabet/alphabet.json')\n",
    "punctuation_json = standards_dir.joinpath('punctuation/punctuation.json')\n",
    "transcription_dir = standards_dir.joinpath('transcriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriber:\n",
    "    \"\"\"Transcribe a string according to transcription rules.\n",
    "    \n",
    "    This transcription class is essentially a filter\n",
    "    which determines which characters make it into a new,\n",
    "    transcribed string. The filter is applied on a letter-by-letter \n",
    "    basis. A \"letter\" (token) is defined by the `tokens` argument \n",
    "    and can include diacritics/accents. The filter is applied \n",
    "    in one of three methods:\n",
    "        1. replacements on a unicode composed letter (NFC)\n",
    "        2. or replacements on punctuation if letter is punctuation\n",
    "        3. or replacements on a unicode decomposed letter (NFD)\n",
    "    The changes are added to a new string which is then returned.\n",
    "    \n",
    "    __init__(tokens, replacements, punctuation, keep):\n",
    "        string: a string to transcribe\n",
    "        tokens: regex for splitting letters (tokens)\n",
    "            to be used with findall\n",
    "        replacements: dict with find:replace mappings\n",
    "        keep: regex for characters to keep\n",
    "            \n",
    "    Returns:\n",
    "        str in transcribed form\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens='', replace={}, \n",
    "                 punctuation='', keep='', keep_case=False, apply_function=None):    \n",
    "        self.tokenize = re.compile(f'{tokens}|{punctuation}').findall\n",
    "        self.punct = re.compile(punctuation)\n",
    "        self.keep = re.compile(keep)\n",
    "        self.keep_case = keep_case\n",
    "        self.apply_function = apply_function\n",
    "        \n",
    "        # ensure normalized characters for pattern searches\n",
    "        self.repl = {\n",
    "            unicodedata.normalize('NFC',f):r \n",
    "                for f,r in replace.items()\n",
    "        }\n",
    "        \n",
    "    def convert(self, string, normalize='NFC'):\n",
    "        \"\"\"Convert string to transcription.\n",
    "        \n",
    "        Returns:\n",
    "            str in transcribed form\n",
    "        \"\"\"\n",
    "        \n",
    "        string = unicodedata.normalize('NFC',string)\n",
    "        if not self.keep_case:\n",
    "            string = string.lower()\n",
    "        transcription = ''\n",
    "\n",
    "        for token in self.tokenize(string):\n",
    "\n",
    "            # filter a composed string\n",
    "            if token in self.repl:\n",
    "                transcription += self.repl[token]\n",
    "\n",
    "            # keep punctuation\n",
    "            elif self.punct.match(token):\n",
    "                transcription += token\n",
    "\n",
    "            # filter at \n",
    "            else:\n",
    "                for char in unicodedata.normalize('NFD', token):\n",
    "\n",
    "                    # attempt second match on char by char basis\n",
    "                    if char in self.repl:\n",
    "                        transcription += self.repl[char]\n",
    "\n",
    "                    # attempt to keep with keep-set\n",
    "                    elif self.keep.match(char):\n",
    "                        transcription += char   \n",
    "        \n",
    "        # apply optional function\n",
    "        if self.apply_function:\n",
    "            transcription = self.apply_function(transcription)\n",
    "        \n",
    "        return unicodedata.normalize(normalize, transcription)\n",
    "    \n",
    "    \n",
    "trans_full = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "   \n",
    "    # char combinations\n",
    "    'p̌':'p<',\n",
    "    'ṭ': 't',\n",
    "    'ð̣': '6',\n",
    "\n",
    "    # non-latin vowels\n",
    "    '\\u0131': 'i',  # 0x0131 ı dotless i\n",
    "    '\\u0251': 'a',  # 0x0251 ɑ alpha\n",
    "    '\\u0259': '9',  # 0x0259 ə schwa\n",
    "    '\\u025B': '3',  # 0x025B ɛ open e\n",
    "    \n",
    "    # vowel accents\n",
    "    '\\u0300': '`',  # 0x0300 à grave\n",
    "    '\\u0301': \"'\",  # 0x0301 á acute\n",
    "    '\\u0304': '_',  # 0x0304 ā macron\n",
    "    '\\u0306': '%',  # 0x0306 ă breve\n",
    "    '\\u0308': '\"',  # 0x0308 ä diaeresis\n",
    "    '\\u0303': '~',  # 0x0303 ã tilde\n",
    "    '\\u02C8': '', # 0x2c8 ˈ small vertical line\n",
    "        \n",
    "    # non-latin consonants\n",
    "    '\\u00F0': '6',  # 0x00F0 ð eth\n",
    "    '\\u025F': '4',  # 0x025F ɟ small dotless j with stroke\n",
    "    '\\u0248': '4',  # 0x0248 Ɉ capital J with stroke\n",
    "    '\\u03B8': '8',  # 0x03B8 θ greek theta\n",
    "    '\\u02B8': '7',  # 0x02B8 ʸ small superscript y\n",
    "    '\\u02BE': ')',  # 0x02BE ʾ right half ring (alaph)\n",
    "    '\\u02BF': '(',  # 0x02BF ʿ left half ring (ayin)\n",
    "        \n",
    "    # consonant diacritics\n",
    "    '\\u207A': '+',  # 0x207A ⁺ superscript plus\n",
    "    '\\u030C': '>',  # 0x030C x̌ caron\n",
    "    '\\u0302': '^',  # 0x0302 x̂ circumflex\n",
    "    '\\u0307': ';',  # 0x0307 ẋ dot above\n",
    "    '\\u0323': '.',  # 0x0323 x̣ dot below\n",
    "    '\\u032D': '<',  # 0x032D x̭ circumflex below\n",
    "    \n",
    "    # punctuation\n",
    "    '\\u02C8': '|', # 0x2c8 ˈ small vertical line\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "trans_lite = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*', \n",
    "    'replace': {\n",
    "        'ʾ': ')',\n",
    "        'ʿ': '(',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '%',\n",
    "        'ḍ': 'D',\n",
    "        'ð': '6',\n",
    "        'ð̣': '^',\n",
    "        'ġ': 'G',\n",
    "        'ḥ': 'H',\n",
    "        'ɟ': '4',\n",
    "        'Ɉ': '4',\n",
    "        'k̭': '&',\n",
    "        'ḷ': 'L',\n",
    "        'ṃ': 'M',    \n",
    "        'p̣': 'P',\n",
    "        'ṛ': 'R',\n",
    "        'ṣ': 'S',\n",
    "        'š': '$',\n",
    "        'ṱ': '+',\n",
    "        'ṭ': 'T',\n",
    "        'θ': '8',\n",
    "        'ž': '7',\n",
    "        'ẓ': 'Z',\n",
    "        'ā̀': 'A',\n",
    "        'ā́': 'A',\n",
    "        'ă': '@',\n",
    "        'ắ': '@',\n",
    "        'ằ': '@',\n",
    "        'ē': 'E',\n",
    "        'ɛ': '3',\n",
    "        'ī': 'I',\n",
    "        'ĭ': '9',\n",
    "        'ə': '9',\n",
    "        'o': 'o',\n",
    "        'ō': 'O',\n",
    "        'ū': 'U',\n",
    "        'ŭ': '2',\n",
    "        'ı': 'i',\n",
    "        'ɑ': 'a',\n",
    "        'ˈ': '|'\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_urmi = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'c': 'k',\n",
    "        'c̭': 'k',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'q',\n",
    "        'ɟ': 'g',\n",
    "        'Ɉ': 'g',\n",
    "        'ə': 'i',\n",
    "        'v': 'w',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_barwar = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'k',\n",
    "        'θ': 't',\n",
    "        'ð': 'd',\n",
    "        'ɛ': 'e',\n",
    "        'ə': 'i',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "trans_text = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "    },\n",
    "    'punctuation': '[-–=]',\n",
    "    'keep': '[^\\s.,?!:;\\u02C8]+',\n",
    "    'keep_case': True,\n",
    "}\n",
    "\n",
    "trans_no_accent = {\n",
    "\n",
    "}\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"Strip accents from vowels. \n",
    "    \n",
    "    This is done in a class so .convert can be called\n",
    "    alongside the transcriber within a loop\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def convert(self, word):\n",
    "        \"\"\"Strip accents and spaces from NENA text on a node.\n",
    "\n",
    "        Args:\n",
    "            word: a node number to get normalized text\n",
    "        \"\"\"\n",
    "        accents = '\\u0300|\\u0301|\\u0304|\\u0306|\\u0308|\\u0303'\n",
    "        norm = unicodedata.normalize('NFD', word) # decompose for accent stripping\n",
    "        norm = re.sub(accents, '', norm) # strip accents\n",
    "        return norm\n",
    "\n",
    "dialect2trans = {\n",
    "    'ALL': {     \n",
    "        'text': Transcriber(**trans_text), # full utf8 text without punctuation\n",
    "        'text_trans': Transcriber(**trans_full), # transcription full\n",
    "        'text_lite': Transcriber(**trans_lite), # transcription lite feature\n",
    "        'text_noaccent': Normalizer(),\n",
    "    },\n",
    "    'Barwar': {\n",
    "        'fuzzy': Transcriber(**fuzzy_barwar), # fuzzy\n",
    "    },\n",
    "    'Urmi_C': {\n",
    "        'fuzzy': Transcriber(**fuzzy_urmi), # fuzzy\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe Alphabet and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 chars loaded\n"
     ]
    }
   ],
   "source": [
    "# load alphabet/punct data\n",
    "alpha_data = json.loads(alphabet_json.read_text())\n",
    "punct_data = json.loads(punctuation_json.read_text())\n",
    "    \n",
    "char_data = []\n",
    "for dtype in (alpha_data, punct_data):\n",
    "    for d in dtype:\n",
    "        cdata = (d['decomposed_regex'], d['decomposed_string'])\n",
    "        char_data.append(cdata)\n",
    "        \n",
    "print(len(char_data), 'chars loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose transcription data\n",
    "# data is simply a regex string to transcribed string mapping in a dict\n",
    "\n",
    "for dialect, transes in dialect2trans.items():\n",
    "    \n",
    "    dialect_dir = transcription_dir.joinpath(dialect)\n",
    "    dialect_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for trans_name, tscriber in transes.items():\n",
    "        \n",
    "        trans_file = dialect_dir.joinpath(trans_name+'.json')\n",
    "        trans_data = {}\n",
    "        for re_patt, string in char_data:\n",
    "            trans_data[re_patt] = tscriber.convert(string)\n",
    "            \n",
    "        with open(trans_file, 'w') as outfile:\n",
    "            json.dump(trans_data, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
