{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alphabet\n",
    "\n",
    "We start with developing sets and regular expressions that validate canonical letters in the NENA text-corpus. These patterns will be used for validating text-input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-app in /Users/cody/github/annotation/app-nena/code:\n",
      "\trepo clone offline under ~/github (local github)\n",
      "Using data in /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02:\n",
      "\trepo clone offline under ~/github (local github)\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Documentation:</b> <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs\" title=\"provenance of Northeastern Neo-Aramaic Text Corpus\">NENA_TF</a> <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/transcription.md\" title=\"('NENA transcription script',)\">Character table</a> <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#features.md\" title=\"NENA_TF feature documentation\">Feature docs</a> <a target=\"_blank\" href=\"https://github.com/annotation/app-nena\" title=\"nena API documentation\">nena API</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/\" title=\"text-fabric-api\">Text-Fabric API 7.11.1</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Use/Search/\" title=\"Search Templates Introduction and Reference\">Search Reference</a><details open><summary><b>Loaded features</b>:</summary>\n",
       "<p><b>Northeastern Neo-Aramaic Text Corpus</b>: <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#class\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/class.tf\">class</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#comment\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/comment.tf\">comment</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#dialect\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/dialect.tf\">dialect</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#end\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/end.tf\">end</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#foreign\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/foreign.tf\">foreign</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#full\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/full.tf\">full</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#full_end\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/full_end.tf\">full_end</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#fuzzy\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/fuzzy.tf\">fuzzy</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#fuzzy_end\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/fuzzy_end.tf\">fuzzy_end</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#gloss\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/gloss.tf\">gloss</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#grm_desc\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/grm_desc.tf\">grm_desc</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#informant\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/informant.tf\">informant</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lang\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/lang.tf\">lang</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lemma\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/lemma.tf\">lemma</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lemma_form\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/lemma_form.tf\">lemma_form</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lite\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/lite.tf\">lite</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lite_end\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/lite_end.tf\">lite_end</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#number\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/number.tf\">number</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#otype\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/otype.tf\">otype</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#place\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/place.tf\">place</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#speaker\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/speaker.tf\">speaker</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/text.tf\">text</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_id\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/text_id.tf\">text_id</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_lite\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/text_lite.tf\">text_lite</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_norm\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/text_norm.tf\">text_norm</a>  <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#title\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/title.tf\">title</a>  <b><i><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#oslots\" title=\"/Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.02/oslots.tf\">oslots</a></i></b> </p></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"CharisSIL-R\";\n",
       "  src:\n",
       "    local(\"CharisSIL-R.otf\"),\n",
       "    url(\"https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/CharisSIL-R.woff?raw=true\");\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: normal;\n",
       "    color: #000000;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: x-small;\n",
       "  font-weight: normal;\n",
       "}\n",
       "\n",
       ".ll {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       "\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: ltr;\n",
       "}\n",
       "\n",
       ".stress,.word,.letter,.macro {\n",
       "    border-radius: 0.2em;\n",
       "    border-spacing: 5px;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.4em;\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: ltr;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       "\n",
       ".letter {\n",
       "    border: 1px solid #cccccc;\n",
       "}\n",
       "\n",
       ".word {\n",
       "    border: 1px solid #00ffff;\n",
       "}\n",
       "\n",
       ".stress {\n",
       "    border: 1px solid #0000ff;\n",
       "}\n",
       "\n",
       ".prosa {\n",
       "    border: 1.5px dotted #993333;\n",
       "    border-radius: 0.2em;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.4em;\n",
       "    border-color: #a30404;\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: ltr;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       "\n",
       ".speaker {\n",
       "    vertical-align: super;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #6666dd;\n",
       "}\n",
       "\n",
       ".macro {\n",
       "    border: 2px solid #444499;\n",
       "}\n",
       "\n",
       "\n",
       ".line {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: ltr;\n",
       "}\n",
       "\n",
       ".ln {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       "\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       "\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "\n",
       ".ara,.ara a:visited,.ara a:link {\n",
       "    font-family: \"CharisSIL-R\";\n",
       "    font-size: 12pt;\n",
       "    color: #000000;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       "\n",
       "/* What do the following classes do? */\n",
       "\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       "\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       "\n",
       "tr.tf, td.tf, th.tf {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>API members</b>:</summary>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">C Computed</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Call AllComputeds</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Cs ComputedString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">E Edge</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Eall AllEdges</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Es EdgeString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ensureLoaded</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">TF</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ignored</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">loadLog</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Locality/#locality\" title=\"doc\">L Locality</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">cache</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">error</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">indent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">info</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">isSilent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">reset</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">setSilent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">silentOff</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">silentOn</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">warning</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">N Nodes</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKey</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKeyTuple</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">otypeRank</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortNodes</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">F Feature</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fall AllFeatures</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fs FeatureString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Search/#search\" title=\"doc\">S Search</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Text/#text\" title=\"doc\">T Text</a></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import collections\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "pd.options.display.max_rows = 200\n",
    "from tf.app import use\n",
    "nena = use('nena:clone', hoist=globals(), checkout='clone', version='0.02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using the corpus\n",
    "\n",
    "We use the NENA Text-Fabric corpus to test what characters are present in the corpus. We check to ensure that characters and accents are combined consistently. \n",
    "\n",
    "We also make corrections to the counted set to derive a host of acceptable characters which should go into the standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>63304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>38896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə</th>\n",
       "      <td>29280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>á</th>\n",
       "      <td>26857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>26509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "a  63304\n",
       "l  38896\n",
       "ə  29280\n",
       "á  26857\n",
       "n  26509"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "examples = collections.defaultdict(list)\n",
    "counts = collections.Counter()\n",
    "\n",
    "# should fix these letters to be indicated as foreign\n",
    "ignore = [\n",
    "    re.compile(let) for let in {'ɑ','ŕ', 'ã̀'}\n",
    "]\n",
    "\n",
    "def ignore_letter(let):\n",
    "    \"\"\"Check whether letter is known foreign\"\"\"\n",
    "    for i_letter in ignore:\n",
    "        if i_letter.findall(let):\n",
    "            return True\n",
    "\n",
    "for letter in F.otype.s('letter'):\n",
    "    word = L.u(letter,'word')[0]\n",
    "    letter_text = F.text.v(letter).lower()\n",
    "    # skip foreign words\n",
    "    if F.lang.v(word) or F.foreign.v(word) or ignore_letter(letter_text):\n",
    "        continue\n",
    "        \n",
    "    # add replacements that will be handled later\n",
    "    # in the architecture / conversion process\n",
    "    replacements = [\n",
    "        ('⁺', ''), # should go in \"begin\"\n",
    "        ('ɉ', 'ɟ'), # should replace everywhere \n",
    "        ('ʸ', 'y'), # should replace everywhere\n",
    "        ('ĉ', 'č'), # should fix error in text\n",
    "        ('p̂', 'p̭'), # should put in acceptable substitutions\n",
    "    ]\n",
    "    \n",
    "    for find, replace in replacements:\n",
    "        letter_text = letter_text.replace(find, replace)\n",
    "    \n",
    "    examples[letter_text].append([letter, word])\n",
    "    counts[letter_text] += 1\n",
    "    \n",
    "letter_counts = pd.DataFrame.from_dict(counts, orient='index').sort_values(by=0, ascending=False)\n",
    "\n",
    "display(letter_counts.head())\n",
    "print(len(letter_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>63304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>38896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə</th>\n",
       "      <td>29280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>á</th>\n",
       "      <td>26857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>26509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>25744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>23795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>23351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>20583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ʾ</th>\n",
       "      <td>19306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>18628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>à</th>\n",
       "      <td>16123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>14536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>13163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>12913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>12067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>9672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>í</th>\n",
       "      <td>9214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>9029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>š</th>\n",
       "      <td>8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə́</th>\n",
       "      <td>8178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>6196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k̭</th>\n",
       "      <td>6063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ì</th>\n",
       "      <td>6054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>5718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ú</th>\n",
       "      <td>5040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ə̀</th>\n",
       "      <td>4414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>4177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ù</th>\n",
       "      <td>3901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>3619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>θ</th>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṱ</th>\n",
       "      <td>3378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɟ</th>\n",
       "      <td>3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>é</th>\n",
       "      <td>2953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>2659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ</th>\n",
       "      <td>2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>è</th>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ó</th>\n",
       "      <td>2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṭ</th>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ò</th>\n",
       "      <td>1517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṣ</th>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ð</th>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>č</th>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ́</th>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ̀</th>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ă</th>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ắ</th>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p̭</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>č̭</th>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṛ</th>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ằ</th>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ā</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ġ</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ḷ</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ā̀</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ʿ</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ā́</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ḗ</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ḥ</th>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ž</th>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ḕ</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>č̣</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ē</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ū̀</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ō</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ī</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ī́</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ī̀</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ū</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ŭ</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c̭</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ū́</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṓ</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṃ</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ŭ̀</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ṑ</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ð̣</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ẓ</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p̣</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ḍ</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ŭ́</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ɛ̄</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "a   63304\n",
       "l   38896\n",
       "ə   29280\n",
       "á   26857\n",
       "n   26509\n",
       "m   25744\n",
       "t   23795\n",
       "r   23351\n",
       "x   20583\n",
       "ʾ   19306\n",
       "y   18628\n",
       "à   16123\n",
       "b   14536\n",
       "d   13163\n",
       "e   12913\n",
       "u   12067\n",
       "i    9672\n",
       "í    9214\n",
       "v    9029\n",
       "š    8912\n",
       "ə́   8178\n",
       "w    6196\n",
       "k̭   6063\n",
       "ì    6054\n",
       "s    5718\n",
       "p    5240\n",
       "z    5232\n",
       "ú    5040\n",
       "ə̀   4414\n",
       "h    4404\n",
       "q    4177\n",
       "c    4006\n",
       "ù    3901\n",
       "o    3619\n",
       "θ    3615\n",
       "ṱ    3378\n",
       "ɟ    3030\n",
       "é    2953\n",
       "k    2659\n",
       "ɛ    2431\n",
       "g    2314\n",
       "è    2262\n",
       "ó    2234\n",
       "ṭ    2072\n",
       "ò    1517\n",
       "j    1398\n",
       "ṣ    1397\n",
       "ð    1375\n",
       "č    1333\n",
       "ɛ́   1041\n",
       "ɛ̀    961\n",
       "ă     638\n",
       "ắ     595\n",
       "p̭    551\n",
       "č̭    543\n",
       "ṛ     474\n",
       "f     336\n",
       "ằ     297\n",
       "ā     270\n",
       "ġ     266\n",
       "ḷ     208\n",
       "ā̀    202\n",
       "ʿ     190\n",
       "ā́    182\n",
       "ḗ     182\n",
       "ḥ     181\n",
       "ž     175\n",
       "ḕ     172\n",
       "č̣     84\n",
       "ē      82\n",
       "ū̀     78\n",
       "ō      78\n",
       "ī      75\n",
       "ī́     65\n",
       "ī̀     62\n",
       "ū      56\n",
       "ŭ      46\n",
       "c̭     40\n",
       "ū́     37\n",
       "ṓ      26\n",
       "ṃ      24\n",
       "ŭ̀      9\n",
       "ṑ       9\n",
       "ð̣      6\n",
       "ẓ       6\n",
       "p̣      5\n",
       "ḍ       4\n",
       "ŭ́      3\n",
       "ɛ̄      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for ordering issues\n",
    "\n",
    "Are all letters with the same set of characters also ordered in the same way?\n",
    "\n",
    "We can check for such bad cases by doing the following:\n",
    "\n",
    "1. decompose all letters \n",
    "2. find letters that intersect on a set of their characters\n",
    "3. check whether intersecting set letters do not `==` each other, if so, store that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad order issues found\n"
     ]
    }
   ],
   "source": [
    "bad_order_issues = []\n",
    "\n",
    "for ia, letter_a in enumerate(letter_counts.index):\n",
    "    \n",
    "    a_charset = set(unicodedata.normalize('NFD', letter_a))\n",
    "    \n",
    "    for ib, letter_b in enumerate(letter_counts.index):\n",
    "        \n",
    "        # skip identical indices\n",
    "        if ia == ib:\n",
    "            continue\n",
    "        \n",
    "        b_charset = set(unicodedata.normalize('NFD', letter_b))\n",
    "\n",
    "        if a_charset == b_charset:\n",
    "            if letter_a != letter_b:\n",
    "                bad_order_issues.append((letter_a, letter_b))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "print(f'{len(bad_order_issues)} bad order issues found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No issues found. Is it because `unicodedata.normalize` fixes these kinds of problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "813\n",
      "780\n"
     ]
    }
   ],
   "source": [
    "example = unicodedata.normalize('NFD', 'č̭')\n",
    "\n",
    "for char in example:\n",
    "    print(ord(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: č̭\n",
      "after: č̭\n"
     ]
    }
   ],
   "source": [
    "# above we have the natural order of the accents\n",
    "# below we change that order to see what happens\n",
    "\n",
    "bad_order = chr(99) + chr(780) + chr(813)\n",
    "\n",
    "print('before:', example)\n",
    "print('after:', bad_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good order:\n",
      " c 99\n",
      " ̭ 813\n",
      " ̌ 780\n",
      "\n",
      "bad order:\n",
      " c 99\n",
      " ̌ 780\n",
      " ̭ 813\n"
     ]
    }
   ],
   "source": [
    "print('good order:')\n",
    "for char in example:\n",
    "    print(' '+char, ord(char))\n",
    "\n",
    "print()\n",
    "\n",
    "print('bad order:')\n",
    "for char in bad_order:\n",
    "    print(' '+char, ord(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to normalize `bad_order` and see if the order issue is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " c 99\n",
      " ̭ 813\n",
      " ̌ 780\n"
     ]
    }
   ],
   "source": [
    "for char in unicodedata.normalize('NFD', bad_order):\n",
    "    print(' '+char, ord(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`unicodedata` does indeed fix bad order in accents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accent range\n",
    "\n",
    "The full range of accents in unicode is given as `\\u0300-\\u036F`. We want to only utilize a subset of these. Let's identify the relevant range now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any_accent = re.compile('[\\u0300-\\u036F]')\n",
    "\n",
    "attested_accents = set()\n",
    "\n",
    "for letter in letter_counts.index:\n",
    "    decomposed = unicodedata.normalize(\"NFD\", letter)\n",
    "    for c in decomposed:\n",
    "        if any_accent.match(c):\n",
    "            attested_accents.add(c)\n",
    "            \n",
    "len(attested_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x323\n",
      "0x306\n",
      "0x32d\n",
      "0x301\n",
      "0x307\n",
      "0x30c\n",
      "0x300\n",
      "0x304\n"
     ]
    }
   ],
   "source": [
    "for c in attested_accents:\n",
    "    print(hex(ord(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write a pattern and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_pattern = re.compile('[\\u0300-\\u033d]')\n",
    "\n",
    "for c in attested_accents:\n",
    "    if not accent_pattern.match(c):\n",
    "        print(hex(ord(c)), 'not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we cannot define a precise range. But we can define a smaller range that eliminates some spurious possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards a Standard\n",
    "\n",
    "Now that we know `unicodedata.normalize` repairs bard orders of accents, and that our corpus contains no bad accents, we can move towards defining some standards for canonical letters.\n",
    "\n",
    "We will store each letter as a dictionary entry with metadata such as acceptable combining accents, as well class data about the letter. \n",
    "\n",
    "To get started, and to generate this dictionary efficiently, we'll begin with a few sets. \n",
    "\n",
    "**Note that for all cases we work with lowercase letters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up letter data into a dictionary\n",
    "# make vowels set\n",
    "vowels = list('aeɛəiou')\n",
    "letter_data = []\n",
    "vowel_data = []\n",
    "\n",
    "# categories identified with findall\n",
    "category2re = {\n",
    "    'point': [\n",
    "        ('[pbfvmw]', 'labial'),\n",
    "        ('[tdθðnlr]|[sz](?!\\u030c)', 'dental-alveolar'),\n",
    "        ('[j]|[csz].?\\u030c', 'palatal-alveolar'),\n",
    "        ('[ɟy]|c(?![\\u032d\\u0323]?\\u030c)', 'palatal'),\n",
    "        ('[kgxg]', 'velar'),\n",
    "        ('q', 'uvular'),\n",
    "        ('h\\u0323|ʿ', 'pharyngeal'),\n",
    "        ('h(?!\\u0323)|ʾ', 'laryngeal')\n",
    "    ],    \n",
    "    'manner': [\n",
    "        ('[pbtdcjɟkqʾ]|g(?!\\u0307)', 'affricative'),\n",
    "        ('[fvθðxhʿ]|g\\u0307', 'fricative'),\n",
    "        ('[sz]', 'sibilant'),\n",
    "        ('[mn]', 'nasal'),\n",
    "        ('l', 'lateral'),\n",
    "        ('[wry]','other'),\n",
    "    ],\n",
    "    'phonation': [\n",
    "        ('[ptck](?![\\u032d\\u0323])|ʾ', 'unvoiced_aspirated'),\n",
    "        ('[ptck]\\u032d', 'unvoiced_unaspirated'),\n",
    "        ('[bdjɟgvðz]', 'voiced'),\n",
    "        ('[fθxh]|s(?!\\u0323)', 'unvoiced'),\n",
    "        ('[ptckðdszmlr]\\u0323|ʿ', 'emphatic'),\n",
    "        ('[mnlwry](?!\\u0323)', 'plain')\n",
    "    ],\n",
    "}\n",
    "\n",
    "category2comp = {}\n",
    "for cat, patterns in category2re.items():\n",
    "    category2comp[cat] = [(re.compile(pat), val) for pat,val in patterns]\n",
    "\n",
    "# for every class, letter, assign data and store in leter dict\n",
    "for letter in sorted(letter_counts.index):\n",
    "    \n",
    "    # decomposed data\n",
    "    decomposed = unicodedata.normalize('NFD', letter)\n",
    "    decomposed_upper = decomposed[0].upper() + decomposed[1:]\n",
    "    decomposed_codes = tuple(ord(c) for c in decomposed)\n",
    "    decomposed_upper_codes = tuple(ord(c) for c in decomposed_upper)\n",
    "    decomposed_regex = f'{decomposed}(?![\\u0300-\\u036F])|{decomposed_upper}(?![\\u0300-\\u036F])'\n",
    "    \n",
    "    # composed data\n",
    "    composed = unicodedata.normalize('NFC', letter)\n",
    "    composed_upper = unicodedata.normalize('NFC', decomposed_upper)\n",
    "    composed_codes = tuple(ord(c) for c in composed)\n",
    "    composed_upper_codes = tuple(ord(c) for c in composed_upper)\n",
    "    \n",
    "    # base data\n",
    "    base = decomposed[0]\n",
    "    base_upper = decomposed[0].upper()\n",
    "    base_code = ord(base)\n",
    "    base_upper_code = ord(base_upper)\n",
    "    \n",
    "    # accents data\n",
    "    accents = decomposed[1:]\n",
    "    accent_codes = tuple(ord(a) for a in accents)\n",
    "    letter_class = 'vowel' if base in vowels else 'consonant'\n",
    "    \n",
    "    # handle consonants / vowels differently\n",
    "    ldat = {\n",
    "            'decomposed_regex': decomposed_regex,\n",
    "            \n",
    "            'decomposed_string': decomposed,\n",
    "            'decomposed_upper_string': decomposed_upper,\n",
    "            'decomposed_codepoints': decomposed_codes,\n",
    "            'decomposed_upper_codepoints': decomposed_upper_codes,\n",
    "            \n",
    "            'composed_string': composed,\n",
    "            'composed_upper_string': composed_upper,\n",
    "            'composed_codepoints': composed_codes,\n",
    "            'composed_upper_codepoints': composed_upper_codes,\n",
    "            \n",
    "            'base_string': base,\n",
    "            'base_upper_string': base_upper,\n",
    "            'base_code': base_code,\n",
    "            'base_upper_code': base_upper_code,\n",
    "\n",
    "            'decomposed_accent_string': accents,\n",
    "            'decomposed_accent_codes': accent_codes,\n",
    "            'class': letter_class,\n",
    "        }\n",
    "    \n",
    "    # apply categorizations\n",
    "    for category, patterns in category2comp.items():\n",
    "        for patt, value in patterns:\n",
    "            if patt.findall(decomposed):\n",
    "                ldat[category] = value\n",
    "    \n",
    "    if letter_class == 'consonant':\n",
    "        letter_data.append(ldat)\n",
    "    else:\n",
    "        vowel_data.append(ldat)\n",
    "\n",
    "def sort_letters(letter_list):\n",
    "    \"\"\"Sort letter list\"\"\"\n",
    "    return sorted(\n",
    "        letter_list, \n",
    "        key=lambda data: (data['base_code'], len(data['decomposed_accent_codes']), ''.join(chr(c) for c in data['decomposed_codepoints']))\n",
    "    )\n",
    "        \n",
    "letter_data = sort_letters(letter_data)\n",
    "vowel_data = sort_letters(vowel_data)\n",
    "letter_data.extend(vowel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ld in letter_data:\n",
    "#     pprint(ld, sort_dicts=False, indent=4)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../alphabet.json', 'w') as outfile:\n",
    "    json.dump(letter_data, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are composed/decomposed preserved with json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../alphabet.json', 'r') as infile:\n",
    "    alphabet = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet[64]['composed_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet[64]['decomposed_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yes. Json loading preserves the decomposed string.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Alphabet Regex Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(string):\n",
    "    return unicodedata.normalize('NFD', string).lower()\n",
    "\n",
    "def tokenize_string(string):\n",
    "    norm_string = normalize_string(string)\n",
    "    return re.findall('.[\\u0300-\\u036F]*', norm_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'č̭ c̭'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = \"č̭ c̭\"\n",
    "\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "re2data = {re.compile(rd['decomposed_regex']):rd for rd in alphabet.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "č̭ matched re.compile('č̭(?![̀-ͯ])') [99, 813, 780]\n",
      "c̭ matched re.compile('c̭(?![̀-ͯ])') [99, 813]\n"
     ]
    }
   ],
   "source": [
    "unmatched = set()\n",
    "\n",
    "for c in tokenize_string(test_sample):\n",
    "    match = False\n",
    "    for patt in re2data:\n",
    "        if patt.match(c):\n",
    "            print(c, 'matched', patt, re2data[patt]['decomposed_codepoints'])\n",
    "            match = True\n",
    "    if not match:\n",
    "        unmatched.add(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Regex\n",
    "\n",
    "We need a regular expression that validates good nena text. We build that pattern here using the standardized alphabet.\n",
    "\n",
    "To do this, we need to cluster base characters on attested accentuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = unicodedata.normalize('NFD', 'hám-ʾən ʾàsqətˈ ʾap-ʾáyya qabū̀l-ila')\n",
    "\n",
    "# re.findall('.[\\u0300-\\u036F]*', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ð 240 \\u0f0\n"
     ]
    }
   ],
   "source": [
    "test_letter = 'ð'\n",
    "\n",
    "def make_unistring(char):\n",
    "    \"\"\"Makes \\\\u unicode string from a c\"\"\"\n",
    "    return '\\\\u'+hex(ord(char)).replace('x','')\n",
    "\n",
    "for c in unicodedata.normalize('NFD', test_letter):\n",
    "    print(' '+c, ord(c), make_unistring(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ð']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = unicodedata.normalize('NFD', test_letter)\n",
    "\n",
    "re.findall('[a-zðɟəɛʾʿθ](?![\\u0300-\\u036F])', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster on overlapping features for regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accents2bases = collections.defaultdict(list)\n",
    "for letter, ldat in letter_dict.items():\n",
    "    uni_string = ''.join(make_unistring(chr(c)) for c in ldat['accent_codes'])\n",
    "    \n",
    "    if '036f' in uni_string:\n",
    "        print(letter, uni_string)\n",
    "    \n",
    "    accents2bases[uni_string].append(ldat['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accents2bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accents2bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cluster a second time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases2accents = collections.defaultdict(set)\n",
    "\n",
    "for accents, lset in accents2bases.items():\n",
    "    if not accents:\n",
    "        continue\n",
    "    bases2accents[tuple(lset)].add(accents)\n",
    "    for accents2, lset2 in accents2bases.items():\n",
    "        if accents != accents2:\n",
    "            continue\n",
    "        elif lset == lset2:\n",
    "            bases2accents[tuple(lset)].add(accents2)        \n",
    "            \n",
    "# for accents, lset in accents2bases.items():\n",
    "#     print(accents)\n",
    "#     print(lset)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bases2accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accented_sorted = sorted(bases2accents.items(), key=lambda k: len(k[0]), reverse=True)\n",
    "\n",
    "for bases, accents in accented_sorted:\n",
    "    print('[' + ''.join(bases) + ']', ' | '.join(accents))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char(char):\n",
    "    \"\"\"Normalize character for regex testing.\n",
    "    \n",
    "    Characters normalized by:\n",
    "        * decomposing/sorting chars with unicodedata.normalize\n",
    "        * converted to lowercase\n",
    "    Decomposition allows for one-to-one matching with accents.\n",
    "    NB: that normalize_char removes capitalization and\n",
    "    thus should not replace text inputs.\n",
    "    \n",
    "    Arguments:\n",
    "        char: str of single character\n",
    "    \n",
    "    Returns:\n",
    "        str normalized\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFD', char).lower()\n",
    "\n",
    "base_chars = '[a-zðɟəɛʾʿθ]'\n",
    "unaccented = base_chars + '(?![\\u0300-\\u036F])'\n",
    "cons_accented = '[dhlmprstzð]\\u0323|[ckpt]\\u032d|[csz]\\u030c|c[\\u0323\\u032d]\\u030c|g\\u0307'\n",
    "vowel_accented = '[aeiouəɛ][\\u0300\\u0301]|[aeiouɛ]\\u0304|[aeiou]\\u0304[\\u0300\\u0301]|[au]\\u0306[\\u0300\\u0301]?'\n",
    "one_regex = '|'.join([unaccented, cons_accented, vowel_accented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "for letter in letter_dict:\n",
    "    letter_text = unicodedata.normalize('NFD', letter)\n",
    "    if not test_regex.findall(letter_text):\n",
    "        print(letter, 'not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every letter is thus captured. Let's test the new system against bogus letters currently residing in the corpus (some of these are valid for foreign strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_letters = set()\n",
    "\n",
    "for letter in F.otype.s('letter'):\n",
    "    letter_text = normalize_char(F.text.v(letter)).replace('⁺', '')\n",
    "    if not test_regex.findall(letter_text):\n",
    "        bad_letters.add(letter_text)\n",
    "        \n",
    "bad_letters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
