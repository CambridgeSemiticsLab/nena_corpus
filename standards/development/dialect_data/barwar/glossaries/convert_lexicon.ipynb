{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Lexicon to a Standardized JSON Lexicon Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from pathlib import Path\n",
    "glossary_file = 'bar glossary general.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing standard Lexicon and Analyzed Word JSONs\n",
    "\n",
    "We need to match words in the text with their lexemes. We should also allow for some\n",
    "additional data to be added to words later on.\n",
    "\n",
    "A nice model to follow is that of the Eep Talstra Centre for Bible and Computer, which\n",
    "encodes data about Hebrew words in two formats: \n",
    "\n",
    "* lexicon - 2 columns, first is the lexical form of a word, second is lexicon data (example [here](https://github.com/ETCBC/data_creation/blob/master/grammar_libraries/synvar_library/hebrew/lexicon))\n",
    "* anwb (or anzb) an analytical word book - 2 columns, first is an inflected surface\n",
    "    form of a word, second is a tag containing parsing data as well as the lexical form\n",
    "    of the word. The anwb enables the link to the lexicon file. (example [here](https://raw.githubusercontent.com/ETCBC/data_creation/master/grammar_libraries/standard_library/hebrew/anwb))\n",
    "    \n",
    "We will implement a similar approach but with JSON files. The format is simple: a key and dictionary of values.\n",
    "For the lexicon, the key is a lexeme string with linked attributes. For the inflections data, the key is an\n",
    "inflected form, while the attributes contains a link to the lexeme as well as any localized data.\n",
    "\n",
    "**lexicon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_eg = {\n",
    "    \n",
    "    'ʾabaya.1': {\n",
    "        'pos': 'noun',\n",
    "        'gn': 'm',\n",
    "        'gloss': \"a man's cloak\",\n",
    "        'entry': 'ʾabàyele də́rya b-réšaˈ He put his cloak over her (A26:50)...',\n",
    "    },\n",
    "    \n",
    "    # ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inflections**\n",
    "\n",
    "**NB: An inflection can contain attributes that can overwrite attributes from the lexicon.**\n",
    "\n",
    "Inflections can also match multiple lexemes. Each individual form receives an inflection entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflections_eg = [\n",
    "    {\n",
    "        'form': 'ʾabbaya',\n",
    "        'lex': 'ʾabaya', \n",
    "        'nu': 'sg',\n",
    "    },\n",
    "    {\n",
    "        'form':'ʾabaye',\n",
    "        'lex': 'ʾabaya', \n",
    "        'nu': 'pl',\n",
    "    },\n",
    "    {   \n",
    "        'form': 'ʾabbaye',\n",
    "        'lex': 'ʾabaya',\n",
    "        'nu': 'pl',\n",
    "    },\n",
    "    \n",
    "    #...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All text will be stored in decomposed Unicode form. Inflected forms should be written\n",
    "without accents on vowels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the standards\n",
    "\n",
    "We now load the glossary information we have for Barwar. We need to make a few things.\n",
    "\n",
    "* word grammar standards - standard codes and acceptable tag values for word grammar \n",
    "    (e.g. nu=number, lex=lexeme, etc.)\n",
    "* parse grammatical descriptions\n",
    "* convert lexicon file\n",
    "* convert inflections file\n",
    "\n",
    "First, we load the glossary data and begin inspecting it to construct the standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_entry(text):\n",
    "    \"\"\"Normalize text by removing style elements/quirks\"\"\"\n",
    "    # remove trailing spaces\n",
    "    text = text.strip()\n",
    "    # remove style asterixes\n",
    "    text = text.replace('*', '')\n",
    "    # remove end of word -\n",
    "    text = re.sub('[\\-–—]$', '', text)\n",
    "    # replace weird hyphens\n",
    "    text = re.sub('—|–', '-', text)\n",
    "    # replace weird tags\n",
    "    text = text.replace('<|>', ' | ')\n",
    "    # remove begin/end single quotes\n",
    "    text = re.sub('^\\u0027|\\u0027$', '', text)\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    return text\n",
    "\n",
    "with open(glossary_file, 'r') as infile:\n",
    "    raw_glossary_data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma': 'ʾabaya, ʾabbaya',\n",
       " 'grm_desc': 'n.m.',\n",
       " 'forms': [['pl.', 'ʾabaye, ʾabbaye']],\n",
       " 'trans': '**man’s cloak**',\n",
       " 'lang': 'A.',\n",
       " 'examples': '*ʾabàyele də́rya b-réša*<|> He put his cloak over her (A26:50); *ʾɛ́-ga kúlla ʾaġăwáθa šexìye*<|> *kúlla b-ʾabbàye-wawa*<|> At that time all aghas and elders wore the abbaya gown (A26:47); *ʾíman ṱ-ila-nabólle bə̀dra*<|> *m-gu-ʾằra,*<|> *mattíwa ... ʾabàya y-amrə́xwale*<|> When they took it (the rice) to the threshing floor from that ground, they would lay down what we called a cloak (B5:80).',\n",
       " 'raw_entry': '*ʾabaya, ʾabbaya* n.m. (pl. *ʾabaye, ʾabbaye*) (A.) **man’s cloak** | *ʾabàyele də́rya b-réša*<|> He put his cloak over her (A26:50); *ʾɛ́-ga kúlla ʾaġăwáθa šexìye*<|> *kúlla b-ʾabbàye-wawa*<|> At that time all aghas and elders wore the abbaya gown (A26:47); *ʾíman ṱ-ila-nabólle bə̀dra*<|> *m-gu-ʾằra,*<|> *mattíwa ... ʾabàya y-amrə́xwale*<|> When they took it (the rice) to the threshing floor from that ground, they would lay down what we called a cloak (B5:80).'}"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_glossary_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some complicated forms will not be added due to ambiguities in the entries.\n",
    "Those are selected and excluded below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 bad entries found\n",
      "3352 good entries found\n"
     ]
    }
   ],
   "source": [
    "bads = []\n",
    "\n",
    "for lem in raw_glossary_data:\n",
    "    if {'annotations', 'inbetweens', 'tail'} & set(lem.keys()):\n",
    "        bads.append(lem)\n",
    "    if 'grm_desc' not in lem:\n",
    "        bads.append(lem)\n",
    "\n",
    "glossary_data = [lem for lem in raw_glossary_data if lem not in bads]\n",
    "\n",
    "\n",
    "print(len(bads), 'bad entries found')\n",
    "print(len(glossary_data), 'good entries found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Grammatical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3352, 9)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_df = pd.DataFrame(glossary_data).fillna('')\n",
    "\n",
    "lex_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>grm_desc</th>\n",
       "      <th>forms</th>\n",
       "      <th>trans</th>\n",
       "      <th>lang</th>\n",
       "      <th>examples</th>\n",
       "      <th>raw_entry</th>\n",
       "      <th>meanings</th>\n",
       "      <th>numeral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ʾabaya, ʾabbaya</td>\n",
       "      <td>n.m.</td>\n",
       "      <td>[[pl., ʾabaye, ʾabbaye]]</td>\n",
       "      <td>**man’s cloak**</td>\n",
       "      <td>A.</td>\n",
       "      <td>*ʾabàyele də́rya b-réša*&lt;|&gt; He put his cloak o...</td>\n",
       "      <td>*ʾabaya, ʾabbaya* n.m. (pl. *ʾabaye, ʾabbaye*)...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ʾabona</td>\n",
       "      <td>n.m.</td>\n",
       "      <td>[[pl., ʾabone]]</td>\n",
       "      <td>**bishop**</td>\n",
       "      <td>A.</td>\n",
       "      <td></td>\n",
       "      <td>*ʾabona* n.m. (pl. *ʾabone*) (A.) **bishop**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ʾăbu</td>\n",
       "      <td>part.</td>\n",
       "      <td></td>\n",
       "      <td>**attributive particle** (A. literally: father...</td>\n",
       "      <td></td>\n",
       "      <td>*ʾăbu-bə̀rqa*&lt;|&gt; electrician(s) (B10:50); *băy...</td>\n",
       "      <td>*ʾăbu* part. **attributive particle** (A. lite...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ʾAðər</td>\n",
       "      <td>n.m.</td>\n",
       "      <td></td>\n",
       "      <td>**March**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>*ʾAðər* n.m. **March**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ʾadət</td>\n",
       "      <td>n.f.</td>\n",
       "      <td>[[pl., ʾadətte, ʾadəttaθa]]</td>\n",
       "      <td>**custom**</td>\n",
       "      <td></td>\n",
       "      <td>*ʾáxni ʾádət díyən hàtxɛla*&lt;|&gt; Our custom ...</td>\n",
       "      <td>*ʾadət* n.f. (pl. *ʾadətte, ʾadəttaθa*) **cust...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lemma grm_desc                        forms  \\\n",
       "0  ʾabaya, ʾabbaya     n.m.     [[pl., ʾabaye, ʾabbaye]]   \n",
       "1           ʾabona     n.m.              [[pl., ʾabone]]   \n",
       "2             ʾăbu    part.                                \n",
       "3            ʾAðər     n.m.                                \n",
       "4            ʾadət     n.f.  [[pl., ʾadətte, ʾadəttaθa]]   \n",
       "\n",
       "                                               trans lang  \\\n",
       "0                                    **man’s cloak**   A.   \n",
       "1                                         **bishop**   A.   \n",
       "2  **attributive particle** (A. literally: father...        \n",
       "3                                          **March**        \n",
       "4                                         **custom**        \n",
       "\n",
       "                                            examples  \\\n",
       "0  *ʾabàyele də́rya b-réša*<|> He put his cloak o...   \n",
       "1                                                      \n",
       "2  *ʾăbu-bə̀rqa*<|> electrician(s) (B10:50); *băy...   \n",
       "3                                                      \n",
       "4  *ʾáxni ʾádət díyən hàtxɛla*<|> Our custom ...   \n",
       "\n",
       "                                           raw_entry meanings numeral  \n",
       "0  *ʾabaya, ʾabbaya* n.m. (pl. *ʾabaye, ʾabbaye*)...                   \n",
       "1       *ʾabona* n.m. (pl. *ʾabone*) (A.) **bishop**                   \n",
       "2  *ʾăbu* part. **attributive particle** (A. lite...                   \n",
       "3                             *ʾAðər* n.m. **March**                   \n",
       "4  *ʾadət* n.f. (pl. *ʾadətte, ʾadəttaθa*) **cust...                   "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>grm_desc</th>\n",
       "      <th>forms</th>\n",
       "      <th>trans</th>\n",
       "      <th>lang</th>\n",
       "      <th>examples</th>\n",
       "      <th>raw_entry</th>\n",
       "      <th>meanings</th>\n",
       "      <th>numeral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>rəḥqa</td>\n",
       "      <td>n.m. adv./adj.</td>\n",
       "      <td></td>\n",
       "      <td>**distance; distant.**</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>*rəḥqa* n.m. adv./adj. **distance; distant.** ...</td>\n",
       "      <td>[(1) **distance** | *xá-bena xzéle m-rə́ḥqa ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma        grm_desc forms                   trans lang examples  \\\n",
       "2284  rəḥqa  n.m. adv./adj.        **distance; distant.**                 \n",
       "\n",
       "                                              raw_entry  \\\n",
       "2284  *rəḥqa* n.m. adv./adj. **distance; distant.** ...   \n",
       "\n",
       "                                               meanings numeral  \n",
       "2284  [(1) **distance** | *xá-bena xzéle m-rə́ḥqa ...          "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_df[lex_df.grm_desc.str.contains('n.m. adv./adj.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.m.                1460\n",
       "n.f.                1011\n",
       "adj.                 309\n",
       "n.pl.                110\n",
       "adj. invar.           71\n",
       "adv.                  66\n",
       "part.                 65\n",
       "n.m./adj.             49\n",
       "num.                  37\n",
       "n.m./f.               26\n",
       "n.m                   22\n",
       "m.                    19\n",
       "interj.               18\n",
       "prep.                 11\n",
       "mod.                   7\n",
       "pron. pl.              6\n",
       "n.f./adj.              4\n",
       "pron. fs.              4\n",
       "n.f                    4\n",
       "pron. ms.              4\n",
       "f.                     3\n",
       "n.f./m.                3\n",
       "m.n.                   3\n",
       "mod. adv.              3\n",
       "n.pl.tan.              3\n",
       "pron.                  3\n",
       "n.m/adj.               2\n",
       "adj. m.                2\n",
       "pron. cs.              2\n",
       "adj. cs.               2\n",
       "n.m. /adj.             2\n",
       "part., prep.           1\n",
       "imper. pl.             1\n",
       "pl. tan.               1\n",
       "n.f./adv.              1\n",
       "adj. adv. mod.         1\n",
       "pl.                    1\n",
       "n.f./adj..             1\n",
       "part. pron.            1\n",
       "pl.tan.                1\n",
       "adj.f.                 1\n",
       "n.m. adj.              1\n",
       "n.m..                  1\n",
       "n.m. adv. adj.         1\n",
       "n.f./adj.f             1\n",
       "pron. mod.             1\n",
       "m./f.                  1\n",
       "n.m. adv./adj.         1\n",
       "n.m., adj.             1\n",
       "n.pl.tant.             1\n",
       "adj. f.                1\n",
       "adj./adv. invar.       1\n",
       "Name: grm_desc, dtype: int64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_df.grm_desc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word grammar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_grammar = Path('/Users/cody/github/CambridgeSemiticsLab/nena_corpus/standards/word_grammar.json')\n",
    "\n",
    "grammar_data = {\n",
    "    'pos': {\n",
    "        'desc': 'part of speech',\n",
    "        'tags': {\n",
    "            'NOUN': 'noun',\n",
    "            'VERB': 'verb',\n",
    "            'PREP': 'preposition',\n",
    "            'ADJV': 'adjective',\n",
    "            'ADVB': 'adverb',\n",
    "            'PART': 'particle',\n",
    "            'MODI': 'non-attributive modifier',\n",
    "            'PRON': 'pronoun',\n",
    "            'INTJ': 'interjection',\n",
    "            'NUMR': 'numeral',\n",
    "        }\n",
    "    },\n",
    "    'form': {\n",
    "        'desc': 'variability of word form',\n",
    "        'tags': {\n",
    "            'INVAR': 'lexeme is invariable',\n",
    "        }\n",
    "    },\n",
    "    'st': {\n",
    "        'desc': 'state',\n",
    "        'tags': {\n",
    "            'C': 'construct',\n",
    "            'A': 'absolute',\n",
    "        }\n",
    "    },\n",
    "    'gn': {\n",
    "        'desc': 'gender',\n",
    "        'tags': {\n",
    "            'F': 'feminine',\n",
    "            'M': 'masculine',\n",
    "            'MF': 'either masculine or feminine',\n",
    "            'C': 'common',\n",
    "        }\n",
    "    },\n",
    "    'nu': {\n",
    "        'desc': 'number',\n",
    "        'tags': {\n",
    "            'SG': 'singular',\n",
    "            'PL': 'plural',\n",
    "        }\n",
    "    },\n",
    "    'nu_class': {\n",
    "        'desc': 'semantic class of number',\n",
    "        'tags': {\n",
    "            'TANT': 'pluralis tantum: noun with only a plural form'\n",
    "        }\n",
    "    },\n",
    "    'trns': {\n",
    "        'desc': 'transitivity',\n",
    "        'tags': {\n",
    "            'TR': 'transitive',\n",
    "            'ITR': 'intransitive',\n",
    "        }\n",
    "    },\n",
    "    'syn': {\n",
    "        'desc': 'synonymy/antonymy',\n",
    "        'tags': {\n",
    "            'AN': 'antonym',\n",
    "            'SN': 'synonym',\n",
    "        }\n",
    "    },\n",
    "    'stem': {\n",
    "        'desc': 'stem of a verb',\n",
    "        'tags': {\n",
    "            'I': 'stem I verb',\n",
    "            'II': 'stem II verb',\n",
    "            'III': 'stem III verb',\n",
    "            'Q': 'quadriliteral verb',\n",
    "        }\n",
    "    },\n",
    "    'tense': {\n",
    "        'desc': 'grammatical tense of a verb',\n",
    "        'tags': {\n",
    "            'IMP': 'imperative',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(word_grammar, 'w') as outfile:\n",
    "    json.dump(grammar_data, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Glossary Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pattern(pattern):\n",
    "    print('MATCHED:')\n",
    "    for tag in lex_df.grm_desc.unique():\n",
    "        if type(tag) == str:\n",
    "            if re.search(pattern, tag):\n",
    "                print(tag)\n",
    "    print()\n",
    "    print('NOT MATCHED')\n",
    "    for tag in lex_df.grm_desc.unique():\n",
    "        if type(tag) == str:\n",
    "            if not re.search(pattern, tag):\n",
    "                print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a series of regex patterns to parse grammatical data\n",
    "re_to_gram_raw = [\n",
    "    [\n",
    "        'pos',\n",
    "        [\n",
    "            (r'\\.n\\.|^n\\.', 'NOUN'),\n",
    "            (r'adj', 'ADJV'),\n",
    "            (r'adv', 'ADVB'),\n",
    "            (r'part', 'PART'),\n",
    "            (r'num', 'NUMR'),\n",
    "            (r'interj', 'INTJ'),\n",
    "            (r'prep', 'PREP'),\n",
    "            (r'mod', 'MODI'),\n",
    "            (r'pron', 'PRON'),\n",
    "            (r'imper', 'VERB'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'form',\n",
    "        [\n",
    "            (r'inv', 'INVAR')\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'st',\n",
    "        [\n",
    "            (r'cst', 'C'),\n",
    "            (r'abs', 'A'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'gn',\n",
    "        [\n",
    "            (r'[^\\w]m[^\\w]|^m[^\\w]|[^\\w]m$|[^\\w]ms\\.|^ms\\.|mpl', 'M'),\n",
    "            (r'[^\\w]f[^\\w]|^f[^\\w]|[^\\w]f$|[^\\w]fs\\.|^fs\\.|fpl', 'F'),\n",
    "            (r'[^\\w]cs[^\\w]|^cs[^\\w]|[^\\w]cs$', 'C'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'nu',\n",
    "        [\n",
    "            (r'[mf]s|sing|sg', 'SG'),\n",
    "            (r'pl', 'PL'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'nu_class',\n",
    "        [\n",
    "            ('tan', 'TANT'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'trns',\n",
    "        [\n",
    "            ('tr\\.', 'TR'),\n",
    "            ('intr\\.', 'ITR'),\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'syn',\n",
    "        [\n",
    "            ('anton\\.', 'AN'),\n",
    "            ('synon\\.', 'SN')\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        'tense',\n",
    "        [\n",
    "            ('imper\\.', 'IMP')\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "re_to_gram = []\n",
    "\n",
    "for gram, patts in re_to_gram_raw:\n",
    "    patts_comp = [(re.compile(patt), repl) for patt, repl in patts]\n",
    "    re_to_gram.append([gram, patts_comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pattern(r'tan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Lexicon Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': ['NOUN'], 'gn': 'MF'}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_tags('n.f./m.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tags(tag_string):\n",
    "    \"\"\"Parse grammatical tags\"\"\"\n",
    "    parsing = collections.defaultdict(list)\n",
    "    for cat, patts2vals in re_to_gram:\n",
    "        for patt, val in patts2vals:\n",
    "            if patt.search(tag_string):\n",
    "                parsing[cat].append(val)\n",
    "    parsing = clean_duplicates(parsing, tag_string)\n",
    "    return parsing\n",
    "\n",
    "def clean_duplicates(parsed_data, tag):\n",
    "    \"\"\"Clean any duplicate information in a parsed string\"\"\"\n",
    "    new_data = {}\n",
    "    for cat, values in parsed_data.items():\n",
    "\n",
    "        if cat != 'pos' and len(values) == 1:\n",
    "            new_data[cat] = values[0]\n",
    "            \n",
    "        elif cat == 'pos' and len(values) == 1:\n",
    "            new_data['pos'] = values\n",
    "            \n",
    "        elif cat == 'pos':\n",
    "            new_data[cat] = [values[0]]\n",
    "            new_data['pos_context'] = values\n",
    "        \n",
    "        elif cat == 'gn':\n",
    "            if 'M' in values and 'F' in values:\n",
    "                new_data['gn'] = 'MF'\n",
    "            else:\n",
    "                raise Exception(f'multiple genders found in {tag}')\n",
    "        \n",
    "        else:\n",
    "            raise Exception(f'multiple {cat} values found in {values}')\n",
    "\n",
    "    return new_data\n",
    "        \n",
    "def add_lex_number(form, pos, lnum):\n",
    "    \"\"\"Build a lexical entry with lex disambig number\n",
    "    \n",
    "    Count begins at 2\n",
    "    \"\"\"\n",
    "    if lnum != 1:\n",
    "        lnum = str(lnum)\n",
    "        return '.'.join([form, lnum])\n",
    "    else:\n",
    "        return '.'.join([form])\n",
    "\n",
    "def get_lex_id(form, pos, lexicon):\n",
    "    \"\"\"Compile a lexical entry based on surface form.\n",
    "    \n",
    "    If form is already witnessed in a dictionary, add\n",
    "    disambiguator numbers.\n",
    "    \"\"\"\n",
    "    lnum = 1\n",
    "    lex_id = add_lex_number(form, pos, lnum)\n",
    "    while lex_id in lexicon:\n",
    "        lnum += 1\n",
    "        lex_id = add_lex_number(form, pos, lnum)\n",
    "    return lex_id\n",
    "\n",
    "def clean_empty_values(data_dict):\n",
    "    return {k:v for k, v in data_dict.items() if v}\n",
    "\n",
    "def build_lexeme(form, gloss_data, parsed_data,\n",
    "                 lexicon, default_pos=''):\n",
    "    \"\"\"Build a lexeme from its parsed data and gloss data.\n",
    "    \n",
    "    Args:\n",
    "        parsed_data: parsing data from the grammatical\n",
    "            tag\n",
    "        gloss_data: data direct from the glossary\n",
    "        default_pos: supply part of speech value\n",
    "            if missing; some forms only have gender,\n",
    "            and the pos is inherited from the entry\n",
    "    \"\"\"\n",
    "\n",
    "    form = norm_entry(form)\n",
    "    gloss = norm_entry(gloss_data.get('trans',''))\n",
    "    meanings = '; '.join(gloss_data.get('meanings',[]))\n",
    "    entry = norm_entry(gloss_data.get('examples','') + meanings)\n",
    "    lang = norm_entry(gloss_data.get('lang',''))\n",
    "    parsed_data_no_pos = {k:v for k,v in parsed_data.items() if k != 'pos'}\n",
    "    \n",
    "    for pos in parsed_data.get('pos', [default_pos]):\n",
    "        if not pos:\n",
    "            raise Exception('blank pos encountered')\n",
    "\n",
    "        # add data in desired order / headings\n",
    "        lex_data = {\n",
    "            'form': form,\n",
    "            'pos': pos,\n",
    "        }\n",
    "        lex_data.update(parsed_data_no_pos)\n",
    "        lex_data.update({\n",
    "            'gloss': gloss,\n",
    "            'entry': entry,\n",
    "            'lang': lang,\n",
    "        })\n",
    "        lex_data = clean_empty_values(lex_data)\n",
    "        lex_id = get_lex_id(form, pos, lexicon)\n",
    "        yield (lex_id, lex_data)\n",
    "        \n",
    "def build_inflection(form, parsed_data, new_parses={}, default_pos=''):\n",
    "    \"\"\"Build an inflected form for the inflections file\"\"\"\n",
    "    form = normalize_entry(form)\n",
    "    parsed_data_no_pos = {k:v for k,v in parsed_data.items() if k != 'pos'}\n",
    "    for pos in parsed_data.get('pos', [default_pos]):\n",
    "        data = {'form': form, 'pos': pos}\n",
    "        data.update(parsed_data_no_pos)\n",
    "        data.update(new_parses) # write over with localized parses\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lem(lemma):\n",
    "    \"\"\"Splits a composite lemma into multiple forms\"\"\"\n",
    "    return re.split('\\s*;\\s*|\\s*,\\s*', lemma)\n",
    "\n",
    "def split_forms(form):\n",
    "    \"\"\"Split forms into words\"\"\"\n",
    "    grm_tag, wordforms = form\n",
    "    words = wordforms.split(',')\n",
    "    words = [word.strip() for word in words]\n",
    "    for word in words:\n",
    "        yield (grm_tag, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {}\n",
    "inflections = []\n",
    "\n",
    "for lem in glossary_data:\n",
    "\n",
    "    grm_desc = lem['grm_desc']\n",
    "    parsed_grm = parse_tags(grm_desc)\n",
    "\n",
    "    lemma_forms = split_lem(lem['lemma'])\n",
    "    primary_lemma = lemma_forms[0]\n",
    "\n",
    "    lex2data = list(build_lexeme(\n",
    "        primary_lemma, \n",
    "        lem, \n",
    "        parsed_grm, \n",
    "        lexicon, \n",
    "        'NOUN' # default pos for when only gender is given\n",
    "    ))\n",
    "    lexemes = []\n",
    "    for lexeme, lex_data in lex2data:\n",
    "        lexemes.append(lexeme)\n",
    "        lexicon[lexeme] = lex_data\n",
    "    \n",
    "    \n",
    "    for lexeme in lexemes:\n",
    "        parsed_grm['lex'] = lexeme\n",
    "        for form in lemma_forms:\n",
    "            for inflection in build_inflection(form, parsed_grm, default_pos='NOUN'):\n",
    "                inflections.append(inflection)\n",
    "\n",
    "        for form in lem.get('forms', []):\n",
    "            for grm_tag, wordforms in split_forms(form):\n",
    "                form_parsed_grm = parse_tags(grm_tag)\n",
    "                for form in wordforms:\n",
    "                    for inflection in build_inflection(form, parsed_grm, form_parsed_grm, 'NOUN'):\n",
    "                        inflections.append(inflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lexicon.json', 'w') as outfile:\n",
    "    json.dump(lexicon, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inflections.json', 'w') as outfile:\n",
    "    json.dump(inflections, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
